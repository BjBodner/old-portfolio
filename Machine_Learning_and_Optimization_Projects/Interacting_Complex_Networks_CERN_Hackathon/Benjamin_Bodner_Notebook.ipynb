{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import math\n",
    "import csv\n",
    "import array\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.25695455 0.74531269 0.47999889 0.65111411 0.36705798 1.3735466\n",
      " 0.40186521 1.25038254 1.59796798]\n"
     ]
    }
   ],
   "source": [
    "## Please ignore the \n",
    "\n",
    "\n",
    "Data = pandas.read_csv(r\"C:\\Users\\benjy\\OneDrive\\Brown\\Fall 2018\\Hackathon\\Python files\\Higgs_Higgs_500k.csv\")\n",
    "\n",
    "Data = np.array(Data)\n",
    "print(Data[1:10,18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dividing into test train and cv\n",
    "CV_min_index = int(0.8*Data.shape[0])\n",
    "CV_max_index = int(0.9*Data.shape[0])\n",
    "\n",
    "Train_X = Data[0:CV_min_index,1:28]\n",
    "Train_Y = Data[0:CV_min_index,0]\n",
    "\n",
    "CV_X = Data[CV_min_index:CV_max_index,1:28]\n",
    "CV_Y = Data[CV_min_index:CV_max_index,0]\n",
    "\n",
    "Test_X = Data[CV_max_index:Data.shape[0],1:28]\n",
    "Test_Y = Data[CV_max_index:Data.shape[0],0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TotalParameters_in_3_layer_net 142\n",
      "TotalParameters_in_4_layer_net 174\n",
      "TotalParameters_in_5_layer_net 2506\n"
     ]
    }
   ],
   "source": [
    "## Net specifications\n",
    "\n",
    "## The two here is because of the comlpex networks\n",
    "\n",
    "## The 3 layer nets are of size\n",
    "Number_Of_Nurons_in_3_layer_net = np.array([10,5,1])\n",
    "TotalParameters_in_3_layer_net = 2*(Number_Of_Nurons_in_3_layer_net[0]*Number_Of_Nurons_in_3_layer_net[1] + \\\n",
    "Number_Of_Nurons_in_3_layer_net[1]*Number_Of_Nurons_in_3_layer_net[2] + np.sum(Number_Of_Nurons_in_3_layer_net))\n",
    "\n",
    "## The 4 layer nets are of size\n",
    "Number_Of_Nurons_in_4_layer_net = np.array([10,5,3,1])\n",
    "TotalParameters_in_4_layer_net = 2*(Number_Of_Nurons_in_4_layer_net[0]*Number_Of_Nurons_in_4_layer_net[1] + \\\n",
    "Number_Of_Nurons_in_4_layer_net[1]*Number_Of_Nurons_in_4_layer_net[2] + \\\n",
    "Number_Of_Nurons_in_4_layer_net[2]*Number_Of_Nurons_in_4_layer_net[3] + np.sum(Number_Of_Nurons_in_4_layer_net))\n",
    "\n",
    "## The 5 layer nets are of size\n",
    "# Number_Of_Nurons_in_5_layer_net = np.array([10,10,4,4,1])\n",
    "Number_Of_Nurons_in_5_layer_net = np.array([28,18,18,18,1])\n",
    "TotalParameters_in_5_layer_net = 2*(Number_Of_Nurons_in_5_layer_net[0]*Number_Of_Nurons_in_5_layer_net[1] + \\\n",
    "Number_Of_Nurons_in_5_layer_net[1]*Number_Of_Nurons_in_5_layer_net[2] + \\\n",
    "Number_Of_Nurons_in_5_layer_net[2]*Number_Of_Nurons_in_5_layer_net[3] +\\\n",
    "Number_Of_Nurons_in_5_layer_net[3]*Number_Of_Nurons_in_5_layer_net[4] + np.sum(Number_Of_Nurons_in_5_layer_net))\n",
    "\n",
    "# ## The 6 layer nets are of size\n",
    "# Number_Of_Nurons_in_6_layer_net = np.array([10,6,4,3,2,1])\n",
    "# TotalParameters_in_6_layer_net = 2*(Number_Of_Nurons_in_6_layer_net[0]*Number_Of_Nurons_in_6_layer_net[1] + \\\n",
    "# Number_Of_Nurons_in_6_layer_net[1]*Number_Of_Nurons_in_6_layer_net[2] + \\\n",
    "# Number_Of_Nurons_in_6_layer_net[2]*Number_Of_Nurons_in_6_layer_net[3] +\\\n",
    "# Number_Of_Nurons_in_6_layer_net[3]*Number_Of_Nurons_in_6_layer_net[4] +\\\n",
    "# Number_Of_Nurons_in_6_layer_net[4]*Number_Of_Nurons_in_6_layer_net[5] + np.sum(Number_Of_Nurons_in_6_layer_net))\n",
    "\n",
    "\n",
    "print(\"TotalParameters_in_3_layer_net\",TotalParameters_in_3_layer_net)\n",
    "print(\"TotalParameters_in_4_layer_net\",TotalParameters_in_4_layer_net)\n",
    "print(\"TotalParameters_in_5_layer_net\",TotalParameters_in_5_layer_net)\n",
    "# print(\"TotalParameters_in_6_layer_net\",TotalParameters_in_6_layer_net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Net1_(object):\n",
    "    \n",
    "    def __init__(self, Parameter_Vector, Number_Of_Specialists):\n",
    "        \n",
    "        N_0 = 0\n",
    "        N1 = Number_Of_Specialists\n",
    "\n",
    "        \n",
    "        ## Reshaping into Weights\n",
    "        self.W1 =Parameter_Vector[N_0:N_0 + N1*1].reshape(N1,1) \n",
    "        N_0 += N1*1    \n",
    "\n",
    "        \n",
    "        ## Reshaping into biases\n",
    "        self.b1 = Parameter_Vector[N_0:N_0 + 1].reshape(1,1) \n",
    "        N_0 += 1    \n",
    "        \n",
    "\n",
    "        \n",
    "def Reshape_into_1_layer_net(Parameter_Vector,Number_Of_Specialists):\n",
    "    Net_ = Net1_(Parameter_Vector,Number_Of_Specialists)\n",
    "    return Net_\n",
    "\n",
    "Parameter_Vector = np.random.rand(174)\n",
    "\n",
    "# Net1 = Reshape_into_1_layer_net(Parameter_Vector,NumberOfNetworks)\n",
    "\n",
    "# print(Net1.W1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.16848099+0.15079139j 0.03515738+0.6350519j  0.22820743+0.89887132j\n",
      "  0.05170275+0.90497685j 0.11242938+0.42913845j]\n",
      " [0.62185549+0.31904159j 0.16666419+0.60581108j 0.83045359+0.10630113j\n",
      "  0.44507388+0.31896229j 0.82059145+0.97813191j]\n",
      " [0.27608644+0.80171404j 0.32310736+0.40353682j 0.38371254+0.03847635j\n",
      "  0.0331404 +0.5387777j  0.26552368+0.69057716j]\n",
      " [0.89997711+0.89963091j 0.90650895+0.67601596j 0.42844321+0.85459959j\n",
      "  0.07151261+0.83736563j 0.34731521+0.77636584j]\n",
      " [0.30408111+0.58294068j 0.41902613+0.00639335j 0.74971928+0.13051239j\n",
      "  0.0076706 +0.05682784j 0.45886146+0.86070151j]\n",
      " [0.84600858+0.56075077j 0.30342943+0.75544124j 0.83026401+0.6343046j\n",
      "  0.20387045+0.53159943j 0.11441244+0.81916741j]\n",
      " [0.1937419 +0.0451635j  0.47713707+0.7693193j  0.67018898+0.7329021j\n",
      "  0.7185524 +0.68723077j 0.64897676+0.43111922j]\n",
      " [0.04499176+0.18267086j 0.86488758+0.31351195j 0.68068853+0.95996528j\n",
      "  0.29817394+0.0915135j  0.90343292+0.8024691j ]\n",
      " [0.71386931+0.95566092j 0.97382518+0.67347772j 0.01019809+0.91205927j\n",
      "  0.57033784+0.78871887j 0.01996262+0.64978683j]\n",
      " [0.11941702+0.9893798j  0.66153836+0.38431162j 0.71256907+0.04257406j\n",
      "  0.47300782+0.26868471j 0.11764685+0.59520849j]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Defining the 3 layer complex network\n",
    "\n",
    "\n",
    "class Net3_(object):\n",
    "    \n",
    "    def __init__(self, Parameter_Vector, Number_Of_Nurons_in_3_layer_net):\n",
    "        \n",
    "        N_0 = 0\n",
    "        N1 = Number_Of_Nurons_in_3_layer_net[0]\n",
    "        N2 = Number_Of_Nurons_in_3_layer_net[1]\n",
    "        N3 = Number_Of_Nurons_in_3_layer_net[2]\n",
    "        \n",
    "        ## Reshaping into Weights\n",
    "        self.W1 = Parameter_Vector[N_0 :N_0 + N1*N2].reshape(N1,N2) + \\\n",
    "                    1j*Parameter_Vector[N_0 + N1*N2:N_0 + 2*N1*N2].reshape(N1,N2)\n",
    "        N_0 += 2*N1*N2     \n",
    "        \n",
    "        self.W2 =Parameter_Vector[N_0:N_0 + N2*N3].reshape(N2,N3) + \\\n",
    "                    1j*Parameter_Vector[N_0+ N2*N3:N_0 + 2*N2*N3].reshape(N2,N3)\n",
    "        N_0 += 2*N2*N3      \n",
    "        \n",
    "        self.W3 =Parameter_Vector[N_0:N_0 + N3*1].reshape(N3,1) + \\\n",
    "                    1j*Parameter_Vector[N_0+ N3*1:N_0 + 2*N3*1].reshape(N3,1)\n",
    "        N_0 += 2*N3*1    \n",
    "\n",
    "        \n",
    "        \n",
    "        ## Reshaping into biases\n",
    "        self.b1 = Parameter_Vector[N_0 :N_0 + N2].reshape(N2,1) + 1j*Parameter_Vector[N_0 + N2:N_0 + 2*N2].reshape(N2,1)\n",
    "        N_0 += 2*N2        \n",
    "        \n",
    "        self.b2 =  Parameter_Vector[N_0:N_0 + N3].reshape(N3,1) + 1j* Parameter_Vector[N_0 + N3:N_0 + 2*N3].reshape(N3,1)\n",
    "        N_0 += 2*N3      \n",
    "        \n",
    "        self.b3 = Parameter_Vector[N_0:N_0 + 1].reshape(1,1) + 1j*Parameter_Vector[N_0 + 1:N_0 + 2*1].reshape(1,1)\n",
    "        N_0 += 2*1    \n",
    "        \n",
    "\n",
    "        \n",
    "def Reshape_into_3_layer_net(Parameter_Vector,Number_Of_Nurons_in_3_layer_net):\n",
    "    Net_ = Net3_(Parameter_Vector,Number_Of_Nurons_in_3_layer_net)\n",
    "    return Net_\n",
    "\n",
    "Parameter_Vector = np.random.rand(142)\n",
    "\n",
    "Net3 = Reshape_into_3_layer_net(Parameter_Vector,Number_Of_Nurons_in_3_layer_net)\n",
    "\n",
    "print(Net3.W1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining the 4 layer complex network\n",
    "\n",
    "\n",
    "class Net4_(object):\n",
    "    \n",
    "    def __init__(self, Parameter_Vector, Number_Of_Nurons_in_4_layer_net):\n",
    "        \n",
    "        N_0 = 0\n",
    "        N1 = Number_Of_Nurons_in_4_layer_net[0]\n",
    "        N2 = Number_Of_Nurons_in_4_layer_net[1]\n",
    "        N3 = Number_Of_Nurons_in_4_layer_net[2]\n",
    "        N4 = Number_Of_Nurons_in_4_layer_net[3]\n",
    "        \n",
    "        ## Reshaping into Weights\n",
    "        self.W1 = Parameter_Vector[N_0 :N_0 + N1*N2].reshape(N1,N2) + \\\n",
    "                    1j*Parameter_Vector[N_0 + N1*N2:N_0 + 2*N1*N2].reshape(N1,N2)\n",
    "        N_0 += 2*N1*N2     \n",
    "        \n",
    "        self.W2 =Parameter_Vector[N_0:N_0 + N2*N3].reshape(N2,N3) + \\\n",
    "                    1j*Parameter_Vector[N_0+ N2*N3:N_0 + 2*N2*N3].reshape(N2,N3)\n",
    "        N_0 += 2*N2*N3      \n",
    "        \n",
    "        self.W3 =Parameter_Vector[N_0:N_0 + N3*N4].reshape(N3,N4) + \\\n",
    "                    1j*Parameter_Vector[N_0+ N3*N4:N_0 + 2*N3*N4].reshape(N3,N4)\n",
    "        N_0 += 2*N3*N4      \n",
    "\n",
    "        self.W4 =Parameter_Vector[N_0:N_0 + N4*1].reshape(N4,1) + \\\n",
    "                    1j*Parameter_Vector[N_0+ N4*1:N_0 + 2*N4*1].reshape(N4,1)\n",
    "        N_0 += 2*N4*1   \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ## Reshaping into biases\n",
    "        self.b1 = Parameter_Vector[N_0 :N_0 + N2].reshape(N2,1) + 1j*Parameter_Vector[N_0 + N2:N_0 + 2*N2].reshape(N2,1)\n",
    "        N_0 += 2*N2        \n",
    "        \n",
    "        self.b2 =  Parameter_Vector[N_0:N_0 + N3].reshape(N3,1) + 1j* Parameter_Vector[N_0 + N3:N_0 + 2*N3].reshape(N3,1)\n",
    "        N_0 += 2*N3      \n",
    "        \n",
    "        self.b3 =  Parameter_Vector[N_0:N_0 + N4].reshape(N4,1) + 1j* Parameter_Vector[N_0 + N4:N_0 + 2*N4].reshape(N4,1)\n",
    "        N_0 += 2*N4      \n",
    "        \n",
    "        self.b4 = Parameter_Vector[N_0:N_0 + 1].reshape(1,1) + 1j*Parameter_Vector[N_0 + 1:N_0 + 2*1].reshape(1,1)\n",
    "        N_0 += 2*1    \n",
    "        \n",
    "def Reshape_into_4_layer_net(Parameter_Vector,Number_Of_Nurons_in_4_layer_net):\n",
    "    Net_ = Net4_(Parameter_Vector,Number_Of_Nurons_in_4_layer_net)\n",
    "    return Net_\n",
    "\n",
    "Parameter_Vector = np.random.rand(174)\n",
    "\n",
    "\n",
    "Net4 = Reshape_into_4_layer_net(Parameter_Vector,Number_Of_Nurons_in_4_layer_net)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 18)\n"
     ]
    }
   ],
   "source": [
    "## Defining the 5 layer complex network\n",
    "\n",
    "\n",
    "class Net5_(object):\n",
    "    \n",
    "    def __init__(self, Parameter_Vector, Number_Of_Nurons_in_5_layer_net):\n",
    "        \n",
    "        N_0 = 0\n",
    "        N1 = Number_Of_Nurons_in_5_layer_net[0]\n",
    "        N2 = Number_Of_Nurons_in_5_layer_net[1]\n",
    "        N3 = Number_Of_Nurons_in_5_layer_net[2]\n",
    "        N4 = Number_Of_Nurons_in_5_layer_net[3]\n",
    "        N5 = Number_Of_Nurons_in_5_layer_net[4]\n",
    "        \n",
    "        ## Reshaping into Weights\n",
    "        self.W1 = Parameter_Vector[N_0 :N_0 + N1*N2].reshape(N1,N2) + \\\n",
    "                    1j*Parameter_Vector[N_0 + N1*N2:N_0 + 2*N1*N2].reshape(N1,N2)\n",
    "        N_0 += 2*N1*N2     \n",
    "        \n",
    "        self.W2 =Parameter_Vector[N_0:N_0 + N2*N3].reshape(N2,N3) + \\\n",
    "                    1j*Parameter_Vector[N_0+ N2*N3:N_0 + 2*N2*N3].reshape(N2,N3)\n",
    "        N_0 += 2*N2*N3      \n",
    "        \n",
    "        self.W3 =Parameter_Vector[N_0:N_0 + N3*N4].reshape(N3,N4) + \\\n",
    "                    1j*Parameter_Vector[N_0+ N3*N4:N_0 + 2*N3*N4].reshape(N3,N4)\n",
    "        N_0 += 2*N3*N4      \n",
    "\n",
    "        self.W4 =Parameter_Vector[N_0:N_0 + N4*N5].reshape(N4,N5) + \\\n",
    "                    1j*Parameter_Vector[N_0+ N4*N5:N_0 + 2*N4*N5].reshape(N4,N5)\n",
    "        N_0 += 2*N4*N5     \n",
    "        \n",
    "        self.W5 =Parameter_Vector[N_0:N_0 + N5*1].reshape(N5,1) + \\\n",
    "                    1j*Parameter_Vector[N_0+ N5*1:N_0 + 2*N5*1].reshape(N5,1)\n",
    "        N_0 += 2*N5*1          \n",
    "        \n",
    "        \n",
    "        ## Reshaping into biases\n",
    "        self.b1 = Parameter_Vector[N_0 :N_0 + N2].reshape(N2,1) + 1j*Parameter_Vector[N_0 + N2:N_0 + 2*N2].reshape(N2,1)\n",
    "        N_0 += 2*N2        \n",
    "        \n",
    "        self.b2 =  Parameter_Vector[N_0:N_0 + N3].reshape(N3,1) + 1j* Parameter_Vector[N_0 + N3:N_0 + 2*N3].reshape(N3,1)\n",
    "        N_0 += 2*N3      \n",
    "        \n",
    "        self.b3 =  Parameter_Vector[N_0:N_0 + N4].reshape(N4,1) + 1j* Parameter_Vector[N_0 + N4:N_0 + 2*N4].reshape(N4,1)\n",
    "        N_0 += 2*N4      \n",
    "        \n",
    "        self.b4 =  Parameter_Vector[N_0:N_0 + N5].reshape(N5,1) + 1j* Parameter_Vector[N_0 + N5:N_0 + 2*N5].reshape(N5,1)\n",
    "        N_0 += 2*N5        \n",
    "        \n",
    "        self.b5 = Parameter_Vector[N_0:N_0 + 1].reshape(1,1) + 1j*Parameter_Vector[N_0 + 1:N_0 + 2*1].reshape(1,1)\n",
    "        N_0 += 2*1    \n",
    "        \n",
    "def Reshape_into_5_layer_net(Parameter_Vector,Number_Of_Nurons_in_5_layer_net):\n",
    "    Net_ = Net5_(Parameter_Vector,Number_Of_Nurons_in_5_layer_net)\n",
    "    return Net_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Parameter_Vector = np.random.rand(TotalParameters_in_5_layer_net)\n",
    "Net5 = Reshape_into_5_layer_net(Parameter_Vector,Number_Of_Nurons_in_5_layer_net)\n",
    "\n",
    "print(Net5.W1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.22210964]]\n",
      "1j\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Defining the Running through network functions\n",
    "\n",
    "\n",
    "def Run_Through_5_layer_Net(Net,X):\n",
    "\n",
    "        Z1 = np.dot(Net.W1.T,X) + Net.b1\n",
    "#         A1 = np.maximum(Z1,0) + 0.1*np.minimum(Z1,0)\n",
    "        A1 = np.maximum(Z1.real,0) + 0.1*np.minimum(Z1.real,0) + 1j*(np.maximum(Z1.imag,0) + 0.1*np.minimum(Z1.imag,0))\n",
    "\n",
    "\n",
    "        Z2 = np.dot(Net.W2.T,A1) + Net.b2\n",
    "#         A2 = np.maximum(Z2,0) + 0.1*np.minimum(Z2,0)\n",
    "        A2 = np.maximum(Z2.real,0) + 0.1*np.minimum(Z2.real,0) + 1j*(np.maximum(Z2.imag,0) + 0.1*np.minimum(Z2.imag,0))\n",
    "    \n",
    "        Z3 = np.dot(Net.W3.T,A2) + Net.b3\n",
    "#         A3 = np.maximum(Z3,0) + 0.1*np.minimum(Z3,0)\n",
    "        A3 = np.maximum(Z3.real,0) + 0.1*np.minimum(Z3.real,0) + 1j*(np.maximum(Z3.imag,0) + 0.1*np.minimum(Z3.imag,0))        \n",
    "        \n",
    "        Z4 = np.dot(Net.W4.T,A3) + Net.b4\n",
    "#         A4 = np.maximum(Z4,0) + 0.1*np.minimum(Z4,0)\n",
    "        A4 = np.maximum(Z4.real,0) + 0.1*np.minimum(Z4.real,0) + 1j*(np.maximum(Z4.imag,0) + 0.1*np.minimum(Z4.imag,0))\n",
    "    \n",
    "    \n",
    "        Z5 = np.dot(Net.W4.T,A3) + Net.b5\n",
    "#         A5 = 1/(1 + np.exp(-(10**-4)*Z5.real)) \n",
    "        A5 = 1/(1 + np.exp(-(10**-4)*np.maximum(np.minimum(Z5.real,10**5),-10**5)))\n",
    "        \n",
    "        \n",
    "        return A5\n",
    "\n",
    "    \n",
    "    \n",
    "def Run_Through_4_layer_Net(Net,X):\n",
    "\n",
    "        Z1 = np.dot(Net.W1.T,X) + Net.b1\n",
    "        A1 = np.maximum(Z1,0) + 0.1*np.minimum(Z1,0)\n",
    "\n",
    "\n",
    "        Z2 = np.dot(Net.W2.T,A1) + Net.b2\n",
    "        A2 = np.maximum(Z2,0) + 0.1*np.minimum(Z2,0)\n",
    "\n",
    "        Z3 = np.dot(Net.W3.T,A2) + Net.b3\n",
    "        A3 = np.maximum(Z3,0) + 0.1*np.minimum(Z3,0)\n",
    "        \n",
    "        \n",
    "        Z4 = np.dot(Net.W4.T,A3) + Net.b4\n",
    "        A4 = 1/(1 + np.exp(-Z4.real))\n",
    "\n",
    "        \n",
    "        return A4\n",
    "    \n",
    "    \n",
    "    \n",
    "def Run_Through_3_layer_Net(Net,X):\n",
    "\n",
    "        Z1 = np.dot(Net.W1.T,X) + Net.b1\n",
    "        A1 = np.maximum(Z1,0) + 0.1*np.minimum(Z1,0)\n",
    "\n",
    "        Z2 = np.dot(Net.W2.T,A1) + Net.b2\n",
    "        A2 = np.maximum(Z2,0) + 0.1*np.minimum(Z2,0)\n",
    "\n",
    "        Z3 = np.dot(Net.W3.T,A2) + Net.b3\n",
    "        A3 =  1/(1 + np.exp(-Z3.real))\n",
    "        \n",
    "        \n",
    "        return A3\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "X = np.random.rand(28,1)\n",
    "# print(Run_Through_3_layer_Net(Net3,X))\n",
    "# print(Run_Through_4_layer_Net(Net4,X))\n",
    "print(Run_Through_5_layer_Net(Net5,X))\n",
    "\n",
    "a = -1 + 1j\n",
    "print(np.maximum(a.real ,0) + 1j*np.maximum(a.imag ,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.24411324 0.22070054 0.24472298 0.21333014]]\n"
     ]
    }
   ],
   "source": [
    "## Tests on the network\n",
    "\n",
    "## Defining the Running through network functions\n",
    "\n",
    "\n",
    "def Run_Through_5_layer_Net_And_Save_Activations(Net,X):\n",
    "\n",
    "        Z1 = np.dot(Net.W1.T,X) + Net.b1\n",
    "#         A1 = np.maximum(Z1,0) + 0.1*np.minimum(Z1,0)\n",
    "        A1 = np.maximum(Z1.real,0) + 0.1*np.minimum(Z1.real,0) + 1j*(np.maximum(Z1.imag,0) + 0.1*np.minimum(Z1.imag,0))\n",
    "\n",
    "\n",
    "        Z2 = np.dot(Net.W2.T,A1) + Net.b2\n",
    "#         A2 = np.maximum(Z2,0) + 0.1*np.minimum(Z2,0)\n",
    "        A2 = np.maximum(Z2.real,0) + 0.1*np.minimum(Z2.real,0) + 1j*(np.maximum(Z2.imag,0) + 0.1*np.minimum(Z2.imag,0))\n",
    "    \n",
    "        Z3 = np.dot(Net.W3.T,A2) + Net.b3\n",
    "#         A3 = np.maximum(Z3,0) + 0.1*np.minimum(Z3,0)\n",
    "        A3 = np.maximum(Z3.real,0) + 0.1*np.minimum(Z3.real,0) + 1j*(np.maximum(Z3.imag,0) + 0.1*np.minimum(Z3.imag,0))    \n",
    "        \n",
    "        Z4 = np.dot(Net.W4.T,A3) + Net.b4\n",
    "#         A4 = np.maximum(Z4,0) + 0.1*np.minimum(Z4,0)\n",
    "        A4 = np.maximum(Z4.real,0) + 0.1*np.minimum(Z4.real,0) + 1j*(np.maximum(Z4.imag,0) + 0.1*np.minimum(Z4.imag,0))\n",
    "        \n",
    "        \n",
    "        Z5 = np.dot(Net.W4.T,A3) + Net.b5\n",
    "#         A5 = 1/(1 + np.exp(-(10**-4)*Z5.real)) \n",
    "        A5 = 1/(1 + np.exp(-(10**-4)*np.maximum(np.minimum(Z5.real,10**5),-10**5)))\n",
    "        \n",
    "        return A5, A4, A3 ,A2\n",
    "\n",
    "    \n",
    "    \n",
    "def Run_Through_4_layer_Net_And_Save_Activations(Net,X):\n",
    "\n",
    "        Z1 = np.dot(Net.W1.T,X) + Net.b1\n",
    "        A1 = np.maximum(Z1,0) + 0.1*np.minimum(Z1,0)\n",
    "\n",
    "\n",
    "        Z2 = np.dot(Net.W2.T,A1) + Net.b2\n",
    "        A2 = np.maximum(Z2,0) + 0.1*np.minimum(Z2,0)\n",
    "\n",
    "        Z3 = np.dot(Net.W3.T,A2) + Net.b3\n",
    "        A3 = np.maximum(Z3,0) + 0.1*np.minimum(Z3,0)\n",
    "        \n",
    "        \n",
    "        Z4 = np.dot(Net.W4.T,A3) + Net.b4\n",
    "        A4 = 1/(1 + np.exp(-Z4.real))\n",
    "\n",
    "        \n",
    "        return A4 , A3 , A2 ,A1\n",
    "    \n",
    "    \n",
    "    \n",
    "def Run_Through_3_layer_Net_And_Save_Activations(Net,X):\n",
    "\n",
    "        Z1 = np.dot(Net.W1.T,X) + Net.b1\n",
    "        A1 = np.maximum(Z1,0) + 0.1*np.minimum(Z1,0)\n",
    "    \n",
    "\n",
    "        Z2 = np.dot(Net.W2.T,A1) + Net.b2\n",
    "        A2 = np.maximum(Z2,0) + 0.1*np.minimum(Z2,0)\n",
    "\n",
    "        Z3 = np.dot(Net.W3.T,A2) + Net.b3\n",
    "        A3 =  1/(1 + np.exp(-Z3.real))\n",
    "        print(A3.shape)\n",
    "        \n",
    "        return A3 , A2 , A1\n",
    "    \n",
    "    \n",
    "    \n",
    "X = np.random.rand(28,4)\n",
    "# a,b,c = Run_Through_3_layer_Net_And_Save_Activations(Net3,X)\n",
    "A5 , A4 , A3, A2  = Run_Through_5_layer_Net_And_Save_Activations(Net5,X)\n",
    "print( A5)\n",
    "# print(Run_Through_4_layer_Net(Net4,X))\n",
    "# print(Run_Through_5_layer_Net(Net5,X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using pinball algorithm\n",
    "## This block has all the functions for the training process\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cmath\n",
    "\n",
    "        \n",
    "## Generate Random Indexes\n",
    "def Generate_Random_Indexes():\n",
    "    Random_Indexes = np.random.randint(0,27, size=28)\n",
    "    p = 1\n",
    "    while p == 1:\n",
    "        p = 0\n",
    "        for i in range (0,28):\n",
    "            for j in range(i+1,28):\n",
    "                if Random_Indexes[j] == Random_Indexes[i]:\n",
    "                    Random_Indexes[j] = np.random.randint(27, size=1)\n",
    "                    p = 1\n",
    "                    break\n",
    "\n",
    "\n",
    "    return Random_Indexes     \n",
    "\n",
    "    \n",
    "def   Calculate_Loss(Predictions,Y):\n",
    "#     print(len(Y))\n",
    "    return -(1/len(Y))*np.sum(Y*np.log(Predictions+ 10**(-10)) + (1-Y)*np.log(1-Predictions+ 10**(-10)))\n",
    "    \n",
    "    \n",
    "    \n",
    "def   Calculate_Loss_For_Confidence(Predictions,Y):\n",
    "#     print(len(Y))\n",
    "    return -(1/np.sum(Confidence_Killing_Vector))*np.sum((Y*np.log(Predictions+ 10**(-10)) + (1-Y)*np.log(1-Predictions+ 10**(-10)))*Confidence_Killing_Vector)\n",
    "       \n",
    "    \n",
    "def   Calculate_Loss_For_UnConfidence(Predictions,Y):\n",
    "#     print(len(Y))\n",
    "    return -(1/np.sum(UnConfidence_Killing_Vector))*np.sum((Y*np.log(Predictions+ 10**(-10)) + (1-Y)*np.log(1-Predictions+ 10**(-10)))*UnConfidence_Killing_Vector)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def   Calculate_Specialist_Loss(Predictions,Y):\n",
    "#     LossVector = -(Y*np.log(Predictions + 10**(-10)) + (1-Y)*np.log(1-Predictions + 10**(-10)))\n",
    "#     return -(1/len(Y))*np.sum((Y*np.log(Predictions+ 10**(-10)) + \\\n",
    "#                                (1-Y)*np.log(1-Predictions+ 10**(-10)))*np.exp(-Specialization_Paramter*LossVector))\n",
    "\n",
    "\n",
    "    return -(1/np.sum(Confidence_Train_Y))*np.sum((Y*np.log(Predictions+ 10**(-10)) + \\\n",
    "                               (1-Y)*np.log(1-Predictions+ 10**(-10)))*Confidence_Train_Y)\n",
    "\n",
    "\n",
    "\n",
    "#     return -(1/len(Y))*np.sum((Y*np.log(Predictions+ 10**(-10)) + (1-Y)*np.log(1-Predictions+ 10**(-10)))*np.exp(-LossVector1))\n",
    "\n",
    "\n",
    "\n",
    "def   Create_Loss_historgram(Predictions,Y,Killing_Vector,Plot_Histogram):\n",
    "    \n",
    "    LossVector = -(Y*np.log(Predictions + 10**(-10)) + (1-Y)*np.log(1-Predictions + 10**(-10)))*Killing_Vector\n",
    "\n",
    "    NumberOfBins = 40\n",
    "    Bins = np.linspace(0,2,NumberOfBins)\n",
    "    Counts_in_bin = np.zeros(NumberOfBins)\n",
    "    \n",
    "    \n",
    "    for i in range (0,LossVector.shape[1]):\n",
    "        for j in range (0,len(Bins)-1):\n",
    "            if LossVector[0,i] > Bins[j] :\n",
    "                if LossVector[0,i] <= Bins[j+1]:\n",
    "                    Counts_in_bin[j] = Counts_in_bin[j] + 1\n",
    "                    \n",
    "    Counts_in_bin = Counts_in_bin/np.sum(Killing_Vector)\n",
    "    \n",
    "    if Plot_Histogram == 1:\n",
    "        plt.plot(Bins,Counts_in_bin)\n",
    "        \n",
    "    return Bins,Counts_in_bin\n",
    "    \n",
    "    \n",
    "    \n",
    "def Check_if_The_Net_passes_the_Specialist_test(Bins,Counts_in_bin):\n",
    "    Specialist_test_result = 0\n",
    "    \n",
    "    if np.max(Counts_in_bin) > 0.1:\n",
    "        if Bins[np.argmax(Counts_in_bin)] < Specialist_test_Threshold: ## This corresponds to 70 percent precision\n",
    "            Specialist_test_result = 1\n",
    "    \n",
    "    return Specialist_test_result\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "## Defining Cost Function - For 3 layer net\n",
    "class Cost_Function:\n",
    "    def Cost_Function(self,Individuals_Parameter_Vector): \n",
    "\n",
    "        NumberOfSamples = Individuals_Parameter_Vector.shape[1]\n",
    "        Cost_Function1 = np.zeros(NumberOfSamples)\n",
    "        \n",
    "        for i in range (0,NumberOfSamples):\n",
    "#             Net4 = Reshape_into_4_layer_net(Individuals_Parameter_Vector[:,i],Number_Of_Nurons_in_4_layer_net)\n",
    "#             Predictions = Run_Through_4_layer_Net(Net4,Train_X1 )\n",
    "            \n",
    "            Net5 = Reshape_into_5_layer_net(Individuals_Parameter_Vector[:,i],Number_Of_Nurons_in_5_layer_net)\n",
    "            Predictions = Run_Through_5_layer_Net(Net5,Train_X1 )\n",
    "            Cost_Function1[i] = Calculate_Loss(Predictions,Train_Y1)\n",
    "\n",
    "            \n",
    "        return Cost_Function1    \n",
    "\n",
    "    \n",
    "## Defining Specialist Cost Function - For 3 layer net\n",
    "class Specialist_Cost_Function:\n",
    "    def Cost_Function(self,Individuals_Parameter_Vector): \n",
    "\n",
    "        NumberOfSamples = Individuals_Parameter_Vector.shape[1]\n",
    "        Cost_Function1 = np.zeros(NumberOfSamples)\n",
    "        \n",
    "        for i in range (0,NumberOfSamples):\n",
    "#             Net4 = Reshape_into_4_layer_net(Individuals_Parameter_Vector[:,i],Number_Of_Nurons_in_4_layer_net)\n",
    "#             Predictions = Run_Through_4_layer_Net(Net4,Train_X1 )\n",
    "            Net5 = Reshape_into_5_layer_net(Individuals_Parameter_Vector[:,i],Number_Of_Nurons_in_5_layer_net)\n",
    "            Predictions = Run_Through_5_layer_Net(Net5,Train_X1 )\n",
    "            \n",
    "            Cost_Function1[i] = Calculate_Specialist_Loss(Predictions,Train_Y1)\n",
    "\n",
    "            \n",
    "        return Cost_Function1    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class Cost_Function_For_Confidence:\n",
    "    def Cost_Function(self,Individuals_Parameter_Vector): \n",
    "\n",
    "        NumberOfSamples = Individuals_Parameter_Vector.shape[1]\n",
    "        Cost_Function1 = np.zeros(NumberOfSamples)\n",
    "        \n",
    "        for i in range (0,NumberOfSamples):\n",
    "\n",
    "            Net5 = Reshape_into_5_layer_net(Individuals_Parameter_Vector[:,i],Number_Of_Nurons_in_5_layer_net)\n",
    "            Predictions = Run_Through_5_layer_Net(Net5,Confidence_Train_X )\n",
    "            Cost_Function1[i] = Calculate_Loss_For_Confidence(Predictions,Confidence_Train_Y)\n",
    "            \n",
    "        return Cost_Function1   \n",
    "\n",
    "    \n",
    "class Cost_Function_For_UnConfidence:\n",
    "    def Cost_Function(self,Individuals_Parameter_Vector): \n",
    "\n",
    "        NumberOfSamples = Individuals_Parameter_Vector.shape[1]\n",
    "        Cost_Function1 = np.zeros(NumberOfSamples)\n",
    "        \n",
    "        for i in range (0,NumberOfSamples):\n",
    "            Net5 = Reshape_into_5_layer_net(Individuals_Parameter_Vector[:,i],Number_Of_Nurons_in_5_layer_net)\n",
    "            Predictions = Run_Through_5_layer_Net(Net5,Confidence_Train_X )\n",
    "            Cost_Function1[i] = Calculate_Loss_For_UnConfidence(Predictions,UnConfidence_Train_Y)\n",
    "            \n",
    "        return Cost_Function1   \n",
    "    \n",
    "def Get_Predictions_and_Confidence_From_Networks(Net5,Net5_Confidence,X_test):\n",
    "    A5 , A4 , A3 ,A2 = Run_Through_4_layer_Net_And_Save_Activations(Net5,X_test)\n",
    "\n",
    "    Predictions_Of_Specialist = A5\n",
    "\n",
    "    Confidence_Data_from_X = np.concatenate((A2,A3,A4,A5))\n",
    "    Net4_Confidence\n",
    "\n",
    "    Confidence_Of_Specialist = Run_Through_5_layer_Net(Net5_Confidence,Confidence_Data_from_X )\n",
    "    \n",
    "    return Predictions_Of_Specialist,Confidence_Of_Specialist\n",
    "\n",
    "\n",
    "\n",
    "def Get_Predictions_and_Confidence_From_Networks_1(Net5,Net5_Confidence,Net5_UnConfidence,X):\n",
    "    A5 , A4 , A3 ,A2 = Run_Through_5_layer_Net_And_Save_Activations(Net5,X)\n",
    "\n",
    "    Predictions_Of_Specialist = A5\n",
    "\n",
    "    Confidence_Data_from_X = np.concatenate((A2,A3,A4,A5))\n",
    "\n",
    "    Confidence_Of_Specialist = Run_Through_5_layer_Net(Net5_Confidence,Confidence_Data_from_X )\n",
    "    Unconfidence_Of_Specialist = Run_Through_5_layer_Net(Net5_Confidence,Confidence_Data_from_X )\n",
    "    \n",
    "    Total_Confidence = Confidence_Of_Specialist*(1-Unconfidence_Of_Specialist)\n",
    "    return Predictions_Of_Specialist,Total_Confidence\n",
    "    \n",
    "## Defining Pinball HyperParameters\n",
    "NumberOfSamples = 30\n",
    "Search_Ratio = 0.7;\n",
    "AmplitudeOfLinearSearch = 1.0;\n",
    "AmplitudeOfRandomSearch = 1.0;\n",
    "print_Cost = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def Geterate_Net_With_Basic_Training():\n",
    "    Cost1 = Cost_Function()\n",
    "#     for i in range (0,1):\n",
    "    Initial_Parameters = np.random.rand(TotalParameters_in_5_layer_net,1)\n",
    "#     Initial_Parameters = np.random.rand(TotalParameters_in_5_layer_net,1)\n",
    "    CurrentCost = Cost1.Cost_Function(Initial_Parameters)\n",
    "\n",
    "    Killing_Vector = Initial_Parameters>0\n",
    "    print(\"Initial Cost is is: \", CurrentCost)\n",
    "\n",
    "    Best_Individual, CurrentCost = Optimize_With_Pinball_Algorithm(Train_X1 ,Train_Y1,Initial_Parameters,Cost1,Killing_Vector,Number_Of_Itterations,print_Cost,NumberOfSamples,Search_Ratio,AmplitudeOfLinearSearch,AmplitudeOfRandomSearch)\n",
    "    print(\"Final_Cost is: \", CurrentCost)\n",
    "\n",
    "\n",
    "    ## This is to test the best Net to see if it passes the test\n",
    "#         Net4 = Reshape_into_4_layer_net(Best_Individual,Number_Of_Nurons_in_4_layer_net)\n",
    "#         Predictions = Run_Through_4_layer_Net(Net4,Train_X1 )\n",
    "\n",
    "    Net5 = Reshape_into_5_layer_net(Best_Individual,Number_Of_Nurons_in_5_layer_net)\n",
    "    Predictions = Run_Through_5_layer_Net(Net5,Train_X1 )\n",
    "        \n",
    "#         Bins,Counts_in_bin = Create_Loss_historgram(Predictions,Train_Y1,Plot_Histogram = 0)\n",
    "#         Specialist_test_result = Check_if_The_Net_passes_the_Specialist_test(Bins,Counts_in_bin)\n",
    "#         print(\"Specialist_test_result\",Specialist_test_result)\n",
    "\n",
    "#         print(\" \")\n",
    "#         if Specialist_test_result == 1:\n",
    "#             if math.isnan(Cost1.Cost_Function(Best_Individual)) == 0:\n",
    "\n",
    "#                 break\n",
    "\n",
    "    return  Net5, Best_Individual, Predictions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     Net4,Predictions_Of_Specialist = Perform_Specialist_Training_For_Net(Best_Individual)\n",
    "\n",
    "def Perform_Specialist_Training_For_Net(Best_Individual):\n",
    "    Cost1 = Cost_Function()\n",
    "    Cost2 = Specialist_Cost_Function()\n",
    "\n",
    "    Net5 = Reshape_into_5_layer_net(Best_Individual,Number_Of_Nurons_in_5_layer_net)\n",
    "    Predictions = Run_Through_5_layer_Net(Net5,Train_X1 )\n",
    "    LossVector1 = -(Train_Y1*np.log(Predictions + 10**(-10)) + (1-Train_Y1)*np.log(1-Predictions + 10**(-10)))\n",
    "    Bins,Counts_in_bin = Create_Loss_historgram(Predictions,Train_Y1,Plot_Histogram = 1)\n",
    "\n",
    "    print(\"Regular Cost Before Specialist training: \", Cost1.Cost_Function(Best_Individual))    \n",
    "    print(\"Specialist Cost Before Specialist training: \", Cost2.Cost_Function(Best_Individual))  \n",
    "    print(\" \")\n",
    "    Best_Individual_After_Specialist_Training, CurrentCost = Optimize_With_Pinball_Algorithm(Train_X1 ,Train_Y1,Best_Individual,Cost2,Number_Of_Itterations_For_Specialist_Training,print_Cost,NumberOfSamples,Search_Ratio,AmplitudeOfLinearSearch,AmplitudeOfRandomSearch)\n",
    "\n",
    "    Net5 = Reshape_into_5_layer_net(Best_Individual_After_Specialist_Training,Number_Of_Nurons_in_5_layer_net)\n",
    "    Predictions = Run_Through_5_layer_Net(Net5,Train_X1 )\n",
    "    Bins,Counts_in_bin = Create_Loss_historgram(Predictions,Train_Y1,Plot_Histogram = 1)\n",
    "    print(\"Regular Cost after Specialist training: \", Cost1.Cost_Function(Best_Individual_After_Specialist_Training))    \n",
    "    print(\"Specialist Cost after Specialist training: \", Cost2.Cost_Function(Best_Individual_After_Specialist_Training))  \n",
    "\n",
    "    plt.show()    \n",
    "    ## Saving the predictions from the specialist\n",
    "#     Predictions_Of_Specialist[network,:] = Predictions\n",
    "\n",
    "    return Net5, Best_Individual, Predictions\n",
    "\n",
    "\n",
    "\n",
    "def Train_Confidence_Network():\n",
    "    ## Train Confidence Network\n",
    "    Cost3 = Cost_Function_For_Confidence()\n",
    "    Cost1 = Cost_Function()\n",
    "#     Number_Of_Itterations_For_Confidence_Training = 200\n",
    "\n",
    "    for i in range (0,1):\n",
    "#         Initial_Parameters = np.random.rand(174,1)\n",
    "        Initial_Parameters = np.random.rand(TotalParameters_in_5_layer_net,1)\n",
    "#         CurrentCost = Cost1.Cost_Function(Initial_Parameters)\n",
    "        print(\"Initial Cost is is: \", CurrentCost)\n",
    "\n",
    "        Best_Individual, CurrentCost = Optimize_With_Pinball_Algorithm(Confidence_Train_X ,Confidence_Train_Y,Initial_Parameters,Cost3,Number_Of_Itterations_For_Confidence_Training,print_Cost,NumberOfSamples,Search_Ratio,AmplitudeOfLinearSearch,AmplitudeOfRandomSearch)\n",
    "        print(\"Final_Cost is: \", CurrentCost)\n",
    "\n",
    "\n",
    "        ## This is to test the best Net to see if it passes the test\n",
    "#         Net4_Confidence = Reshape_into_4_layer_net(Best_Individual,Number_Of_Nurons_in_4_layer_net)\n",
    "#         Predictions = Run_Through_4_layer_Net(Net4_Confidence,Confidence_Train_X )\n",
    "        \n",
    "        Net5_Confidence = Reshape_into_5_layer_net(Best_Individual,Number_Of_Nurons_in_5_layer_net)\n",
    "        Predictions = Run_Through_5_layer_Net(Net5_Confidence,Confidence_Train_X )\n",
    "        \n",
    "        Bins,Counts_in_bin = Create_Loss_historgram(Predictions,Confidence_Train_Y,Plot_Histogram = 0)\n",
    "        Specialist_test_result = Check_if_The_Net_passes_the_Specialist_test(Bins,Counts_in_bin)\n",
    "        print(\"Specialist test result For confidence Net\",Specialist_test_result)\n",
    "\n",
    "#         print(\" \")\n",
    "        if Specialist_test_result == 1:\n",
    "            if math.isnan(Cost1.Cost_Function(Best_Individual)) == 0:\n",
    "                break\n",
    "\n",
    "    plt.figure(2)\n",
    "    Bins,Counts_in_bin = Create_Loss_historgram(Predictions,Confidence_Train_Y,Plot_Histogram = 1)\n",
    "\n",
    "#     Confidence_Of_Specialist[network,:] = Predictions\n",
    "\n",
    "    return Net5_Confidence,Best_Individual,Predictions\n",
    "\n",
    "\n",
    "def Train_UnConfidence_Network():       \n",
    "    ## Train UnConfidence Network\n",
    "    Cost3 = Cost_Function_For_UnConfidence()\n",
    "    Cost1 = Cost_Function()\n",
    "#     Number_Of_Itterations_For_Confidence_Training = 200\n",
    "\n",
    "    for i in range (0,1):\n",
    "#         Initial_Parameters = np.random.rand(174,1)\n",
    "        Initial_Parameters = np.random.rand(TotalParameters_in_5_layer_net,1)\n",
    "        \n",
    "#         CurrentCost = Cost1.Cost_Function(Initial_Parameters)\n",
    "        print(\"Initial Cost is is: \", CurrentCost)\n",
    "\n",
    "        Best_Individual, CurrentCost = Optimize_With_Pinball_Algorithm(Confidence_Train_X ,UnConfidence_Train_Y,Initial_Parameters,Cost3,Number_Of_Itterations_For_Confidence_Training,print_Cost,NumberOfSamples,Search_Ratio,AmplitudeOfLinearSearch,AmplitudeOfRandomSearch)\n",
    "        print(\"Final_Cost is: \", CurrentCost)\n",
    "\n",
    "\n",
    "        ## This is to test the best Net to see if it passes the test\n",
    "#         Net4_UnConfidence = Reshape_into_4_layer_net(Best_Individual,Number_Of_Nurons_in_4_layer_net)\n",
    "#         Predictions = Run_Through_4_layer_Net(Net4_UnConfidence,Confidence_Train_X )\n",
    "        \n",
    "        Net5_UnConfidence = Reshape_into_5_layer_net(Best_Individual,Number_Of_Nurons_in_5_layer_net)\n",
    "        Predictions = Run_Through_5_layer_Net(Net5_UnConfidence,Confidence_Train_X )\n",
    "        \n",
    "        Bins,Counts_in_bin = Create_Loss_historgram(Predictions,UnConfidence_Train_Y,Plot_Histogram = 0)\n",
    "        Specialist_test_result = Check_if_The_Net_passes_the_Specialist_test(Bins,Counts_in_bin)\n",
    "        print(\"Specialist test result For Unconfidence Net\",Specialist_test_result)\n",
    "\n",
    "\n",
    "        if Specialist_test_result == 1:\n",
    "            if math.isnan(Cost1.Cost_Function(Best_Individual)) == 0:\n",
    "                break\n",
    "\n",
    "    plt.figure(3)\n",
    "    Bins,Counts_in_bin = Create_Loss_historgram(Predictions,UnConfidence_Train_Y,Plot_Histogram = 1)\n",
    "\n",
    "#     UnConfidence_Of_Specialist[network,:] = Predictions\n",
    "    return Net5_UnConfidence, Best_Individual, Predictions\n",
    "\n",
    "\n",
    "\n",
    "def Generate_Confidence_Data(Net5_s):\n",
    "    ## Gather Training data for Confidence Network\n",
    "    A5 , A4 , A3 ,A2 = Run_Through_5_layer_Net_And_Save_Activations(Net5_s,Train_X1)\n",
    "    Predictions = A5\n",
    "    LossVector1 = -(Train_Y1*np.log(Predictions + 10**(-10)) + (1-Train_Y1)*np.log(1-Predictions + 10**(-10)))\n",
    "\n",
    "    Confidence_Train_X = np.concatenate((A2,A3,A4,A5))\n",
    "    Confidence_Train_Y = LossVector1 < Confidence_Threshold\n",
    "    UnConfidence_Train_Y = LossVector1 > UnConfidence_Threshold\n",
    "\n",
    "    return Confidence_Train_X, Confidence_Train_Y, UnConfidence_Train_Y\n",
    "\n",
    "\n",
    "\n",
    "# def Save_to_CSV(Specialists_Parameters,Specialists_Confidence_Parameters,Specialists_UnConfidence_Parameters,Random_Indexes_,Total_Score,network):\n",
    "def     Save_to_CSV(Specialists_Parameters,Specialists_C1_Parameters,Specialists_C2_Parameters,\\\n",
    "                    Specialists_C3_Parameters,Specialists_C4_Parameters,Specialists_C5_Parameters\\\n",
    "                    ,Specialists_C6_Parameters,Random_Indexes_,Total_Score,network):\n",
    "    \n",
    "        ## Save the parameters of the network\n",
    "        Name_Of_Parameter_File = \"Specialist\" + str (network) + \"_Parameters.csv\"\n",
    "    #     print(Name_Of_Parameter_File )\n",
    "\n",
    "\n",
    "        with open(Name_Of_Parameter_File, 'w') as csvfile:\n",
    "            fieldnames = ['Specialists_Parameters','Specialists_C1_Parameters','Specialists_C2_Parameters',\\\n",
    "                         'Specialists_C3_Parameters','Specialists_C4_Parameters','Specialists_C5_Parameters',\\\n",
    "                          'Specialists_C6_Parameters',]\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "            for i in range(0,Best_Individual_Specialist.shape[0]):\n",
    "                writer.writerow({ 'Specialists_Parameters':Specialists_Parameters[i], \\\n",
    "                                'Specialists_C1_Parameters':Specialists_C1_Parameters[i], \\\n",
    "                                'Specialists_C2_Parameters':Specialists_C2_Parameters[i],\\\n",
    "                                'Specialists_C3_Parameters':Specialists_C3_Parameters[i],\\\n",
    "                                'Specialists_C4_Parameters':Specialists_C4_Parameters[i],\\\n",
    "                                'Specialists_C5_Parameters':Specialists_C5_Parameters[i],\\\n",
    "                                'Specialists_C6_Parameters':Specialists_C6_Parameters[i]})\n",
    "\n",
    "\n",
    "        ## Save the random Indexes\n",
    "        Temp = Random_Indexes_\n",
    "# #         Random_Indexes1 = np.array([int(Temp[0]),int(Temp[1]),int(Temp[2]),int(Temp[3]),int(Temp[4]),\\\n",
    "#                       int(Temp[5]),int(Temp[6]),int(Temp[7]),int(Temp[8]),int(Temp[9]),\\\n",
    "#                         int(Temp[10]),int(Temp[11]),int(Temp[12]),int(Temp[13]),int(Temp[14]),\\\n",
    "#                                    int(Temp[15]),int(Temp[16]),int(Temp[17])])      \n",
    "        Random_Indexes1 = np.array([int(Temp[0]),int(Temp[1]),int(Temp[2]),int(Temp[3]),int(Temp[4]),\\\n",
    "                      int(Temp[5]),int(Temp[6]),int(Temp[7]),int(Temp[8]),int(Temp[9]),\\\n",
    "                        int(Temp[10]),int(Temp[11]),int(Temp[12]),int(Temp[13]),int(Temp[14]),\\\n",
    "                        int(Temp[15]),int(Temp[16]),int(Temp[17]),\\\n",
    "                        int(Temp[18]),int(Temp[19]),int(Temp[20]),int(Temp[21]),int(Temp[22]),\\\n",
    "                        int(Temp[23]),int(Temp[24]),int(Temp[25]),int(Temp[26]),int(Temp[27])])   \n",
    "\n",
    "        Name_Of_Index_File = \"Specialist\" + str (network) + \"_Indexes.csv\"\n",
    "\n",
    "        with open(Name_Of_Index_File, 'w') as csvfile:\n",
    "            fieldnames = ['Indexes']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "            for i in range(0,Random_Indexes1.shape[0]):\n",
    "                writer.writerow({ 'Indexes':Random_Indexes1[i] })\n",
    "\n",
    "\n",
    "        Name_Of_Score_File = \"Specialist\" + str (network) + \"_Score.csv\"\n",
    "\n",
    "        with open(Name_Of_Score_File, 'w') as csvfile:\n",
    "            fieldnames = ['Score']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "    #         for i in range(0,Random_Indexes1.shape[0]):\n",
    "            writer.writerow({ 'Score':Total_Score })           \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Load_Parameters_And_Indexes_Of_Specialist(network):\n",
    "    Name_Of_Parameter_File = \"Specialist\" + str (network) + \"_Parameters.csv\"\n",
    "    Name_Of_Index_File = \"Specialist\" + str (network) + \"_Indexes.csv\"\n",
    "    Name_Of_Score_File = \"Specialist\" + str (network) + \"_Score.csv\"\n",
    "    \n",
    "\n",
    "    ## Open Parameter array\n",
    "    Parameters_Path = r\"C:\\Users\\benjy\\OneDrive\\Brown\\Fall 2018\\Hackathon\\Python files\" \n",
    "    Parameters_Path1 = Parameters_Path + \"\\\\\" + Name_Of_Parameter_File\n",
    "    Parameters_temp = pandas.read_csv(Parameters_Path1)\n",
    "    Parameters_Array = np.array(Parameters_temp)\n",
    "\n",
    "    ## Open Index vector\n",
    "    Index_Path = r\"C:\\Users\\benjy\\OneDrive\\Brown\\Fall 2018\\Hackathon\\Python files\" \n",
    "    Index_Path1 = Index_Path + \"\\\\\" + Name_Of_Index_File\n",
    "    Random_Indexes_temp = pandas.read_csv(Index_Path1)\n",
    "    Random_Indexes1 = np.array(Random_Indexes_temp)\n",
    "\n",
    "    ## Open Score File\n",
    "    Score_Path = r\"C:\\Users\\benjy\\OneDrive\\Brown\\Fall 2018\\Hackathon\\Python files\" \n",
    "    Score_Path1 = Index_Path + \"\\\\\" + Name_Of_Score_File\n",
    "    Score_temp = pandas.read_csv(Score_Path1)\n",
    "    Score1_ = np.array(Score_temp)\n",
    "    \n",
    "    return Parameters_Array,Random_Indexes1,Score1_ \n",
    "\n",
    "\n",
    "\n",
    "def Get_Confident_Specialists_Group_Prediction(X,NumberOfNetworks):\n",
    "    Number_Of_Predictions = X.shape[0]\n",
    "    Predictions_Of_Specialist = np.zeros((NumberOfNetworks,Number_Of_Predictions))\n",
    "    Total_Confidence_Of_Network = np.zeros((NumberOfNetworks,Number_Of_Predictions))\n",
    "    Score_Or_Network = np.zeros((NumberOfNetworks,Number_Of_Predictions))\n",
    "    for network in range (0,NumberOfNetworks):\n",
    "\n",
    "        ## Loading Network parameters from csv\n",
    "        Parameters_Array,Random_Indexes1,Score_ = Load_Parameters_And_Indexes_Of_Specialist(network)\n",
    "\n",
    "        ## Costructing Netwrok objects\n",
    "        Net5 = Reshape_into_5_layer_net(Parameters_Array[:,0],Number_Of_Nurons_in_5_layer_net)\n",
    "        Net5_Confidence = Reshape_into_5_layer_net(Parameters_Array[:,1],Number_Of_Nurons_in_5_layer_net)\n",
    "        Net5_UnConfidence = Reshape_into_5_layer_net(Parameters_Array[:,2],Number_Of_Nurons_in_5_layer_net)\n",
    "\n",
    "        ## Using Only the relavent indexes\n",
    "        X1 = X[:,Random_Indexes1[:,0]].T\n",
    "        print(X1.shape)\n",
    "        ## Getting specialist preditictions and confidence\n",
    "        Predictions_Of_Specialist[network,:],Total_Confidence_Of_Network[network,:] = Get_Predictions_and_Confidence_From_Networks_1(Net5,Net5_Confidence,Net5_UnConfidence,X1)\n",
    "        Score_Or_Network[network,:] = Score_\n",
    "\n",
    "    Confident_Specialists_Group_Prediction = np.sum(Predictions_Of_Specialist*Total_Confidence_Of_Network*Score_Or_Network,0)\\\n",
    "                                                /(np.sum(Total_Confidence_Of_Network*Score_Or_Network,0))\n",
    "\n",
    "    return Confident_Specialists_Group_Prediction\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Rescale_To_Optimal_Threshold_Using_Training_Set(Confident_Specialists_Group_Prediction):\n",
    "    # This is to move it to be between 0 and 1\n",
    "    Confident_Specialists_Group_Prediction = Confident_Specialists_Group_Prediction- np.min(Confident_Specialists_Group_Prediction)\n",
    "    Confident_Specialists_Group_Prediction = Confident_Specialists_Group_Prediction/np.max(Confident_Specialists_Group_Prediction)\n",
    "\n",
    "    Min_Subtraction_Term = np.min(Confident_Specialists_Group_Prediction)\n",
    "    Max_Division_Term = np.max(Confident_Specialists_Group_Prediction)\n",
    "\n",
    "    Accuracy = np.zeros(10)\n",
    "    delta = 0.1\n",
    "    BestThreshold = 0.5\n",
    "\n",
    "    for rescan in range(0,2):\n",
    "#         print(1)\n",
    "        delta = 0.1\n",
    "        for j in range(0,10):\n",
    "            Threshold_Prediction = np.linspace(BestThreshold - delta,BestThreshold + delta,11)\n",
    "    #         Accuracy = np.zeros(10)\n",
    "            for i in range (0,10):\n",
    "                Accuracy[i] = np.sum(Test_Y == (Confident_Specialists_Group_Prediction > Threshold_Prediction[i]))/Test_Y.shape[0] \n",
    "\n",
    "            index = np.argmax(Accuracy)\n",
    "            BestThreshold = Threshold_Prediction[index]\n",
    "            delta = delta/2\n",
    "\n",
    "\n",
    "    for i in range(0,Confident_Specialists_Group_Prediction.shape[0]):\n",
    "        if Confident_Specialists_Group_Prediction[i] < BestThreshold:\n",
    "            Confident_Specialists_Group_Prediction[i] = (Confident_Specialists_Group_Prediction[i] - BestThreshold)*(0.5/BestThreshold) + 0.5\n",
    "\n",
    "        if Confident_Specialists_Group_Prediction[i] > BestThreshold:\n",
    "            Confident_Specialists_Group_Prediction[i] = (Confident_Specialists_Group_Prediction[i] - BestThreshold)*(0.5/(1-BestThreshold))  + 0.5\n",
    "\n",
    "#     print(np.sum(Test_Y == (Confident_Specialists_Group_Prediction > 0.5))/Test_Y.shape[0] )   \n",
    "\n",
    "\n",
    "\n",
    "    Confident_Specialists_Group_Prediction_Power = ((Confident_Specialists_Group_Prediction*2)**10)/2\n",
    "    Max_Prediction = np.max(Confident_Specialists_Group_Prediction_Power)-0.5\n",
    "\n",
    "    for i in range(0,Confident_Specialists_Group_Prediction.shape[0]):\n",
    "        if Confident_Specialists_Group_Prediction_Power[i] > 0.5:\n",
    "            Confident_Specialists_Group_Prediction_Power[i] = ((Confident_Specialists_Group_Prediction_Power[i]-0.5)/(2*Max_Prediction)) + 0.5\n",
    "\n",
    "\n",
    "    Rescaled_Confident_Specialists_Group_Prediction = Confident_Specialists_Group_Prediction_Power\n",
    "    return Rescaled_Confident_Specialists_Group_Prediction, BestThreshold, Min_Subtraction_Term, Max_Division_Term\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Rescale_Test_Predictions(Confident_Specialists_Group_Prediction,BestThreshold, Min_Subtraction_Term, Max_Division_Term):\n",
    "    \n",
    "\n",
    "    # This is to move it to be between 0 and 1\n",
    "    Confident_Specialists_Group_Prediction = Confident_Specialists_Group_Prediction- Min_Subtraction_Term\n",
    "    Confident_Specialists_Group_Prediction = Confident_Specialists_Group_Prediction/Max_Division_Term\n",
    "    \n",
    "    ## This is to adjust for learned threshold\n",
    "    for i in range(0,Confident_Specialists_Group_Prediction.shape[0]):\n",
    "        if Confident_Specialists_Group_Prediction[i] < BestThreshold:\n",
    "            Confident_Specialists_Group_Prediction[i] = (Confident_Specialists_Group_Prediction[i] - BestThreshold)*(0.5/BestThreshold) + 0.5\n",
    "\n",
    "        if Confident_Specialists_Group_Prediction[i] > BestThreshold:\n",
    "            Confident_Specialists_Group_Prediction[i] = (Confident_Specialists_Group_Prediction[i] - BestThreshold)*(0.5/(1-BestThreshold))  + 0.5\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "    ## This is increse the prediction contrast\n",
    "    Confident_Specialists_Group_Prediction_Power = ((Confident_Specialists_Group_Prediction*2)**10)/2\n",
    "    Max_Prediction = np.max(Confident_Specialists_Group_Prediction_Power)-0.5\n",
    "\n",
    "    for i in range(0,Confident_Specialists_Group_Prediction.shape[0]):\n",
    "        if Confident_Specialists_Group_Prediction_Power[i] > 0.5:\n",
    "            Confident_Specialists_Group_Prediction_Power[i] = ((Confident_Specialists_Group_Prediction_Power[i]-0.5)/(2*Max_Prediction)) + 0.5\n",
    "\n",
    "    Rescaled_Confident_Specialists_Group_Prediction = Confident_Specialists_Group_Prediction_Power\n",
    "    return Rescaled_Confident_Specialists_Group_Prediction\n",
    "\n",
    "\n",
    "\n",
    "def Find_Optimal_Threshold_Using_Training_Set(Confident_Specialists_Group_Prediction,Y):\n",
    "    # This is to move it to be between 0 and 1\n",
    "    Confident_Specialists_Group_Prediction = Confident_Specialists_Group_Prediction- np.min(Confident_Specialists_Group_Prediction)\n",
    "    Confident_Specialists_Group_Prediction = Confident_Specialists_Group_Prediction/np.max(Confident_Specialists_Group_Prediction)\n",
    "\n",
    "    Min_Subtraction_Term = np.min(Confident_Specialists_Group_Prediction)\n",
    "    Max_Division_Term = np.max(Confident_Specialists_Group_Prediction)\n",
    "\n",
    "    Accuracy = np.zeros(10)\n",
    "    delta = 0.1\n",
    "    BestThreshold = 0.5\n",
    "\n",
    "#     print(Y.shape)\n",
    "#     print(Confident_Specialists_Group_Prediction.shape)\n",
    "    for rescan in range(0,2):\n",
    "#         print(1)\n",
    "        delta = 0.1\n",
    "        for j in range(0,10):\n",
    "            Threshold_Prediction = np.linspace(BestThreshold - delta,BestThreshold + delta,11)\n",
    "    #         Accuracy = np.zeros(10)\n",
    "            for i in range (0,10):\n",
    "                Accuracy[i] = np.sum(Y == (Confident_Specialists_Group_Prediction > Threshold_Prediction[i]))/Confident_Specialists_Group_Prediction.shape[0] \n",
    "\n",
    "            index = np.argmax(Accuracy)\n",
    "            BestThreshold = Threshold_Prediction[index]\n",
    "            delta = delta/2\n",
    "\n",
    "\n",
    "    print(\"Best Threshold is\",BestThreshold)\n",
    "    print(\"Best Accuracy is:\",np.max(Accuracy))\n",
    "\n",
    "    return BestThreshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def Create_Roc_Curve(Predictions):\n",
    "\n",
    "    ## Generating ROC\n",
    "    n1 = 0\n",
    "    n2 = 0\n",
    "    Positive_Predictions = np.zeros(Test_Y.shape[0])\n",
    "    Negative_Predictions = np.zeros(Test_Y.shape[0])\n",
    "    print(Predictions.shape)\n",
    "\n",
    "    for i in range (0,Test_Y.shape[0]):\n",
    "        if Test_Y[i] == 1:\n",
    "            Positive_Predictions[n1] = Predictions[i]\n",
    "            n1 += 1\n",
    "        if Test_Y[i] == 0:\n",
    "            Negative_Predictions[n2] = Predictions[i]\n",
    "            n2 += 1\n",
    "\n",
    "\n",
    "    Bins,Counts_in_bin_Positive = Create_Predictions_historgram(Positive_Predictions)\n",
    "    Bins,Counts_in_bin_Negative = Create_Predictions_historgram(Negative_Predictions)\n",
    "    plt.legend([\"Background\",\"Signal\"])\n",
    "\n",
    "    plt.figure(3)\n",
    "    plt.plot(Counts_in_bin_Positive,1-Counts_in_bin_Negative)\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.xlabel(\"Signal\")\n",
    "    plt.ylabel(\"Background\")        \n",
    "\n",
    "    return Counts_in_bin_Positive,1-Counts_in_bin_Negative\n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "def   Create_Predictions_historgram(Predictions):\n",
    "    \n",
    "\n",
    "    NumberOfBins = 100\n",
    "    Bins = np.linspace(0,1,NumberOfBins)\n",
    "    Counts_in_bin = np.zeros(NumberOfBins)\n",
    "    \n",
    "\n",
    "    for i in range (0,Predictions.shape[0]):\n",
    "        for j in range (0,len(Bins)-1):\n",
    "            if Predictions[i] > Bins[j] :\n",
    "                if Predictions[i] <= Bins[j+1]:\n",
    "                    Counts_in_bin[j] = Counts_in_bin[j] + 1\n",
    "                    \n",
    "    Counts_in_bin = Counts_in_bin/np.max(Counts_in_bin)\n",
    "    \n",
    "\n",
    "    plt.plot(Bins,Counts_in_bin)\n",
    "    plt.title(\"Predictions Distribution\")\n",
    "    \n",
    "    return Bins,Counts_in_bin\n",
    "\n",
    "\n",
    "def Plot_ROC_And_Claculate_AUC(Test_Y,Final_Predictions):\n",
    "\n",
    "    import sklearn\n",
    "    from sklearn import metrics\n",
    "\n",
    "    y_true = Test_Y\n",
    "    y_score = Final_Predictions\n",
    "\n",
    "    AUC = sklearn.metrics.roc_auc_score(y_true, y_score)\n",
    "    print(\"AUC is:\",AUC)\n",
    "\n",
    "\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    n_classes = 1\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = sklearn.metrics.roc_curve(y_true, y_score)\n",
    "        roc_auc[i] = sklearn.metrics.auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = sklearn.metrics.roc_curve(y_true.ravel(), y_score.ravel())\n",
    "    roc_auc[\"micro\"] = sklearn.metrics.auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "\n",
    "\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"], color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[\"micro\"])\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27,)\n"
     ]
    }
   ],
   "source": [
    "import array\n",
    "array.array('i')\n",
    "a = np.array(array.array('i',(i for i in range(0,27))))\n",
    "\n",
    "\n",
    "## This is the Main Window For training the nets\n",
    "NumberOfSamples = 30\n",
    "Search_Ratio = 0.7;\n",
    "AmplitudeOfLinearSearch = 0.5;\n",
    "AmplitudeOfRandomSearch = 0.9;\n",
    "print_Cost = 0\n",
    "\n",
    "\n",
    "## Total length 400000\n",
    "Number_Of_Itterations = 10\n",
    "\n",
    "NumberOfNetworks = 60\n",
    "\n",
    "UnConfidence_Threshold = 0.7 ## This corresponds with about 50 percent\n",
    "Confidence_Threshold = 0.4\n",
    "Specialist_test_Threshold = 0.4\n",
    "\n",
    "Random_Indexes = np.ones((NumberOfNetworks,28))-1\n",
    "\n",
    "Random_Indexes1 = a\n",
    "\n",
    "# Random_Indexes1 = Generate_Random_Indexes()    \n",
    "\n",
    "Random_Indexes1\n",
    "print(Random_Indexes1.shape)\n",
    "## This is to train on a smaller dataset\n",
    "Train_X1 = Train_X[:,Random_Indexes1].T    \n",
    "Train_Y1 = Train_Y[:]\n",
    "    \n",
    "    \n",
    "Predictions_Of_Specialist = np.zeros((NumberOfNetworks,Train_X1.shape[1]))\n",
    "Confidence_Of_Specialist = np.zeros((NumberOfNetworks,Train_X1.shape[1]))\n",
    "UnConfidence_Of_Specialist = np.zeros((NumberOfNetworks,Train_X1.shape[1]))\n",
    "\n",
    "\n",
    "# NumberOf_4_Net_Parameters = 174\n",
    "NumberOf_5_Net_Parameters = TotalParameters_in_5_layer_net\n",
    "\n",
    "Specialists_Parameters = np.zeros((NumberOf_5_Net_Parameters,NumberOfNetworks))\n",
    "Specialists_C1_Parameters  = np.zeros((NumberOf_5_Net_Parameters,NumberOfNetworks))\n",
    "Specialists_C2_Parameters  = np.zeros((NumberOf_5_Net_Parameters,NumberOfNetworks))\n",
    "Specialists_C3_Parameters  = np.zeros((NumberOf_5_Net_Parameters,NumberOfNetworks))\n",
    "Specialists_C4_Parameters  = np.zeros((NumberOf_5_Net_Parameters,NumberOfNetworks))\n",
    "Specialists_C5_Parameters  = np.zeros((NumberOf_5_Net_Parameters,NumberOfNetworks))\n",
    "Specialists_C6_Parameters  = np.zeros((NumberOf_5_Net_Parameters,NumberOfNetworks))\n",
    "\n",
    "\n",
    "Train_Network = 0\n",
    "if Train_Network == 1:\n",
    "    for network in range (34,NumberOfNetworks):\n",
    "\n",
    "\n",
    "        print(\"Creating Net Number : \",network  )\n",
    "        print(\" \")\n",
    "        print(\" \")   \n",
    "        Random_Indexes1 = Generate_Random_Indexes()    \n",
    "        Random_Indexes[network,:] =  Random_Indexes1 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        def Train_Confidence_Network(Confidence_Train_X_,Confidence_Train_Y_,Killing_Vector):\n",
    "\n",
    "            Cost3 = Cost_Function_For_Confidence()\n",
    "\n",
    "            Initial_Parameters = 0.001*np.random.rand(TotalParameters_in_5_layer_net,1)\n",
    "            CurrentCost = Cost3.Cost_Function(Initial_Parameters,Confidence_Train_X_,Confidence_Train_Y_,Killing_Vector)\n",
    "            print(\"Initial Cost is is: \", CurrentCost)\n",
    "\n",
    "            Best_Individual, CurrentCost = Optimize_With_Pinball_Algorithm(Confidence_Train_X_ ,Confidence_Train_Y_,Initial_Parameters,Cost3,Killing_Vector,Number_Of_Itterations_For_Confidence_Training,print_Cost,NumberOfSamples,Search_Ratio,AmplitudeOfLinearSearch,AmplitudeOfRandomSearch)\n",
    "            print(\"Final_Cost is: \", CurrentCost)\n",
    "\n",
    "            Net5_Confidence = Reshape_into_5_layer_net(Best_Individual,Number_Of_Nurons_in_5_layer_net)\n",
    "            Predictions = Run_Through_5_layer_Net(Net5_Confidence,Confidence_Train_X_ )\n",
    "\n",
    "\n",
    "\n",
    "            return Net5_Confidence,Best_Individual,Predictions\n",
    "\n",
    "\n",
    "\n",
    "        class Cost_Function_For_Confidence:\n",
    "            def Cost_Function(self,Individuals_Parameter_Vector,Confidence_Train_X_,Confidence_Train_Y_,Killing_Vector): \n",
    "\n",
    "                NumberOfSamples = Individuals_Parameter_Vector.shape[1]\n",
    "                Cost_Function1 = np.zeros(NumberOfSamples)\n",
    "\n",
    "                for i in range (0,NumberOfSamples):\n",
    "\n",
    "                    Net_C = Reshape_into_5_layer_net(Individuals_Parameter_Vector[:,i],Number_Of_Nurons_in_5_layer_net)\n",
    "                    Predictions = Run_Through_5_layer_Net(Net_C,Confidence_Train_X_ )\n",
    "                    Cost_Function1[i] = Calculate_Loss_For_Confidence(Predictions,Confidence_Train_Y_,Killing_Vector)\n",
    "\n",
    "    #             print(Predictions)\n",
    "                return Cost_Function1   \n",
    "\n",
    "\n",
    "        def   Calculate_Loss_For_Confidence(Predictions,Y,Killing_Vector):\n",
    "\n",
    "            return -(1/np.sum(Killing_Vector))*np.sum((Y*np.log(Predictions+ 10**(-10)) + (1-Y)*np.log(1-Predictions+ 10**(-10)))*Killing_Vector)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        def Make_True_False_Positive_negative_datasets(Predictions,Y,Size_Of_Confience_Dataset):\n",
    "\n",
    "            Number_Of_test_Samples = len(Y)\n",
    "\n",
    "\n",
    "            True_Positive_KillingVector = np.zeros(Number_Of_test_Samples)\n",
    "            False_Positive_KillingVector = np.zeros(Number_Of_test_Samples)\n",
    "            True_Negative_KillingVector = np.zeros(Number_Of_test_Samples)\n",
    "            False_Negative_KillingVector = np.zeros(Number_Of_test_Samples)\n",
    "\n",
    "\n",
    "            Positive_Dataset_Level1 = np.zeros(Number_Of_test_Samples)\n",
    "            Negative_Dataset_Level1 = np.zeros(Number_Of_test_Samples)\n",
    "\n",
    "            for i in range (0,Number_Of_test_Samples):\n",
    "                if Predictions[i]>0.5 and Y[i] == 1:\n",
    "                    True_Positive_KillingVector[i] = 1\n",
    "                    Positive_Dataset_Level1[i] = 1\n",
    "\n",
    "                if Predictions[i]>0.5 and Y[i] == 0:\n",
    "                    False_Positive_KillingVector[i] = 1\n",
    "\n",
    "\n",
    "                if Predictions[i]<0.5 and Y[i] == 1:\n",
    "                    False_Negative_KillingVector[i] = 1\n",
    "\n",
    "\n",
    "                if Predictions[i]<0.5 and Y[i] == 0:\n",
    "                    True_Negative_KillingVector[i] = 1\n",
    "                    Negative_Dataset_Level1[i] = 1\n",
    "\n",
    "\n",
    "\n",
    "            for i in range (0,Number_Of_test_Samples):\n",
    "                if True_Positive_KillingVector[i] == 1.0 and np.sum(True_Positive_KillingVector) > Size_Of_Confience_Dataset/2:\n",
    "                    True_Positive_KillingVector[i] = 0.0\n",
    "\n",
    "                if False_Positive_KillingVector[i] == 1.0 and np.sum(False_Positive_KillingVector) > Size_Of_Confience_Dataset/2:\n",
    "                    False_Positive_KillingVector[i] = 0.0\n",
    "\n",
    "\n",
    "            Size_Of_Confience_Dataset = 50\n",
    "            for i in range (0,Number_Of_test_Samples):\n",
    "                if True_Negative_KillingVector[i] == 1.0 and np.sum(True_Negative_KillingVector) > Size_Of_Confience_Dataset/2:\n",
    "                    True_Negative_KillingVector[i] = 0.0\n",
    "\n",
    "                if False_Negative_KillingVector[i] == 1.0 and np.sum(False_Negative_KillingVector) > Size_Of_Confience_Dataset/2:\n",
    "                    False_Negative_KillingVector[i] = 0.0\n",
    "\n",
    "\n",
    "\n",
    "            Total_Positive_Killing_Vector = True_Positive_KillingVector + False_Positive_KillingVector\n",
    "            Total_Negative_Killing_Vector = True_Negative_KillingVector + False_Negative_KillingVector\n",
    "\n",
    "\n",
    "            return Positive_Dataset_Level1, Negative_Dataset_Level1, Total_Positive_Killing_Vector, Total_Negative_Killing_Vector\n",
    "\n",
    "\n",
    "        Size_Of_Confience_Dataset = 300    \n",
    "        Number_Of_test_Samples1 = 50\n",
    "        Predictions = np.random.rand(Number_Of_test_Samples1)\n",
    "        Y = np.random.rand(Number_Of_test_Samples1) >0.5\n",
    "\n",
    "        Positive_Dataset_Level1, Negative_Dataset_Level1, Total_Positive_Killing_Vector, Total_Negative_Killing_Vector = Make_True_False_Positive_negative_datasets(Predictions,Y,Size_Of_Confience_Dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ## This is to train on a smaller dataset\n",
    "        Train_X1 = Train_X[500*(10*np.mod(network,200)):500*((10*np.mod(network,200)+1)),Random_Indexes1].T    \n",
    "        Train_Y1 = Train_Y[500*(10*np.mod(network,200)):500*((10*np.mod(network,200)+1))]\n",
    "\n",
    "        Initial_Parameters = np.random.rand(TotalParameters_in_5_layer_net,1)\n",
    "            ## Train Basic Network\n",
    "        Confidence_Training = 0\n",
    "        Number_Of_Itterations = 2000\n",
    "        Net5, Best_Individual, Predictions = Geterate_Net_With_Basic_Training()\n",
    "        Best_Individual_Specialist = Best_Individual\n",
    "        Killing_Vector = np.ones(Predictions.shape)\n",
    "        Bins,Counts_in_bin = Create_Loss_historgram(Predictions,Train_Y1,Killing_Vector,Plot_Histogram = 1)\n",
    "\n",
    "\n",
    "        plt.show()  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ## Confidence_Test - 1\n",
    "\n",
    "\n",
    "        Confidence_Test = 0\n",
    "        Number_Of_Samples_In_Confidence_Test = 50\n",
    "\n",
    "        N_1 = 400*(np.mod(network,200)+1) + 3*Number_Of_Samples_In_Confidence_Test*(np.mod(network,200)) + Confidence_Test*Number_Of_Samples_In_Confidence_Test\n",
    "        N_2 = 400*(np.mod(network,200)+1) + 3*Number_Of_Samples_In_Confidence_Test*(np.mod(network,200)) + (Confidence_Test+1)*Number_Of_Samples_In_Confidence_Test\n",
    "\n",
    "        Confidence_Test_X1 = Train_X[N_1:N_2,Random_Indexes1].T    \n",
    "        Confidence_Test_Y1 = Train_Y[N_1:N_2]\n",
    "\n",
    "\n",
    "\n",
    "        ## Creating Confidence Dataset - for C1, C4\n",
    "        A5 , A4 , A3 ,A2 = Run_Through_5_layer_Net_And_Save_Activations(Net5,Confidence_Test_X1)\n",
    "        Predictions = A5\n",
    "    #     Activations = np.concatenate((A2,A3,A4,A5))\n",
    "        Activations = np.concatenate((A2[0:10],A3))\n",
    "        Confidence_Training_X = Activations\n",
    "\n",
    "\n",
    "    #     Size_Of_Confience_Dataset = 50\n",
    "        Y = Confidence_Test_Y1\n",
    "        Positive_efficiency = (np.sum((Y == 1)*(Predictions>0.5))/np.sum(Y == 1))\n",
    "        Negative_efficiency = (np.sum((Y == 0)*(Predictions<0.5))/np.sum(Y == 0))\n",
    "        print(\"Positive_efficiency\",Positive_efficiency)\n",
    "        print(\"Negative_efficiency\",Negative_efficiency)\n",
    "\n",
    "        Positive_Dataset_Level1, Negative_Dataset_Level1, Total_Positive_Killing_Vector, Total_Negative_Killing_Vector = Make_True_False_Positive_negative_datasets(Predictions[0,:],Y,Size_Of_Confience_Dataset)\n",
    "        Confidence_Training = 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        Number_Of_Itterations_For_Confidence_Training = 1000\n",
    "\n",
    "        ## Traing the level 1 networks\n",
    "\n",
    "        ## Train C1 for positive Predictions - Y = Positive_Dataset_Level1\n",
    "        Net5_Confidence_C1,Best_C1_Parameters,Predictions1 = Train_Confidence_Network(Confidence_Training_X,Positive_Dataset_Level1,Total_Positive_Killing_Vector)\n",
    "\n",
    "        a =(Positive_Dataset_Level1*Total_Positive_Killing_Vector)\n",
    "        b = (Predictions1*Total_Positive_Killing_Vector)[0,:] > 0.5 \n",
    "        print(\"The C1 network accuracy is \", np.sum((a == b)*Total_Positive_Killing_Vector)/np.sum(Total_Positive_Killing_Vector))\n",
    "\n",
    "        plt.figure(2)\n",
    "        Bins,Counts_in_bin = Create_Loss_historgram(Predictions1,Positive_Dataset_Level1,Total_Positive_Killing_Vector,Plot_Histogram = 1)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        ## Train C4 for positive Predictions - Y = Positive_Dataset_Level1\n",
    "        Net5_Confidence_C4, Best_C4_Parameters, Predictions4 = Train_Confidence_Network(Confidence_Training_X,Negative_Dataset_Level1,Total_Negative_Killing_Vector)\n",
    "\n",
    "        a =(Negative_Dataset_Level1*Total_Negative_Killing_Vector)\n",
    "        b = (Predictions4*Total_Negative_Killing_Vector)[0,:] > 0.5 \n",
    "        print(\"The C4 network accuracy is \", np.sum((a == b)*Total_Negative_Killing_Vector)/np.sum(Total_Negative_Killing_Vector))\n",
    "\n",
    "        plt.figure(3)\n",
    "        Bins,Counts_in_bin = Create_Loss_historgram(Predictions4,Negative_Dataset_Level1,Total_Negative_Killing_Vector,Plot_Histogram = 1)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ## Confidence_Test - 2\n",
    "\n",
    "        Confidence_Test = 1\n",
    "\n",
    "\n",
    "        N_1 = 400*(np.mod(network,200)+1) + 3*Number_Of_Samples_In_Confidence_Test*(np.mod(network,200)) + Confidence_Test*Number_Of_Samples_In_Confidence_Test\n",
    "        N_2 = 400*(np.mod(network,200)+1) + 3*Number_Of_Samples_In_Confidence_Test*(np.mod(network,200)) + (Confidence_Test+1)*Number_Of_Samples_In_Confidence_Test\n",
    "\n",
    "        Confidence_Test_X1 = Train_X[N_1:N_2,Random_Indexes1].T    \n",
    "        Confidence_Test_Y1 = Train_Y[N_1:N_2]\n",
    "\n",
    "\n",
    "        ## Creating Confidence Dataset - for C1, C4\n",
    "        A5 , A4 , A3 ,A2 = Run_Through_5_layer_Net_And_Save_Activations(Net5,Confidence_Test_X1)\n",
    "        Predictions = A5\n",
    "    #     Activations = np.concatenate((A2,A3,A4,A5))\n",
    "        Activations = np.concatenate((A2[0:10],A3))\n",
    "        Confidence_Training_X = Activations\n",
    "\n",
    "\n",
    "        Y = Confidence_Test_Y1\n",
    "\n",
    "        Positive_Dataset_Level2, Negative_Dataset_Level2, Total_Positive_Killing_Vector, Total_Negative_Killing_Vector = Make_True_False_Positive_negative_datasets(Predictions[0,:],Y,Size_Of_Confience_Dataset)\n",
    "        Confidence_Training = 1\n",
    "\n",
    "\n",
    "        ## Train C2 for positive Predictions - Y = Positive_Dataset_Level1\n",
    "        Net5_Confidence_C2, Best_C2_Parameters ,Predictions2 = Train_Confidence_Network(Confidence_Training_X,Positive_Dataset_Level2,Total_Positive_Killing_Vector)\n",
    "\n",
    "        a =(Positive_Dataset_Level2*Total_Positive_Killing_Vector)\n",
    "        b = (Predictions2*Total_Positive_Killing_Vector)[0,:] > 0.5 \n",
    "        print(\"The C2 network accuracy is \", np.sum((a == b)*Total_Positive_Killing_Vector)/np.sum(Total_Positive_Killing_Vector))\n",
    "\n",
    "        plt.figure(4)\n",
    "        Bins,Counts_in_bin = Create_Loss_historgram(Predictions2,Positive_Dataset_Level2,Total_Positive_Killing_Vector,Plot_Histogram = 1)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        ## Train C5 for positive Predictions - Y = Positive_Dataset_Level1\n",
    "        Net5_Confidence_C5, Best_C5_Parameters,Predictions5 = Train_Confidence_Network(Confidence_Training_X,Negative_Dataset_Level2,Total_Negative_Killing_Vector)\n",
    "\n",
    "        a =(Negative_Dataset_Level2*Total_Negative_Killing_Vector)\n",
    "        b = (Predictions5*Total_Negative_Killing_Vector)[0,:] > 0.5 \n",
    "        print(\"The C5 network accuracy is \", np.sum((a == b)*Total_Negative_Killing_Vector)/np.sum(Total_Negative_Killing_Vector))\n",
    "\n",
    "        plt.figure(5)\n",
    "        Bins,Counts_in_bin = Create_Loss_historgram(Predictions5,Negative_Dataset_Level2,Total_Negative_Killing_Vector,Plot_Histogram = 1)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ## Confidence_Test - 3\n",
    "\n",
    "        Confidence_Test = 2\n",
    "\n",
    "\n",
    "        N_1 = 400*(np.mod(network,200)+1) + 3*Number_Of_Samples_In_Confidence_Test*(np.mod(network,200)) + Confidence_Test*Number_Of_Samples_In_Confidence_Test\n",
    "        N_2 = 400*(np.mod(network,200)+1) + 3*Number_Of_Samples_In_Confidence_Test*(np.mod(network,200)) + (Confidence_Test+1)*Number_Of_Samples_In_Confidence_Test\n",
    "\n",
    "        Confidence_Test_X1 = Train_X[N_1:N_2,Random_Indexes1].T    \n",
    "        Confidence_Test_Y1 = Train_Y[N_1:N_2]\n",
    "\n",
    "\n",
    "        ## Creating Confidence Dataset - for C3, C6\n",
    "        A5 , A4 , A3 ,A2 = Run_Through_5_layer_Net_And_Save_Activations(Net5,Confidence_Test_X1)\n",
    "        Predictions = A5\n",
    "    #     Activations = np.concatenate((A2,A3,A4,A5))\n",
    "        Activations = np.concatenate((A2[0:10],A3))\n",
    "        Confidence_Training_X = Activations\n",
    "\n",
    "\n",
    "\n",
    "        Y = Confidence_Test_Y1\n",
    "\n",
    "        Positive_Dataset_Level3, Negative_Dataset_Level3, Total_Positive_Killing_Vector, Total_Negative_Killing_Vector = Make_True_False_Positive_negative_datasets(Predictions[0,:],Y,Size_Of_Confience_Dataset)\n",
    "        Confidence_Training = 1\n",
    "\n",
    "\n",
    "        ## Train C3 for positive Predictions - Y = Positive_Dataset_Level1\n",
    "        Net5_Confidence_C3,Best_C3_Parameters,Predictions3 = Train_Confidence_Network(Confidence_Training_X,Positive_Dataset_Level3,Total_Positive_Killing_Vector)\n",
    "\n",
    "        a =(Positive_Dataset_Level3*Total_Positive_Killing_Vector)\n",
    "        b = (Predictions3*Total_Positive_Killing_Vector)[0,:] > 0.5 \n",
    "        print(\"The C3 network accuracy is \", np.sum((a == b)*Total_Positive_Killing_Vector)/np.sum(Total_Positive_Killing_Vector))\n",
    "\n",
    "        plt.figure(6)\n",
    "        Bins,Counts_in_bin = Create_Loss_historgram(Predictions2,Positive_Dataset_Level3,Total_Positive_Killing_Vector,Plot_Histogram = 1)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        ## Train C6 for positive Predictions - Y = Positive_Dataset_Level1\n",
    "        Net5_Confidence_C6,Best_C6_Parameters,Predictions6 = Train_Confidence_Network(Confidence_Training_X,Negative_Dataset_Level3,Total_Negative_Killing_Vector)\n",
    "\n",
    "        a =(Negative_Dataset_Level3*Total_Negative_Killing_Vector)\n",
    "        b = (Predictions6*Total_Negative_Killing_Vector)[0,:] > 0.5 \n",
    "        print(\"The C6 network accuracy is \", np.sum((a == b)*Total_Negative_Killing_Vector)/np.sum(Total_Negative_Killing_Vector))\n",
    "\n",
    "        plt.figure(7)\n",
    "        Bins,Counts_in_bin = Create_Loss_historgram(Predictions5,Negative_Dataset_Level3,Total_Negative_Killing_Vector,Plot_Histogram = 1)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    #     Total_Score = Score1*Score2*Score3\n",
    "        Total_Score = 1\n",
    "        print(\"Score Of Network is:\",Total_Score)\n",
    "       ## Saving_Parameters \n",
    "\n",
    "        Specialists_Parameters[:,network] = Best_Individual_Specialist[:,0]\n",
    "        Specialists_C1_Parameters[:,network] = Best_C1_Parameters[:,0]\n",
    "        Specialists_C2_Parameters[:,network] = Best_C2_Parameters[:,0]\n",
    "        Specialists_C3_Parameters[:,network] = Best_C3_Parameters[:,0]\n",
    "        Specialists_C4_Parameters[:,network] = Best_C4_Parameters[:,0]\n",
    "        Specialists_C5_Parameters[:,network] = Best_C5_Parameters[:,0]\n",
    "        Specialists_C6_Parameters[:,network] = Best_C6_Parameters[:,0]\n",
    "\n",
    "        Save_to_CSV(Specialists_Parameters[:,network],Specialists_C1_Parameters[:,network],Specialists_C2_Parameters[:,network],\\\n",
    "                    Specialists_C3_Parameters[:,network],Specialists_C4_Parameters[:,network],Specialists_C5_Parameters[:,network]\\\n",
    "                    ,Specialists_C6_Parameters[:,network],Random_Indexes[network,:],Total_Score,network)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Finished Training All The Networks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of networks is: 50\n",
      "CV results\n",
      "Getting predictions from netwrok number 0 / 50\n",
      "\n",
      "Getting predictions from netwrok number 1 / 50\n",
      "\n",
      "Getting predictions from netwrok number 2 / 50\n",
      "\n",
      "Getting predictions from netwrok number 3 / 50\n",
      "\n",
      "Getting predictions from netwrok number 4 / 50\n",
      "\n",
      "Getting predictions from netwrok number 5 / 50\n",
      "\n",
      "Getting predictions from netwrok number 6 / 50\n",
      "\n",
      "Getting predictions from netwrok number 7 / 50\n",
      "\n",
      "Getting predictions from netwrok number 8 / 50\n",
      "\n",
      "Getting predictions from netwrok number 9 / 50\n",
      "\n",
      "Getting predictions from netwrok number 10 / 50\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-08b4abbb1ce0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m \u001b[0mConfident_Specialists_Group_Prediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGet_Confident_Specialists_Group_Prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCV_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNumberOfNetworks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;31m# Plot ROC\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-08b4abbb1ce0>\u001b[0m in \u001b[0;36mGet_Confident_Specialists_Group_Prediction\u001b[1;34m(X, NumberOfNetworks)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;31m#         print(Random_Indexes1.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;31m## Getting specialist preditictions and confidence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mPredictions_Of_Specialist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTotal_Confidence_Of_Network\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGet_Predictions_and_Confidence_From_Networks_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNet5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNet5_C1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNet5_C2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNet5_C3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNet5_C4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNet5_C5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNet5_C6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m         \u001b[0mScore_Or_Network\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mScore_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-08b4abbb1ce0>\u001b[0m in \u001b[0;36mGet_Predictions_and_Confidence_From_Networks_1\u001b[1;34m(Net5, Net5_C1, Net5_C2, Net5_C3, Net5_C4, Net5_C5, Net5_C6, X)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mGet_Predictions_and_Confidence_From_Networks_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNet5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNet5_C1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNet5_C2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNet5_C3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNet5_C4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNet5_C5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNet5_C6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     \u001b[0mA5\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mA4\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mA3\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mA2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRun_Through_5_layer_Net_And_Save_Activations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNet5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[0mPredictions_Of_Specialist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mA5\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-0743db8989e5>\u001b[0m in \u001b[0;36mRun_Through_5_layer_Net_And_Save_Activations\u001b[1;34m(Net, X)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mZ3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mA2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mNet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m#         A3 = np.maximum(Z3,0) + 0.1*np.minimum(Z3,0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mA3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1j\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimag\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimag\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mZ4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mA3\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mNet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb4\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## This is to test the networks\n",
    "NumberOfNetworks =50\n",
    "\n",
    "# NumberOfNetworks = 360\n",
    "print(\"Number of networks is:\",NumberOfNetworks)\n",
    "print(\"CV results\")\n",
    "# Get Predictions For test data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Get_Confident_Specialists_Group_Prediction(X,NumberOfNetworks):\n",
    "    Number_Of_Predictions = X.shape[0]\n",
    "    Predictions_Of_Specialist = np.zeros((NumberOfNetworks,Number_Of_Predictions))\n",
    "    Total_Confidence_Of_Network = np.zeros((NumberOfNetworks,Number_Of_Predictions))\n",
    "    Score_Or_Network = np.zeros((NumberOfNetworks,Number_Of_Predictions))\n",
    "    \n",
    "    for network in range (0,NumberOfNetworks):\n",
    "\n",
    "        print(\"Getting predictions from netwrok number\",network,\"/\",NumberOfNetworks )\n",
    "        ## Loading Network parameters from csv\n",
    "        Parameters_Array,Random_Indexes1,Score_ = Load_Parameters_And_Indexes_Of_Specialist(network)\n",
    "        print(\"\")\n",
    "        ## Costructing Netwrok objects\n",
    "        Net5 = Reshape_into_5_layer_net(Parameters_Array[:,0],Number_Of_Nurons_in_5_layer_net)\n",
    "        Net5_C1 = Reshape_into_5_layer_net(Parameters_Array[:,1],Number_Of_Nurons_in_5_layer_net)\n",
    "        Net5_C2 = Reshape_into_5_layer_net(Parameters_Array[:,2],Number_Of_Nurons_in_5_layer_net)\n",
    "        Net5_C3 = Reshape_into_5_layer_net(Parameters_Array[:,3],Number_Of_Nurons_in_5_layer_net)\n",
    "        Net5_C4 = Reshape_into_5_layer_net(Parameters_Array[:,4],Number_Of_Nurons_in_5_layer_net)\n",
    "        Net5_C5 = Reshape_into_5_layer_net(Parameters_Array[:,5],Number_Of_Nurons_in_5_layer_net)\n",
    "        Net5_C6 = Reshape_into_5_layer_net(Parameters_Array[:,6],Number_Of_Nurons_in_5_layer_net)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ## Using Only the relavent indexes\n",
    "        X1 = X[:,Random_Indexes1[:,0]].T\n",
    "#         print(Random_Indexes1.shape)\n",
    "        ## Getting specialist preditictions and confidence\n",
    "        Predictions_Of_Specialist[network,:],Total_Confidence_Of_Network[network,:] = Get_Predictions_and_Confidence_From_Networks_1(Net5,Net5_C1,Net5_C2,Net5_C3,Net5_C4,Net5_C5,Net5_C6,X1)\n",
    "        Score_Or_Network[network,:] = Score_\n",
    "\n",
    "    Confident_Specialists_Group_Prediction = np.sum(Predictions_Of_Specialist*Total_Confidence_Of_Network*Score_Or_Network,0)\\\n",
    "                                                /(np.sum(Total_Confidence_Of_Network*Score_Or_Network,0))\n",
    "\n",
    "    return Confident_Specialists_Group_Prediction\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Get_Predictions_and_Confidence_From_Networks_1(Net5,Net5_C1,Net5_C2,Net5_C3,Net5_C4,Net5_C5,Net5_C6,X):\n",
    "    A5 , A4 , A3 ,A2 = Run_Through_5_layer_Net_And_Save_Activations(Net5,X)\n",
    "\n",
    "    Predictions_Of_Specialist = A5\n",
    "\n",
    "#     Confidence_Data_from_X = np.concatenate((A2,A3,A4,A5))\n",
    "#     Activations = np.concatenate((A2,A3,A4,A5))\n",
    "    Confidence_Data_from_X = np.concatenate((A2[0:10],A3))\n",
    "    \n",
    "    \n",
    "    C1 = Run_Through_5_layer_Net(Net5_C1,Confidence_Data_from_X )\n",
    "    C2 = Run_Through_5_layer_Net(Net5_C2,Confidence_Data_from_X )\n",
    "    C3 = Run_Through_5_layer_Net(Net5_C3,Confidence_Data_from_X )\n",
    "    C4 = Run_Through_5_layer_Net(Net5_C4,Confidence_Data_from_X )\n",
    "    C5 = Run_Through_5_layer_Net(Net5_C5,Confidence_Data_from_X )\n",
    "    C6 = Run_Through_5_layer_Net(Net5_C6,Confidence_Data_from_X )\n",
    "    \n",
    "#     print(\"Confidence_Data_from_X\")\n",
    "#     Total_Confidence = (Predictions_Of_Specialist > 0.5)*C1*C2*C3 + (Predictions_Of_Specialist < 0.5)*C4*C5*C6\n",
    "#     Total_Confidence = (Predictions_Of_Specialist > 0.5)*(C1>0.5)*(C2>0.5)*(C3>0.5) + (Predictions_Of_Specialist < 0.5)*(C4>0.5)*(C5>0.5)*(C6>0.5)\n",
    "#     Total_Confidence = (Predictions_Of_Specialist > 0.7)*(C1>0.5)*(C2>0.5)*(C3>0.5)*C1*C2*C3 + (Predictions_Of_Specialist < 0.7)*(C4>0.5)*(C5>0.5)*(C6>0.5)*C4*C5*C5\n",
    "#     Total_Confidence = (Predictions_Of_Specialist > 0.5)*(C1+C2+C3) + (Predictions_Of_Specialist < 0.5)*(C4+C5+C5)\n",
    "    Total_Confidence = (C1*C2*C3)*(1-C4)*(1-C5)*(1-C6)\n",
    "    \n",
    "    return Predictions_Of_Specialist,Total_Confidence\n",
    "    \n",
    "\n",
    "\n",
    "def Load_Parameters_And_Indexes_Of_Specialist(network):\n",
    "    Name_Of_Parameter_File = \"Specialist\" + str (network) + \"_Parameters.csv\"\n",
    "    Name_Of_Index_File = \"Specialist\" + str (network) + \"_Indexes.csv\"\n",
    "    Name_Of_Score_File = \"Specialist\" + str (network) + \"_Score.csv\"\n",
    "    \n",
    "\n",
    "    ## Open Parameter array\n",
    "    Parameters_Path = r\"C:\\Users\\benjy\\OneDrive\\Brown\\Fall 2018\\Hackathon\\Python files\" \n",
    "    Parameters_Path1 = Parameters_Path + \"\\\\\" + Name_Of_Parameter_File\n",
    "    Parameters_temp = pandas.read_csv(Parameters_Path1)\n",
    "    Parameters_Array = np.array(Parameters_temp)\n",
    "\n",
    "    ## Open Index vector\n",
    "    Index_Path = r\"C:\\Users\\benjy\\OneDrive\\Brown\\Fall 2018\\Hackathon\\Python files\" \n",
    "    Index_Path1 = Index_Path + \"\\\\\" + Name_Of_Index_File\n",
    "    Random_Indexes_temp = pandas.read_csv(Index_Path1)\n",
    "    Random_Indexes1 = np.array(Random_Indexes_temp)\n",
    "\n",
    "    ## Open Score File\n",
    "    Score_Path = r\"C:\\Users\\benjy\\OneDrive\\Brown\\Fall 2018\\Hackathon\\Python files\" \n",
    "    Score_Path1 = Index_Path + \"\\\\\" + Name_Of_Score_File\n",
    "    Score_temp = pandas.read_csv(Score_Path1)\n",
    "    Score1_ = np.array(Score_temp)\n",
    "    \n",
    "    return Parameters_Array,Random_Indexes1,Score1_ \n",
    "\n",
    "\n",
    "Confident_Specialists_Group_Prediction = Get_Confident_Specialists_Group_Prediction(CV_X,NumberOfNetworks)\n",
    "# Plot ROC\n",
    "\n",
    "print(Confident_Specialists_Group_Prediction.shape[0])\n",
    "\n",
    "for i in range (0,Confident_Specialists_Group_Prediction.shape[0]):\n",
    "    if math.isnan(Confident_Specialists_Group_Prediction[i]) == 1:\n",
    "        Confident_Specialists_Group_Prediction[i] = 0.5\n",
    "        \n",
    "\n",
    "print(Confident_Specialists_Group_Prediction)\n",
    "\n",
    "\n",
    "Plot_ROC_And_Claculate_AUC(CV_Y,Confident_Specialists_Group_Prediction)\n",
    "\n",
    "\n",
    "print(\"Test results\")\n",
    "Confident_Specialists_Group_Prediction = Get_Confident_Specialists_Group_Prediction(Test_X,NumberOfNetworks)\n",
    "# Plot ROC\n",
    "\n",
    "for i in range (0,Confident_Specialists_Group_Prediction.shape[0]):\n",
    "    if math.isnan(Confident_Specialists_Group_Prediction[i]) == 1:\n",
    "        Confident_Specialists_Group_Prediction[i] = 0.5\n",
    "        \n",
    "Plot_ROC_And_Claculate_AUC(Test_Y,Confident_Specialists_Group_Prediction)\n",
    "\n",
    "\n",
    "# PlotPrediction distribution\n",
    "Bins,Counts_in_bin = Create_Roc_Curve(Confident_Specialists_Group_Prediction)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def Create_Roc_Curve(Predictions):\n",
    "\n",
    "    ## Generating ROC\n",
    "    n1 = 0\n",
    "    n2 = 0\n",
    "    Positive_Predictions = np.zeros(Test_Y.shape[0])\n",
    "    Negative_Predictions = np.zeros(Test_Y.shape[0])\n",
    "    print(Predictions.shape)\n",
    "\n",
    "    for i in range (0,Test_Y.shape[0]):\n",
    "        if Test_Y[i] == 1:\n",
    "            Positive_Predictions[n1] = Predictions[i]\n",
    "            n1 += 1\n",
    "        if Test_Y[i] == 0:\n",
    "            Negative_Predictions[n2] = Predictions[i]\n",
    "            n2 += 1\n",
    "\n",
    "\n",
    "    Bins,Counts_in_bin_Positive = Create_Predictions_historgram(Positive_Predictions)\n",
    "    Bins,Counts_in_bin_Negative = Create_Predictions_historgram(Negative_Predictions)\n",
    "    plt.legend([\"Background\",\"Signal\"])\n",
    "\n",
    "    plt.figure(3)\n",
    "    plt.plot(Counts_in_bin_Positive,1-Counts_in_bin_Negative)\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.xlabel(\"Signal\")\n",
    "    plt.ylabel(\"Background\")        \n",
    "\n",
    "    return Counts_in_bin_Positive,1-Counts_in_bin_Negative\n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "def   Create_Predictions_historgram(Predictions):\n",
    "    \n",
    "\n",
    "    NumberOfBins = 100\n",
    "    Bins = np.linspace(0,1,NumberOfBins)\n",
    "    Counts_in_bin = np.zeros(NumberOfBins)\n",
    "    \n",
    "\n",
    "    for i in range (0,Predictions.shape[0]):\n",
    "        for j in range (0,len(Bins)-1):\n",
    "            if Predictions[i] > Bins[j] :\n",
    "                if Predictions[i] <= Bins[j+1]:\n",
    "                    Counts_in_bin[j] = Counts_in_bin[j] + 1\n",
    "                    \n",
    "#     Counts_in_bin = Counts_in_bin/np.max(Counts_in_bin)\n",
    "    \n",
    "\n",
    "    plt.plot(Bins,Counts_in_bin)\n",
    "    plt.title(\"Predictions Distribution\")\n",
    "    \n",
    "    return Bins,Counts_in_bin\n",
    "\n",
    "\n",
    "def Plot_ROC_And_Claculate_AUC(Test_Y,Final_Predictions):\n",
    "\n",
    "    import sklearn\n",
    "    from sklearn import metrics\n",
    "\n",
    "    y_true = Test_Y\n",
    "    y_score = Final_Predictions\n",
    "\n",
    "    AUC = sklearn.metrics.roc_auc_score(y_true, y_score)\n",
    "    print(\"AUC is:\",AUC)\n",
    "\n",
    "\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    n_classes = 1\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = sklearn.metrics.roc_curve(y_true, y_score)\n",
    "        roc_auc[i] = sklearn.metrics.auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = sklearn.metrics.roc_curve(y_true.ravel(), y_score.ravel())\n",
    "    roc_auc[\"micro\"] = sklearn.metrics.auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "\n",
    "\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"], color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[\"micro\"])\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Find_Optimal_Threshold_Using_Training_Set() missing 1 required positional argument: 'Y'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-267-f9bb18c203ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mBestThreshold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFind_Optimal_Threshold_Using_Training_Set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConfident_Specialists_Group_Prediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: Find_Optimal_Threshold_Using_Training_Set() missing 1 required positional argument: 'Y'"
     ]
    }
   ],
   "source": [
    "\n",
    "BestThreshold = Find_Optimal_Threshold_Using_Training_Set(Confident_Specialists_Group_Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def f(x):\n",
    "    return x*x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    p = Pool(5)\n",
    "    print(p.map(f, [1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, TimeoutError\n",
    "import time\n",
    "import os\n",
    "\n",
    "def f(x):\n",
    "    return x*x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pool = Pool(processes=4)              # start 4 worker processes\n",
    "\n",
    "    # print \"[0, 1, 4,..., 81]\"\n",
    "    print pool.map(f, range(10))\n",
    "\n",
    "    # print same numbers in arbitrary order\n",
    "    for i in pool.imap_unordered(f, range(10)):\n",
    "        print i\n",
    "\n",
    "    # evaluate \"f(20)\" asynchronously\n",
    "    res = pool.apply_async(f, (20,))      # runs in *only* one process\n",
    "    print res.get(timeout=1)              # prints \"400\"\n",
    "\n",
    "    # evaluate \"os.getpid()\" asynchronously\n",
    "    res = pool.apply_async(os.getpid, ()) # runs in *only* one process\n",
    "    print res.get(timeout=1)              # prints the PID of that process\n",
    "\n",
    "    # launching multiple evaluations asynchronously *may* use more processes\n",
    "    multiple_results = [pool.apply_async(os.getpid, ()) for i in range(4)]\n",
    "    print [res.get(timeout=1) for res in multiple_results]\n",
    "\n",
    "    # make a single worker sleep for 10 secs\n",
    "    res = pool.apply_async(time.sleep, (10,))\n",
    "    try:\n",
    "        print res.get(timeout=1)\n",
    "    except TimeoutError:\n",
    "        print \"We lacked patience and got a multiprocessing.TimeoutError\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Make_True_False_Positive_negative_datasets(Predictions,Y,Size_Of_Confience_Dataset):\n",
    "    \n",
    "    Number_Of_test_Samples = len(Y)\n",
    "\n",
    "\n",
    "    True_Positive_KillingVector = np.zeros(Number_Of_test_Samples)\n",
    "    False_Positive_KillingVector = np.zeros(Number_Of_test_Samples)\n",
    "    True_Negative_KillingVector = np.zeros(Number_Of_test_Samples)\n",
    "    False_Negative_KillingVector = np.zeros(Number_Of_test_Samples)\n",
    "\n",
    "\n",
    "    Positive_Dataset_Level1 = np.zeros(Number_Of_test_Samples)\n",
    "    Negative_Dataset_Level1 = np.zeros(Number_Of_test_Samples)\n",
    "\n",
    "    for i in range (0,Number_Of_test_Samples):\n",
    "        if Predictions[i]>0.5 and Y[i] == 1:\n",
    "            True_Positive_KillingVector[i] = 1\n",
    "            Positive_Dataset_Level1[i] = 1\n",
    "\n",
    "        if Predictions[i]>0.5 and Y[i] == 0:\n",
    "            False_Positive_KillingVector[i] = 1\n",
    "\n",
    "\n",
    "        if Predictions[i]<0.5 and Y[i] == 1:\n",
    "            False_Negative_KillingVector[i] = 1\n",
    "\n",
    "\n",
    "        if Predictions[i]<0.5 and Y[i] == 0:\n",
    "            True_Negative_KillingVector[i] = 1\n",
    "            Negative_Dataset_Level1[i] = 1\n",
    "\n",
    "\n",
    "\n",
    "    for i in range (0,Number_Of_test_Samples):\n",
    "        if True_Positive_KillingVector[i] == 1.0 and np.sum(True_Positive_KillingVector) > Size_Of_Confience_Dataset/2:\n",
    "            True_Positive_KillingVector[i] = 0.0\n",
    "\n",
    "        if False_Positive_KillingVector[i] == 1.0 and np.sum(False_Positive_KillingVector) > Size_Of_Confience_Dataset/2:\n",
    "            False_Positive_KillingVector[i] = 0.0\n",
    "\n",
    "\n",
    "    Size_Of_Confience_Dataset = 50\n",
    "    for i in range (0,Number_Of_test_Samples):\n",
    "        if True_Negative_KillingVector[i] == 1.0 and np.sum(True_Negative_KillingVector) > Size_Of_Confience_Dataset/2:\n",
    "            True_Negative_KillingVector[i] = 0.0\n",
    "\n",
    "        if False_Negative_KillingVector[i] == 1.0 and np.sum(False_Negative_KillingVector) > Size_Of_Confience_Dataset/2:\n",
    "            False_Negative_KillingVector[i] = 0.0\n",
    "\n",
    "\n",
    "\n",
    "    Total_Positive_Killing_Vector = True_Positive_KillingVector + False_Positive_KillingVector\n",
    "    Total_Negative_Killing_Vector = True_Positive_KillingVector + False_Positive_KillingVector\n",
    "    \n",
    "\n",
    "    return Positive_Dataset_Level1, Negative_Dataset_Level1, Total_Positive_Killing_Vector, Total_Negative_Killing_Vector\n",
    "    \n",
    "    \n",
    "Size_Of_Confience_Dataset = 50    \n",
    "Number_Of_test_Samples1 = 400\n",
    "Predictions = np.random.rand(Number_Of_test_Samples1)\n",
    "Y = np.random.rand(Number_Of_test_Samples1) >0.5\n",
    "\n",
    "Positive_Dataset_Level1, Negative_Dataset_Level1, Total_Positive_Killing_Vector, Total_Negative_Killing_Vector = Make_True_False_Positive_negative_datasets(Predictions,Y,Size_Of_Confience_Dataset)\n",
    "\n",
    "\n",
    "## Train Net_C1 on positive dataset dataset levelt 1\n",
    "\n",
    "\n",
    "\n",
    "## Train Net_C4 on Negative dataset levelt 1\n",
    "\n",
    "\n",
    "## Test datasets on 400 samples\n",
    "\n",
    "## Create dataset for C2,C3 - Using the predictions of C1\n",
    "\n",
    "## train C2\n",
    "\n",
    "## Train C3\n",
    "\n",
    "\n",
    "## Create dataset for C5,C6 - Using the predictions of C4\n",
    "\n",
    "## train C5\n",
    "\n",
    "## Train C6\n",
    "\n",
    "\n",
    "# TotalConfidence = (Predictions > 0.5)*(C1*(C1>0.5)*C2 + C1*(C1>0.5)*(1-C3)) + (Predictions < 0.5)*(C4*(C4>0.5)*C5 + C4*(C4>0.5)*(1-C6))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.53978687e-05 4.53978687e-05 4.53978687e-05 9.99954602e-01\n",
      " 4.53978687e-05 4.53978687e-05 4.53978687e-05 9.99954602e-01\n",
      " 4.53978687e-05 4.53978687e-05]\n",
      "[1. 1. 0. 1. 0. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Prediction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-0d386e5f450f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPrediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m390\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPositive_Dataset_Level1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m390\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Prediction' is not defined"
     ]
    }
   ],
   "source": [
    "    print((Predictions*Total_Positive_Killing_Vector)[390:400])\n",
    "    print(Positive_Dataset_Level1[390:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40,)\n",
      "(40,)\n",
      "0.7\n"
     ]
    }
   ],
   "source": [
    "a =(Positive_Dataset_Level1*Total_Positive_Killing_Vector)[450:490]\n",
    "b = (Predictions1*Total_Positive_Killing_Vector)[0,450:490] > 0.5 \n",
    "\n",
    "print(a.shape)\n",
    "print(b.shape)\n",
    "print(np.sum(a == b)/40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 50)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.65236679 0.5        0.5        0.5        0.5       ]\n",
      "[1. 0. 1. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(Confident_Specialists_Group_Prediction[15:20])\n",
    "print(CV_Y[15:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27]\n",
      "array('i', [0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "import array\n",
    "array.array('i')\n",
    "a = np.array(array.array('i',(i for i in range(0,28))))\n",
    "print(np.array(a))\n",
    "\n",
    "import array\n",
    "import itertools\n",
    "a = array_of_signed_ints = array.array(\"i\", itertools.repeat(0, 10))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 50)\n",
      "(28, 500)\n"
     ]
    }
   ],
   "source": [
    "print(np.concatenate((A2[0:10],A3)).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
