{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Two Mode Section\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "np.seterr(invalid='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def Two_Mode_Optimizer(Seed_Parameters,Second_Best_Seed_Parameters,NumberOfSamples,Parameter_ChangeVector,Cost_Change,ImprovementIteration,TargetedSearchDecayRate,RandomSearchGrowthRate,TargetedMultiplicationFactor,RandomMultiplicationFactor,SignificantChangeValue,Maximal_Random_Search,Adaptive_Amplitude):\n",
    "def Two_Mode_Optimizer(Seed_Parameters,Second_Best_Seed_Parameters,NumberOfSamples,Parameter_ChangeVector,Cost_Change,Random_Search_exponential_Growth_Factor,Targeted_Search_exponential_Growth_Factor,Targeted_Search_Decay_Rate,Random_Search_Growth_Rate,Targeted_Search_Growth_Rate,Random_Search_Period,Significant_Change_Value,Maximal_Random_Search,Adaptive_Amplitude,Directional_Pool_Ratio,Pool_Size,Directional_Pool):\n",
    "\n",
    "#     Print_Two_Mode_Analytics = 0\n",
    "#     if np.isnan(Cost_Change) == 1:\n",
    "#         Print_Two_Mode_Analytics = 1\n",
    "        \n",
    "    Targeted_Search_Mechanism = 2\n",
    "\n",
    "\n",
    "    Number_Of_Parameters_11 = len(Seed_Parameters)\n",
    "    TargetedSearch = np.zeros((Number_Of_Parameters_11,NumberOfSamples))\n",
    "    Random_Search_exponential_Growth_Factor1 = Random_Search_exponential_Growth_Factor\n",
    "    \n",
    "\n",
    "    ################################# Testing if the improvement since last iteration was big enough #############\n",
    "    if np.sum(np.abs(Parameter_ChangeVector)) > Significant_Change_Value:\n",
    "        Random_Search_exponential_Growth_Factor = 0\n",
    "        Targeted_Search_exponential_Growth_Factor = Targeted_Search_exponential_Growth_Factor + 1\n",
    "\n",
    "    if np.sum(np.abs(Parameter_ChangeVector)) < Significant_Change_Value:\n",
    "        Targeted_Search_exponential_Growth_Factor = 0\n",
    "        \n",
    "        \n",
    "    if Print_Two_Mode_Analytics == 1:\n",
    "        print(\"Two_Mode_Analytics:\")\n",
    "        print(\"---------------------------------------------------------------------\")\n",
    "        print(\"The np.sum(np.abs(Parameter_ChangeVector)) is \",np.sum(np.abs(Parameter_ChangeVector)), )\n",
    "        print(\"As Compared To the Significant_Change_Value of \", Significant_Change_Value)\n",
    "    ###############################################################################################################        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    ################################# This the Random Search Section #############################################\n",
    "    ## Applying a Genetic Algorithm\n",
    "    # This is to broadcast them to a len(InitialParameters),NumberOfSamples size matrix, for element wise multiplication\n",
    "    Seed_Parameters1 = np.zeros((Number_Of_Parameters_11,NumberOfSamples))\n",
    "    Seed_Parameters1[:,:] = np.array([Seed_Parameters]).T\n",
    "    Second_Best_Seed_Parameters1 = np.zeros((Number_Of_Parameters_11,NumberOfSamples))    \n",
    "    Second_Best_Seed_Parameters1[:,:] = np.array([Second_Best_Seed_Parameters]).T    \n",
    "    \n",
    "    \n",
    "    Random_Search_Scalar_Amplitude = Maximal_Random_Search*np.exp(Random_Search_Growth_Rate*(np.sin(np.mod(Random_Search_exponential_Growth_Factor*np.pi/(2*Random_Search_Period),np.pi/2) + 0.01)-1))\n",
    "    Random_Samples = 2*(np.random.rand(Number_Of_Parameters_11,NumberOfSamples)-0.5)\n",
    "    \n",
    "    ## Take half of the parameter each of the best two individuals from last iteration + add a smart \n",
    "    ## RMS addaptive amplitude search around the children of the next generation (Mutation) + at the end \n",
    "    ## subtract the initial seed parameters so that we are only left with change vectors\n",
    "    \n",
    "    Genes = np.random.rand(Number_Of_Parameters_11,NumberOfSamples)\n",
    "\n",
    "\n",
    "    RandomSearch = (Genes > 0.5)*Seed_Parameters1 + \\\n",
    "    (Genes <= 0.5)*Second_Best_Seed_Parameters1 + \\\n",
    "    Random_Samples*Random_Search_Scalar_Amplitude*Adaptive_Amplitude - Seed_Parameters1\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    Number_Of_Directional_Samples = int(Directional_Pool_Ratio*NumberOfSamples)\n",
    "    Directional_Samples = Random_Search_Scalar_Amplitude*np.matmul(Directional_Pool,np.random.rand(len(Directional_Pool[0,:]),Number_Of_Directional_Samples))\n",
    "    RandomSearch[:,0:Number_Of_Directional_Samples] = Directional_Samples \n",
    "    \n",
    "    \n",
    "    \n",
    "    if Print_Two_Mode_Analytics == 1:\n",
    "        print(\"\")\n",
    "        print(\"Random_Search_Scalar_Amplitude = \",Random_Search_Scalar_Amplitude)\n",
    "    ###############################################################################################################\n",
    "\n",
    "        \n",
    "    ################################# This the Targeted Search Section #############################################\n",
    "    RandomNumbers = np.random.rand(NumberOfSamples,1)\n",
    "    Random_Search_Scalar_Amplitude_1 = Maximal_Random_Search*np.exp(Random_Search_Growth_Rate*(np.sin(np.mod(Random_Search_exponential_Growth_Factor1*np.pi/(2*Random_Search_Period),np.pi/2) + 0.01)-1))\n",
    "\n",
    "    \n",
    "    if Targeted_Search_Mechanism == 1:\n",
    "        TargetedSearch_Amplitude = RandomNumbers*np.exp(-Random_Search_exponential_Growth_Factor*Targeted_Search_Decay_Rate)     \\\n",
    "        *Targeted_Search_Growth_Rate*(-Cost_Change)\n",
    "\n",
    "    if Targeted_Search_Mechanism == 2:\n",
    "        TargetedSearch_Amplitude = Random_Search_Scalar_Amplitude_1*RandomNumbers*\\\n",
    "        np.exp(Targeted_Search_exponential_Growth_Factor*Targeted_Search_Growth_Rate\\\n",
    "               -Random_Search_exponential_Growth_Factor*Targeted_Search_Decay_Rate)\n",
    "\n",
    "        \n",
    "    if Print_Two_Mode_Analytics == 1:\n",
    "        print(\"Mean Targeted_Search_Scalar_Amplitude =  = \",np.mean(TargetedSearch_Amplitude))\n",
    "    \n",
    "    if np.max(np.max(TargetedSearch_Amplitude)) > 1000: ## This is if the change vector is too big\n",
    "#         print(\"Applied supression of change vector\")\n",
    "        \n",
    "        if Targeted_Search_Mechanism == 1:\n",
    "            TargetedSearch_Amplitude = RandomNumbers*np.exp(- Random_Search_exponential_Growth_Factor*Targeted_Search_Decay_Rate)     \\\n",
    "            *Targeted_Search_Growth_Rate*(10)\n",
    "\n",
    "        \n",
    "        if Targeted_Search_Mechanism == 2:\n",
    "#             TargetedSearch_Amplitude = Random_Search_Scalar_Amplitude_1 *RandomNumbers*\\\n",
    "#             np.exp(30*Targeted_Search_Growth_Rate\\\n",
    "#                    -Random_Search_exponential_Growth_Factor1*Targeted_Search_Decay_Rate)\n",
    "            TargetedSearch_Amplitude = TargetedSearch_Amplitude/10\n",
    "\n",
    "        if Print_Two_Mode_Analytics == 1:\n",
    "            print(\"np.mean(TargetedSearch_Amplitude) after suppression is = \",np.mean(TargetedSearch_Amplitude))\n",
    "    \n",
    "    if D <= 40:\n",
    "        for sample in range (1,NumberOfSamples):\n",
    "    #         TargetedSearch[:,sample] = TargetedSearch_Amplitude[sample]*Parameter_ChangeVector[:,0]\n",
    "            TargetedSearch[:,sample] = TargetedSearch_Amplitude[sample]*Parameter_ChangeVector\n",
    "\n",
    "    \n",
    "    if D > 40:\n",
    "        TargetedSearch = (np.matmul(np.matmul(np.diag(TargetedSearch_Amplitude[:,0]),np.ones((NumberOfSamples,Number_Of_Parameters_11))),np.diag(Parameter_ChangeVector))).T\n",
    "    ###############################################################################################################\n",
    "    \n",
    "    \n",
    "    ################################ This is the Final Readout and generating Suggestions ########################\n",
    "    if Print_Two_Mode_Analytics == 1:   \n",
    "        print(\"\")\n",
    "        print(\" The Adaptive_Amplitude Vector is :\",Adaptive_Amplitude[:,0])\n",
    "        print(\"---------------------------------------------------------------------\")\n",
    "        print(\"\")\n",
    "    \n",
    "\n",
    "    Suggested_Parameter_Samples_From_Two_Mode = np.array([Seed_Parameters]).T + TargetedSearch + RandomSearch\n",
    "    Suggested_Parameter_Samples_From_Two_Mode[:,0] = Seed_Parameters\n",
    "    \n",
    "    Random_Search_exponential_Growth_Factor = Random_Search_exponential_Growth_Factor +1\n",
    "    ##############################################################################################################\n",
    "\n",
    "    return Suggested_Parameter_Samples_From_Two_Mode, Random_Search_exponential_Growth_Factor, Targeted_Search_exponential_Growth_Factor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pinball_Optimizer(Initial_Parameters,NumberOfSamples,Search_Ratio,AmplitudeOfLinearSearch,AmplitudeOfRandomSearch,Linear_Search_Vector):\n",
    "\n",
    "\n",
    "    ##Initalize infunction arrays\n",
    "    Suggested_Parameter_Samples_From_Pinball = np.zeros((len(Initial_Parameters),NumberOfSamples))\n",
    "\n",
    "    Number_Of_Linear_Samples = int(Search_Ratio*NumberOfSamples)\n",
    "\n",
    "    Step_Size_Vector = np.linspace(-AmplitudeOfLinearSearch/2,AmplitudeOfLinearSearch/2,Number_Of_Linear_Samples)\n",
    "\n",
    "    I = np.argmin(np.abs(Step_Size_Vector))\n",
    "    Step_Size_Vector[I] = 0\n",
    "\n",
    "    for k in range (0,NumberOfSamples):\n",
    "        if k +1<=Number_Of_Linear_Samples:\n",
    "                ## Generate Linear Search Samples around the inital parameter vector\n",
    "            Suggested_Parameter_Samples_From_Pinball[:,k] = Initial_Parameters[:,0] + Linear_Search_Vector[:,0]*Step_Size_Vector[k]                     \n",
    "\n",
    "        else:\n",
    "                ## Generate Random Search Samples around the inital parameter vector  \n",
    "            Suggested_Parameter_Samples_From_Pinball[:,k] = Initial_Parameters[:,0] + 2*AmplitudeOfRandomSearch*(np.random.rand(1,len(Initial_Parameters))-0.5)\n",
    "\n",
    "    return Suggested_Parameter_Samples_From_Pinball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Total_Resources 60.0\n",
      "\n",
      "Minimum Resouces test\n",
      "------------------------------------------------------------------\n",
      "Minimal_Allocation is: 3\n",
      "(Maximal_Allocation is: 19 )\n",
      "Total_Resources After minumum resource allocation Test 60\n",
      "\n",
      "Maximum Resouces test\n",
      "------------------------------------------------------------------\n",
      "Maximal_Allocation is: 27\n",
      "(Minimal_Allocation is: 11 )\n",
      "Total_Resources After maximum resource allocation Test 60\n"
     ]
    }
   ],
   "source": [
    "## Conductor Algorithm\n",
    "\n",
    "def Symmetric_Section_Conductor(Current_Resource_Allocation,Changevector,Hyper_Parameters):\n",
    "    ## The difference is that this function can allow for different masses and interactions between the algorithms\n",
    "    \n",
    "    \n",
    "    Initial_Resource_Allocation = Hyper_Parameters[:,0]\n",
    "    MassVector = Hyper_Parameters[:,1]\n",
    "    Self_Interaction_Spring_Constants = Hyper_Parameters[:,2]\n",
    "    InteractionMatrix = Hyper_Parameters[:,3:3+Number_Of_Algorithms]\n",
    "\n",
    "    \n",
    "    \n",
    "    Norm = np.sqrt(np.matmul(Changevector,Changevector))\n",
    "    if Norm != 0:\n",
    "        Changevector = Changevector/Norm\n",
    "    \n",
    "    R0 = Initial_Resource_Allocation\n",
    "    R = Current_Resource_Allocation\n",
    "    K0 = Self_Interaction_Spring_Constants\n",
    "\n",
    "    ## This updates the allocated resources\n",
    "    H_Self = -(K0/MassVector)*(R-R0) ## This slowly restores the Resouce allocation to the original values\n",
    "    H_Interaction = np.matmul(InteractionMatrix,(Changevector/MassVector).T) ## this is a resouce conserving interaction\n",
    "    R = R + H_Self + H_Interaction.T\n",
    "        \n",
    "\n",
    "    RecomendedResourceAllocation1 = np.round(R)\n",
    "\n",
    "    ## This makes sure that if the total resources aren't conserved then add or subtract from the best Member.\n",
    "    I = np.argmax(RecomendedResourceAllocation1)\n",
    "    RecomendedResourceAllocation1[I] = RecomendedResourceAllocation1[I] + (np.sum(Initial_Resource_Allocation) - np.sum(RecomendedResourceAllocation1))\n",
    "\n",
    "    ## This convertes the array to an int array\n",
    "    RecomendedResourceAllocation = np.arange(len(RecomendedResourceAllocation1))\n",
    "    for i in range(len(RecomendedResourceAllocation1)):\n",
    "        if np.isnan(RecomendedResourceAllocation1[i]):\n",
    "#             print(\"RecomendedResourceAllocation1\",RecomendedResourceAllocation1)\n",
    "#             print(\"Changevector\",Changevector)\n",
    "            Changevector = np.zeros(4)\n",
    "            RecomendedResourceAllocation1 = 15*np.ones(4)\n",
    "        RecomendedResourceAllocation[i] = int(RecomendedResourceAllocation1[i])\n",
    "    \n",
    "    return RecomendedResourceAllocation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Set_Hyperparameters_For_Symmetric_Conductor(Total_Resources,Number_Of_Algorithms,Mass,Self_Spring_Constants,Interaction_Spring_Constants):\n",
    "    \n",
    "#     Initial_Resource_Allocation = np.array([int(Total_Resources/Number_Of_Algorithms) ,int(Total_Resources/Number_Of_Algorithms) ,int(Total_Resources/Number_Of_Algorithms)])\n",
    "    \n",
    "    Initial_Resource_Allocation =int(Total_Resources/Number_Of_Algorithms)*np.ones(Number_Of_Algorithms)\n",
    "\n",
    "    ## This fixes Rounding issues\n",
    "    Initial_Resource_Allocation[0] = Initial_Resource_Allocation[0] + (Total_Resources-np.sum(Initial_Resource_Allocation))\n",
    "    MassVector = Mass*np.ones(Number_Of_Algorithms)\n",
    "    Self_Interaction_Spring_Constants = Self_Spring_Constants*np.ones(Number_Of_Algorithms)\n",
    "    InteractionMatrix = Interaction_Spring_Constants*(-np.ones((Number_Of_Algorithms,Number_Of_Algorithms)) + Number_Of_Algorithms*np.diag(np.ones(Number_Of_Algorithms)))\n",
    "    \n",
    "    Hyperparameters_Array = np.zeros((Number_Of_Algorithms,3+Number_Of_Algorithms))\n",
    "    Hyperparameters_Array[:,0] = Initial_Resource_Allocation\n",
    "    Hyperparameters_Array[:,1] = MassVector\n",
    "    Hyperparameters_Array[:,2] = Self_Interaction_Spring_Constants\n",
    "    Hyperparameters_Array[:,3:3+Number_Of_Algorithms] = InteractionMatrix\n",
    "\n",
    "    return Hyperparameters_Array\n",
    "\n",
    "\n",
    "\n",
    "### Test of the conductor algorithm\n",
    "\n",
    "Total_Resources = 60\n",
    "Number_Of_Algorithms = 4\n",
    "Mass =10\n",
    "Self_Spring_Constants = 10\n",
    "Interaction_Spring_Constants = 40\n",
    "    \n",
    "    \n",
    "Hyperparameters_For_Symmetric_Conductor = Set_Hyperparameters_For_Symmetric_Conductor(Total_Resources,Number_Of_Algorithms,Mass,Self_Spring_Constants,Interaction_Spring_Constants)\n",
    "Initial_Resource_Allocation = Hyperparameters_For_Symmetric_Conductor[:,0]\n",
    "Current_Resource_Allocation = Hyperparameters_For_Symmetric_Conductor[:,0]\n",
    "\n",
    "\n",
    "\n",
    "print(\"Initial Total_Resources\",np.sum(Initial_Resource_Allocation))\n",
    "for i in range (1,50):\n",
    "    Changevector = np.array([-1, 0 ,0,0]) ## Negative is bad\n",
    "    Recomended_Resource_Allocation = Symmetric_Section_Conductor(Current_Resource_Allocation,Changevector,Hyperparameters_For_Symmetric_Conductor) \n",
    "    Current_Resource_Allocation = Recomended_Resource_Allocation\n",
    "\n",
    "\n",
    "Minimal_Allocation = np.min(Current_Resource_Allocation)\n",
    "print(\"\")\n",
    "print(\"Minimum Resouces test\")\n",
    "print(\"------------------------------------------------------------------\")\n",
    "print(\"Minimal_Allocation is:\",Minimal_Allocation)\n",
    "print(\"(Maximal_Allocation is:\",np.max(Current_Resource_Allocation),\")\")\n",
    "print(\"Total_Resources After minumum resource allocation Test\",np.sum(Current_Resource_Allocation))\n",
    "\n",
    "for i in range (1,50):\n",
    "    Changevector = np.array([1,0 ,0,0]) ## Positive is good\n",
    "    Recomended_Resource_Allocation = Symmetric_Section_Conductor(Current_Resource_Allocation,Changevector,Hyperparameters_For_Symmetric_Conductor)\n",
    "    Current_Resource_Allocation = Recomended_Resource_Allocation\n",
    "\n",
    "Maximal_Allocation = np.max(Current_Resource_Allocation)\n",
    "print(\"\")\n",
    "print(\"Maximum Resouces test\")\n",
    "print(\"------------------------------------------------------------------\")\n",
    "print(\"Maximal_Allocation is:\",Maximal_Allocation)\n",
    "print(\"(Minimal_Allocation is:\",np.min(Current_Resource_Allocation),\")\")\n",
    "print(\"Total_Resources After maximum resource allocation Test\",np.sum(Current_Resource_Allocation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[ 0.15222119  0.48265059  0.42738488 -0.10128853 -0.2658909   0.06794057\n",
      " -0.42420935  0.26342591  0.02661909  0.27686681 -0.37500348 -0.09114726]\n",
      "[-0.24244466 -0.11466386 -0.43025705  0.0952122   0.03824184  0.47298651\n",
      " -0.23152725  0.19561902 -0.35209843 -0.01299156 -0.04153837 -0.37235845]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "### These are hyperparameters\n",
    "# Search_Ratio,AmplitudeOfLinearSearch,AmplitudeOfRandomSearch\n",
    "                \n",
    "        \n",
    "class Pinball_Suggestion_Tools(object):\n",
    "\n",
    "    # The class \"constructor\" - It's actually an initializer \n",
    "    def __init__(self, Search_Ratio,Number_Of_Samples,Linear_Search_Vector,Best_Individual):\n",
    "        self.Number_Of_Linear_Samples = int(Search_Ratio*Number_Of_Samples)\n",
    "        self.Linear_Search_Vector    =    Linear_Search_Vector\n",
    "        self.Best_Individual = Best_Individual\n",
    "        self.Number_Of_Samples = Number_Of_Samples\n",
    "        \n",
    "def Create_New_Pinball_Suggestion_Tools_Object(Number_Of_Parameters,Search_Ratio = 0.7,Number_Of_Samples = 10): ## this is where the directional pool is initialized\n",
    "#     N = Number_Of_Parameters\n",
    "#     Number_Of_Samples = 10\n",
    "#     Search_Ratio = 0.7\n",
    "    Linear_Search_Vector = np.random.rand(Number_Of_Parameters) - 0.5\n",
    "    Linear_Search_Vector = Linear_Search_Vector/np.sqrt(np.sum(Linear_Search_Vector**2))\n",
    "    Best_Individual = np.random.rand(Number_Of_Parameters) - 0.5\n",
    "    ## Generating and instance For The object the Pinball Suggestions tools object\n",
    "    return  Pinball_Suggestion_Tools(Search_Ratio,Number_Of_Samples,Linear_Search_Vector,Best_Individual)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Test_Object_Functionality = 1\n",
    "if Test_Object_Functionality == 1:\n",
    "    ## Setting the inputs for the Two mode Suggestions tools object\n",
    "    Number_Of_Parameters = 12\n",
    "    Search_Ratio = 0.5\n",
    "    Number_Of_Samples = 10\n",
    "    a = Create_New_Pinball_Suggestion_Tools_Object(Number_Of_Parameters, Search_Ratio,Number_Of_Samples)\n",
    "    print(a.Number_Of_Linear_Samples)\n",
    "    print(a.Linear_Search_Vector)\n",
    "    print(a.Best_Individual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def Optimize_With_TSTM(InitialParameters,Cost1,NumberOfSamples,TargetedSearchDecayRate,RandomSearchGrowthRate,TargetedMultiplicationFactor ,RandomMultiplicationFactor,SignificantChangeValue,Maximal_Random_Search,print_Cost,Number_Of_Itterations,alpha,beta):\n",
    "    \n",
    "def Get_Pinball_Suggestions(fun,Hyperparameters,Suggestion_Tools,Print_Two_Mode_Analytics):\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    Search_Ratio  = Hyperparameters[0]\n",
    "    AmplitudeOfLinearSearch = Hyperparameters[1]\n",
    "    AmplitudeOfRandomSearch = Hyperparameters[2]\n",
    "    \n",
    "    Best_Individual = Suggestion_Tools.Best_Individual\n",
    "    Number_Of_Samples = Suggestion_Tools.Number_Of_Samples \n",
    "    Linear_Search_Vector = Suggestion_Tools.Linear_Search_Vector\n",
    "    Number_Of_Linear_Samples = Suggestion_Tools.Number_Of_Linear_Samples\n",
    "    \n",
    "    \n",
    "    Suggested_Parameter_Samples_Pinball_Mode = Pinball_Optimizer(Initial_Parameters,NumberOfSamples,Search_Ratio,AmplitudeOfLinearSearch,AmplitudeOfRandomSearch,Linear_Search_Vector)\n",
    "    \n",
    "    \n",
    "    return Suggested_Parameter_Samples_Pinball_Mode\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fun(x):\n",
    "    return np.sum((x-2)**2)\n",
    "\n",
    "\n",
    "def Get_Suggestions_From_Pinball_Section(fun,Hyperparameters_1,Hyperparameters_2,Hyperparameters_3,Hyperparameters_4,Suggestion_Tools_1,Suggestion_Tools_2,Suggestion_Tools_3,Suggestion_Tools_4,Hyperparameters_For_Symmetric_Conductor):\n",
    "\n",
    "    ## This gets The Suggestions from The Different Sets Of hyperparameters Set 1\n",
    "\n",
    "    Suggested_Parameter_Samples_From_Pinball_1,Suggestion_Tools_1  = Get_Pinball_Suggestions(fun,Hyperparameters_1,Suggestion_Tools_1 ,Print_Two_Mode_Analytics)   \n",
    "    Suggested_Parameter_Samples_From_Pinball_2,Suggestion_Tools_2  = Get_Pinball_Suggestions(fun,Hyperparameters_2,Suggestion_Tools_2 ,Print_Two_Mode_Analytics)   \n",
    "    Suggested_Parameter_Samples_From_Pinball_3,Suggestion_Tools_3  = Get_Pinball_Suggestions(fun,Hyperparameters_3,Suggestion_Tools_3 ,Print_Two_Mode_Analytics)   \n",
    "    Suggested_Parameter_Samples_From_Pinball_4,Suggestion_Tools_4  = Get_Pinball_Suggestions(fun,Hyperparameters_4,Suggestion_Tools_4 ,Print_Two_Mode_Analytics) \n",
    "\n",
    "\n",
    "    ## This is to orginize the different indexes of the suggestions for later processing\n",
    "    N1 = len(Suggested_Parameter_Samples_From_Two_Mode_1[0,:])\n",
    "    N2 = len(Suggested_Parameter_Samples_From_Two_Mode_2[0,:])\n",
    "    N3 = len(Suggested_Parameter_Samples_From_Two_Mode_3[0,:])\n",
    "    N4 = len(Suggested_Parameter_Samples_From_Two_Mode_4[0,:])\n",
    "\n",
    "    ## Save the indexes in the Suggestions array to know which samples came from which indvidual\n",
    "    Indexes_Of_The_Suggestions_From_The_Different_Algorithms = np.array([0,N1,N1+N2,N1+N2+N3,N1+N2+N3+N4])\n",
    "    ## Combine all the suggested samples into one array\n",
    "    All_Suggestions_From_Pinball_Section = np.concatenate((Suggested_Parameter_Samples_From_Pinball_1,Suggested_Parameter_Samples_From_Pinball_2,Suggested_Parameter_Samples_From_Pinball_3,Suggested_Parameter_Samples_From_Pinball_4),1)\n",
    "\n",
    "\n",
    "    return All_Suggestions_From_Pinball_Section,Indexes_Of_The_Suggestions_From_The_Different_Algorithms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "def Get_Best_Cost_Change_From_Individuals(Cost_Change_From_Probe_Samples,Indexes_Of_The_Suggestions_From_The_Different_Algorithms1):\n",
    "\n",
    "    Best_Cost_Change_From_Individuals  = np.zeros(4)\n",
    "    Indexes = Indexes_Of_The_Suggestions_From_The_Different_Algorithms1\n",
    "\n",
    "    for i in range (0,4):                                                                                                                       \n",
    "        index_min = np.argmin(Cost_Change_From_Probe_Samples[Indexes[i]:Indexes[i+1]])\n",
    "#         print(\"index_min For the individuals\",index_min)\n",
    "        Best_Cost_Change_From_Individuals[i] = Cost_Change_From_Probe_Samples[Indexes[i]:Indexes[i+1]][index_min]\n",
    "\n",
    "    return  Best_Cost_Change_From_Individuals                                                                                \n",
    "\n",
    "                                                                                                                                   \n",
    "                                                                                                                                   \n",
    "                                                                                                                        \n",
    "         ## Update the Resouce Allocation\n",
    "def  Update_Resource_Allocation(Cost_Change_From_Probe_Samples,Indexes_Of_The_Suggestions_From_The_Different_Algorithms1,Suggestion_Tools_1,Suggestion_Tools_2,Suggestion_Tools_3,Suggestion_Tools_4,Hyperparameters_For_Symmetric_Conductor):\n",
    "                                  \n",
    "    # Normalize Cost\n",
    "    if np.sum(np.abs(Cost_Change_From_Probe_Samples)) != 0:\n",
    "#         Cost_Change_From_Probe_Samples = Cost_Change_From_Probe_Samples/(np.sqrt(np.matmul(Cost_Change_From_Probe_Samples,Cost_Change_From_Probe_Samples)))\n",
    "                                                                            \n",
    "        Cost_Change_From_Probe_Samples = Cost_Change_From_Probe_Samples/(np.sqrt(np.sum(Cost_Change_From_Probe_Samples**2)))\n",
    "                                                                            \n",
    "                 \n",
    "            \n",
    "            \n",
    "                                                                        \n",
    "    Best_Cost_Change_From_Individuals  = Get_Best_Cost_Change_From_Individuals(Cost_Change_From_Probe_Samples,Indexes_Of_The_Suggestions_From_The_Different_Algorithms1)                                                                                                                     \n",
    "    Current_Resource_Allocation = np.array([Suggestion_Tools_1.Number_Of_Samples \\\n",
    "                                            ,Suggestion_Tools_2.Number_Of_Samples  \\\n",
    "                                            ,Suggestion_Tools_3.Number_Of_Samples  \\\n",
    "                                            ,Suggestion_Tools_4.Number_Of_Samples ])\n",
    "\n",
    "    if print_Cost == 1:\n",
    "        print(\"Current_Resource_Allocation\",Current_Resource_Allocation)\n",
    "    ## Important note - there is an extra minus so that negative best change is bad and positive best change is good \n",
    "    Recomended_Resource_Allocation = Symmetric_Section_Conductor(Current_Resource_Allocation,-Best_Cost_Change_From_Individuals,Hyperparameters_For_Symmetric_Conductor) \n",
    "    Current_Resource_Allocation = Recomended_Resource_Allocation\n",
    "\n",
    "    Suggestion_Tools_1.Number_Of_Samples =      Current_Resource_Allocation[0]                                                                                                                     \n",
    "    Suggestion_Tools_2.Number_Of_Samples =      Current_Resource_Allocation[1] \n",
    "    Suggestion_Tools_3.Number_Of_Samples =      Current_Resource_Allocation[2]                                                                                                                                  \n",
    "    Suggestion_Tools_4.Number_Of_Samples =      Current_Resource_Allocation[3]    \n",
    "\n",
    "    return Best_Cost_Change_From_Individuals\n",
    "\n",
    "\n",
    "\n",
    "def Update_Best_Individuals_And_Linear_Search_Vector_From_This_Round(Best_Individual,Parameter_ChangeVector,Cost_Change,Suggestion_Tools_1,Suggestion_Tools_2,Suggestion_Tools_3,Suggestion_Tools_4):                                                                                                                           \n",
    "\n",
    "\n",
    "    Suggestion_Tools_1.Best_Individual = Best_Individual                                                                                                                    \n",
    "    Suggestion_Tools_1.Second_Best_Individual = Second_Best_Individual\n",
    "    Suggestion_Tools_1.Parameter_ChangeVector = Parameter_ChangeVector[:,0]                                                                                                                    \n",
    "    Suggestion_Tools_1.Cost_Change = Cost_Change                                                                                                                     \n",
    "\n",
    "    Suggestion_Tools_2.Best_Individual = Best_Individual                                                                                                                    \n",
    "    Suggestion_Tools_2.Second_Best_Individual = Second_Best_Individual\n",
    "    Suggestion_Tools_2.Parameter_ChangeVector = Parameter_ChangeVector [:,0]                                                                                                                   \n",
    "    Suggestion_Tools_2.Cost_Change = Cost_Change                                                                                                                                        \n",
    "\n",
    "    Suggestion_Tools_3.Best_Individual = Best_Individual                                                                                                                    \n",
    "    Suggestion_Tools_3.Second_Best_Individual = Second_Best_Individual\n",
    "    Suggestion_Tools_3.Parameter_ChangeVector = Parameter_ChangeVector[:,0]                                                                                                                    \n",
    "    Suggestion_Tools_3.Cost_Change = Cost_Change                                                                                                                                        \n",
    "\n",
    "    Suggestion_Tools_4.Best_Individual = Best_Individual                                                                                                                    \n",
    "    Suggestion_Tools_4.Second_Best_Individual = Second_Best_Individual\n",
    "    Suggestion_Tools_4.Parameter_ChangeVector = Parameter_ChangeVector[:,0]                                                                                                                    \n",
    "    Suggestion_Tools_4.Cost_Change = Cost_Change  \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def Update_Best_Individuals_And_Costs_From_This_Round(Best_Individual,Index_Of_Best_Individual,Indexes_Of_The_Suggestions_From_The_Different_Algorithms,Suggestion_Tools_1,Suggestion_Tools_2,Suggestion_Tools_3,Suggestion_Tools_4):                                                                                                                           \n",
    "\n",
    "\n",
    "     \n",
    "    \n",
    "    \n",
    "    if Index_Of_Best_Individual > Indexes_Of_The_Suggestions_From_The_Different_Algorithms[0] and Indexes_Of_The_Suggestions_From_The_Different_Algorithms[0] < Indexes_Of_The_Suggestions_From_The_Different_Algorithms[0] + Suggestion_Tools_1.Number_Of_Linear_Samples:\n",
    "        ## This means that the best sample was found in the directional search\n",
    "        ## that means we should find a new one\n",
    "        Linear_Search_Vector = (np.random.rand(len(Best_Individual )) - 0.5)\n",
    "        Linear_Search_Vector = Linear_Search_Vector/np.sqrt(np.sum(Linear_Search_Vector**2))\n",
    "        Suggestion_Tools_1.Linear_Search_Vector   = Linear_Search_Vector \n",
    "        \n",
    "    else:\n",
    "        Linear_Search_Vector = Best_Individual -  Suggestion_Tools_1.Best_Individual\n",
    "        Linear_Search_Vector = Linear_Search_Vector/np.sqrt(np.sum(Linear_Search_Vector**2))\n",
    "        Suggestion_Tools_1.Linear_Search_Vector   = Linear_Search_Vector      \n",
    "    Suggestion_Tools_1.Best_Individual = Best_Individual                                                                                                       \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#     Suggestion_Tools_2.Best_Individual = Best_Individual                                                                                                                    \n",
    "#     Suggestion_Tools_2.Second_Best_Individual = Second_Best_Individual\n",
    "#     Suggestion_Tools_2.Parameter_ChangeVector = Parameter_ChangeVector [:,0]                                                                                                                   \n",
    "#     Suggestion_Tools_2.Cost_Change = Cost_Change                                                                                                                                        \n",
    "\n",
    "#     Suggestion_Tools_3.Best_Individual = Best_Individual                                                                                                                    \n",
    "#     Suggestion_Tools_3.Second_Best_Individual = Second_Best_Individual\n",
    "#     Suggestion_Tools_3.Parameter_ChangeVector = Parameter_ChangeVector[:,0]                                                                                                                    \n",
    "#     Suggestion_Tools_3.Cost_Change = Cost_Change                                                                                                                                        \n",
    "\n",
    "#     Suggestion_Tools_4.Best_Individual = Best_Individual                                                                                                                    \n",
    "#     Suggestion_Tools_4.Second_Best_Individual = Second_Best_Individual\n",
    "#     Suggestion_Tools_4.Parameter_ChangeVector = Parameter_ChangeVector[:,0]                                                                                                                    \n",
    "#     Suggestion_Tools_4.Cost_Change = Cost_Change  \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def Optimize_With_TWO_Mode_Section(Best_Individual,Second_Best_Individual,PreviousCost,fun,current_iteration,Total_Number_Of_Iterations,Number_Of_Iterations_With_These_Hyperparameters,Hyperparameters_1,Hyperparameters_2,Hyperparameters_3,Hyperparameters_4,Suggestion_Tools_1,Suggestion_Tools_2,Suggestion_Tools_3,Suggestion_Tools_4):\n",
    "\n",
    "    \n",
    "        \n",
    "    Best_Cost_Change_From_Individuals1 = np.zeros((Number_Of_Algorithms,Number_Of_Iterations_With_These_Hyperparameters))\n",
    "         \n",
    "    if current_iteration == 0:\n",
    "        Parameter_ChangeVector = np.zeros((len(Best_Individual),1))\n",
    "        Cost_Change = 0\n",
    "        Update_Best_Individuals_And_Costs_From_This_Round(Best_Individual,Second_Best_Individual,Parameter_ChangeVector,Cost_Change,Suggestion_Tools_1,Suggestion_Tools_2,Suggestion_Tools_3,Suggestion_Tools_4)                                                                                                                           \n",
    "\n",
    "                                                                                                                          \n",
    "    for i in range(0,Number_Of_Iterations_With_These_Hyperparameters):\n",
    "        current_iteration= current_iteration + 1\n",
    "        ## Readout ony if wanted                                                                                                                           \n",
    "        if print_Cost == 1:\n",
    "            print(\"____________________________________________________\")\n",
    "            print(\"Iteration\",current_iteration,\"\\\\\",Total_Number_Of_Iterations)\n",
    "\n",
    "        \n",
    "                                                                                                                                \n",
    "        ##  Get Sample Suggestions from the Section  \n",
    "        All_Suggestions_From_Two_Mode_Section1,Indexes_Of_The_Suggestions_From_The_Different_Algorithms1 =  Get_Suggestions_From_Two_Mode_Section(fun,Hyperparameters_1,Hyperparameters_2,Hyperparameters_3,Hyperparameters_4,Suggestion_Tools_1,Suggestion_Tools_2,Suggestion_Tools_3,Suggestion_Tools_4,Hyperparameters_For_Symmetric_Conductor)\n",
    "\n",
    "        \n",
    "        Cost = np.zeros(len(All_Suggestions_From_Two_Mode_Section1[1,:]))                                                                                                                         \n",
    "        ## Sample the Function at the suggested sample points\n",
    "        for j in range(0,len(All_Suggestions_From_Two_Mode_Section1[1,:])):\n",
    "            Cost[j] =fun(All_Suggestions_From_Two_Mode_Section1[:,j])\n",
    "            if np.isnan(Cost[j]):\n",
    "                if j > 1:\n",
    "                    Cost[j] = np.maximum(10**10,np.max(Cost[0:j-1]))\n",
    "                else: \n",
    "                    Cost[j] = 10**10\n",
    "                    \n",
    "        ## Find the Optimzal Sample and calculate the change vectors                                                                                                                          \n",
    "        index_min = np.argmin(Cost)\n",
    "        CurrentCost = Cost[index_min ]\n",
    "        Cost_Change = CurrentCost - PreviousCost\n",
    "        Parameter_ChangeVector = np.array([All_Suggestions_From_Two_Mode_Section1[:,index_min] - Best_Individual]).T\n",
    "\n",
    "\n",
    "        # Update the Addaptive Amplitudes for the next search\n",
    "        # This should minimize the amplitude of ones that have large variance, and increase those which have high variance.\n",
    "        Cost_Change_From_Probe_Samples  = Cost - PreviousCost\n",
    "\n",
    "        Change_Vectors = All_Suggestions_From_Two_Mode_Section1 - np.array([Best_Individual]).T\n",
    "        Gradients =   Cost_Change_From_Probe_Samples/(Change_Vectors + 0.00001)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        Update_Addaptive_Amplitudes(Gradients,Hyperparameters_1,Hyperparameters_2,Hyperparameters_3,Hyperparameters_4,Suggestion_Tools_1,Suggestion_Tools_2,Suggestion_Tools_3,Suggestion_Tools_4)\n",
    "\n",
    "                 \n",
    "        \n",
    "        ## Update Directional Pools\n",
    "        Update_Directional_Pool(Gradients,Cost_Change_From_Probe_Samples,Hyperparameters_1,Hyperparameters_2,Hyperparameters_3,Hyperparameters_4,Suggestion_Tools_1,Suggestion_Tools_2,Suggestion_Tools_3,Suggestion_Tools_4)\n",
    "\n",
    "             \n",
    "\n",
    "\n",
    "         ## Update Resource Allocation   \n",
    "        Best_Cost_Change_From_Individuals1[:,i] = Update_Resource_Allocation(Cost_Change_From_Probe_Samples,Indexes_Of_The_Suggestions_From_The_Different_Algorithms1,Suggestion_Tools_1,Suggestion_Tools_2,Suggestion_Tools_3,Suggestion_Tools_4,Hyperparameters_For_Symmetric_Conductor)\n",
    "                                                                                                        \n",
    "                                                                                                                                                                                                                                                                                                                                                        \n",
    "        ## Update Best_Individuals_And_Costs_From_This_Round   \n",
    "        Best_Individual = All_Suggestions_From_Two_Mode_Section1[:,index_min]\n",
    "\n",
    "        Cost[index_min ] = 10000*Cost[index_min ]\n",
    "        index_min = np.argmin(Cost)\n",
    "        Second_Best_Individual = All_Suggestions_From_Two_Mode_Section1[:,index_min]\n",
    "        PreviousCost = CurrentCost                                                                                                                \n",
    "        Update_Best_Individuals_And_Costs_From_This_Round(Best_Individual,Second_Best_Individual,Parameter_ChangeVector,Cost_Change,Suggestion_Tools_1,Suggestion_Tools_2,Suggestion_Tools_3,Suggestion_Tools_4)                                                                                                                           \n",
    "                                                                                                             \n",
    "                                                                                                                                                                                                                                                             \n",
    "        if print_Cost == 1:\n",
    "            print(\"Run Summary:\")\n",
    "            print(\"---------------------------------------------------------------------\")\n",
    "            print(\"CurrentCost is:\",CurrentCost,\"After Running Iteration\",current_iteration,\"\\\\\",Total_Number_Of_Iterations)\n",
    "            print(\"the Random_Search_exponential_Growth_Factors are :\",Suggestion_Tools_1.Random_Search_exponential_Growth_Factor,\\\n",
    "                  Suggestion_Tools_2.Random_Search_exponential_Growth_Factor,Suggestion_Tools_3.Random_Search_exponential_Growth_Factor,\\\n",
    "                  Suggestion_Tools_4.Random_Search_exponential_Growth_Factor)\n",
    "            print(\"the Targeted_Search_exponential_Growth_Factor is:\",Suggestion_Tools_1.Targeted_Search_exponential_Growth_Factor   ,\\\n",
    "                 Suggestion_Tools_2.Targeted_Search_exponential_Growth_Factor   ,Suggestion_Tools_3.Targeted_Search_exponential_Growth_Factor   ,\\\n",
    "                 Suggestion_Tools_4.Targeted_Search_exponential_Growth_Factor   )    \n",
    "            print(\"---------------------------------------------------------------------\")\n",
    "            print(\"\")            \n",
    "            \n",
    "          \n",
    "    return Best_Individual,Second_Best_Individual,CurrentCost,PreviousCost,Best_Cost_Change_From_Individuals1,current_iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Force_Suggested_Hyperparameters_To_be_Within_The_Allowed_Limits(Suggested_Hyper_Parameter_Samples_From_Two_Mode,Hyperparameter_Allowed_Values):\n",
    "\n",
    "## The 0 second index is the lower bound of the allowed values, and the 1 second index is the upper bound\n",
    "\n",
    "    Suggested_Hyper_Parameter_Samples_From_Two_Mode[0,:] = np.minimum(np.maximum(Suggested_Hyper_Parameter_Samples_From_Two_Mode[0,:],Hyperparameter_Allowed_Values[0,0]),Hyperparameter_Allowed_Values[0,1])\n",
    "    Suggested_Hyper_Parameter_Samples_From_Two_Mode[1,:] = np.minimum(np.maximum(Suggested_Hyper_Parameter_Samples_From_Two_Mode[1,:],Hyperparameter_Allowed_Values[1,0]),Hyperparameter_Allowed_Values[1,1])\n",
    "    Suggested_Hyper_Parameter_Samples_From_Two_Mode[2,:] = np.minimum(np.maximum(Suggested_Hyper_Parameter_Samples_From_Two_Mode[2,:],Hyperparameter_Allowed_Values[2,0]),Hyperparameter_Allowed_Values[2,1])\n",
    "    Suggested_Hyper_Parameter_Samples_From_Two_Mode[3,:] = np.minimum(np.maximum(Suggested_Hyper_Parameter_Samples_From_Two_Mode[3,:],Hyperparameter_Allowed_Values[3,0]),Hyperparameter_Allowed_Values[3,1])\n",
    "    Suggested_Hyper_Parameter_Samples_From_Two_Mode[4,:] = np.minimum(np.maximum(Suggested_Hyper_Parameter_Samples_From_Two_Mode[4,:],Hyperparameter_Allowed_Values[4,0]),Hyperparameter_Allowed_Values[4,1])\n",
    "    Suggested_Hyper_Parameter_Samples_From_Two_Mode[5,:] = np.minimum(np.maximum(Suggested_Hyper_Parameter_Samples_From_Two_Mode[5,:],Hyperparameter_Allowed_Values[5,0]),Hyperparameter_Allowed_Values[5,1])\n",
    "#     print(\"Before\",Suggested_Hyper_Parameter_Samples_From_Two_Mode[6,:])\n",
    "#     Suggested_Hyper_Parameter_Samples_From_Two_Mode[6,:] = np.minimum(np.maximum((10**Suggested_Hyper_Parameter_Samples_From_Two_Mode[6,:]),Hyperparameter_Allowed_Values[6,0]),Hyperparameter_Allowed_Values[6,1])\n",
    "    Suggested_Hyper_Parameter_Samples_From_Two_Mode[6,:] = np.minimum(np.maximum(Suggested_Hyper_Parameter_Samples_From_Two_Mode[6,:],Hyperparameter_Allowed_Values[6,0]),Hyperparameter_Allowed_Values[6,1])\n",
    "#     print(\"After\",Suggested_Hyper_Parameter_Samples_From_Two_Mode[6,:])\n",
    "    Suggested_Hyper_Parameter_Samples_From_Two_Mode[7,:] = np.minimum(np.maximum(Suggested_Hyper_Parameter_Samples_From_Two_Mode[7,:],Hyperparameter_Allowed_Values[7,0]),Hyperparameter_Allowed_Values[7,1])\n",
    "    Suggested_Hyper_Parameter_Samples_From_Two_Mode[8,:] = np.fmin(np.fmax(Suggested_Hyper_Parameter_Samples_From_Two_Mode[8,:],Hyperparameter_Allowed_Values[8,0]),Hyperparameter_Allowed_Values[8,1])\n",
    "    Suggested_Hyper_Parameter_Samples_From_Two_Mode[9,:] = np.fmin(np.fmax(Suggested_Hyper_Parameter_Samples_From_Two_Mode[9,:],Hyperparameter_Allowed_Values[9,0]),Hyperparameter_Allowed_Values[9,1])\n",
    "    Suggested_Hyper_Parameter_Samples_From_Two_Mode[10,:] = np.fmin(np.fmax(Suggested_Hyper_Parameter_Samples_From_Two_Mode[10,:],Hyperparameter_Allowed_Values[10,0]),Hyperparameter_Allowed_Values[10,1])\n",
    "\n",
    "    \n",
    "    Hyperparameters_1 = Suggested_Hyper_Parameter_Samples_From_Two_Mode[:,0]\n",
    "    Hyperparameters_2 = Suggested_Hyper_Parameter_Samples_From_Two_Mode[:,1]\n",
    "    Hyperparameters_3 = Suggested_Hyper_Parameter_Samples_From_Two_Mode[:,2]\n",
    "    Hyperparameters_4 = Suggested_Hyper_Parameter_Samples_From_Two_Mode[:,3]\n",
    "\n",
    "#     Suggested_Hyper_Parameter_Samples_From_Two_Mode[6,:] = np.log10(Suggested_Hyper_Parameter_Samples_From_Two_Mode[6,:])\n",
    "#     print(\"After2\",Suggested_Hyper_Parameter_Samples_From_Two_Mode[6,:])\n",
    "    return Hyperparameters_1,Hyperparameters_2,Hyperparameters_3,Hyperparameters_4,Suggested_Hyper_Parameter_Samples_From_Two_Mode\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# def Optimize_With_TSTM(InitialParameters,Cost1,NumberOfSamples,TargetedSearchDecayRate,RandomSearchGrowthRate,TargetedMultiplicationFactor ,RandomMultiplicationFactor,SignificantChangeValue,Maximal_Random_Search,print_Cost,Number_Of_Itterations,alpha,beta):\n",
    "    \n",
    "def Run_Optimization_With_TWO_Mode_Online_Hyperparameter_Addaptation(InitialParameters,fun,Total_Number_Of_Iterations,Number_Of_Iterations_With_Hyperparameters_Set,Hyperparameters_1,Hyperparameters_2,Hyperparameters_3,Hyperparameters_4,Suggestion_Tools_1,Suggestion_Tools_2,Suggestion_Tools_3,Suggestion_Tools_4,Hyperparameters_For_Online_Hyperparameter_Search,print_Cost,Total_Resources,Hyperparameter_Allowed_Values):\n",
    "\n",
    "\n",
    "    Number_Of_Iterations_With_These_Hyperparameters = Number_Of_Iterations_With_Hyperparameters_Set\n",
    "\n",
    "    \n",
    "    Random_Search_Growth_Rate  = Hyperparameters_For_Online_Hyperparameter_Search[0]\n",
    "    Random_Search_Period = Hyperparameters_For_Online_Hyperparameter_Search[1]\n",
    "    Maximal_Random_Search =Hyperparameters_For_Online_Hyperparameter_Search[2]\n",
    "    Targeted_Search_Growth_Rate = Hyperparameters_For_Online_Hyperparameter_Search[3]\n",
    "    Targeted_Search_Decay_Rate = Hyperparameters_For_Online_Hyperparameter_Search[4]\n",
    "    Significant_Change_Value = Hyperparameters_For_Online_Hyperparameter_Search[5]\n",
    "    alpha = Hyperparameters_For_Online_Hyperparameter_Search[6]\n",
    "    beta = Hyperparameters_For_Online_Hyperparameter_Search[7]\n",
    "    Hyper_Parameter_Directional_Pool_Ratio = Hyperparameters_For_Online_Hyperparameter_Search[8]\n",
    "    Hyper_Parameter_Pool_Size = Hyperparameters_For_Online_Hyperparameter_Search[9]\n",
    "    Hyper_Parameter_Pool_Beta = Hyperparameters_For_Online_Hyperparameter_Search[10]\n",
    "    \n",
    "    Hyper_Parameter_Directional_Pool = np.random.rand(11,int(Hyper_Parameter_Pool_Size))\n",
    "    ## Maybe add the three hyperparameters for the directional pool here too\n",
    "    \n",
    "    \n",
    "    ## Initialize the suggestion tools for the hyperparameter optimization\n",
    "    Random_Search_exponential_Growth_Factor = 0\n",
    "    Targeted_Search_exponential_Growth_Factor = 0\n",
    "    Hyper_Parameter_ChangeVector = np.zeros(len(Hyperparameters_1))\n",
    "    Hyper_Parameter_Cost_Change = 0\n",
    "    current_iteration = 0\n",
    "    \n",
    "    \n",
    "    ## This is to set the hyperparameters for optimization\n",
    "    Number_Of_Samples = Number_Of_Algorithms\n",
    "    Best_Individual_Hyperparameters = Hyperparameters_1\n",
    "    Second_Best_Individual_Hyperparameters = Hyperparameters_1\n",
    "    \n",
    "    ## This is to set the function parameters for optimization    \n",
    "    Best_Individual_Parameters =     InitialParameters\n",
    "    Second_Best_Individual_Parameters =     InitialParameters\n",
    "    PreviousCost = fun(InitialParameters)\n",
    "\n",
    "\n",
    "    ## Update Directional Pools with random gradients\n",
    "    Random_Parameter_Gradients = np.random.rand(len(InitialParameters),Total_Resources)\n",
    "    Random_Parameter_Gradients[:,0] = 0\n",
    "    Cost_Change_From_Probe_Samples = -np.ones(Total_Resources)\n",
    "    Update_Directional_Pool(Random_Parameter_Gradients,Cost_Change_From_Probe_Samples,Hyperparameters_1,Hyperparameters_2,Hyperparameters_3,Hyperparameters_4,Suggestion_Tools_1,Suggestion_Tools_2,Suggestion_Tools_3,Suggestion_Tools_4)\n",
    "\n",
    "    ## This is find the Cost of the first set of hyperparameters\n",
    "    Best_Individual_Parameters,Second_Best_Individual_Parameters,CurrentCost,PreviousCost,Best_Cost_Change_From_Individuals1,current_iteration =  Optimize_With_TWO_Mode_Section(Best_Individual_Parameters,Second_Best_Individual_Parameters,PreviousCost,fun,current_iteration,Total_Number_Of_Iterations,Number_Of_Iterations_With_These_Hyperparameters,Hyperparameters_1,Hyperparameters_2,Hyperparameters_3,Hyperparameters_4,Suggestion_Tools_1,Suggestion_Tools_2,Suggestion_Tools_3,Suggestion_Tools_4)\n",
    "\n",
    "    Cost_Function_For_Hyperparameter_Optimization =( np.mean(Best_Cost_Change_From_Individuals1,1) + np.max(Best_Cost_Change_From_Individuals1,1))/2\n",
    "    \n",
    "    index_min = np.argmin(Cost_Function_For_Hyperparameter_Optimization)\n",
    "    Current_Hyperparameter_Cost = Cost_Function_For_Hyperparameter_Optimization[index_min ]\n",
    "\n",
    "    Previous_Hyperparameter_Cost = Cost_Function_For_Hyperparameter_Optimization[index_min]\n",
    "\n",
    "\n",
    "    Adaptive_Amplitude = np.ones((len(Hyperparameters_1),Number_Of_Samples))\n",
    "    Cost_Function_For_Hyperparameter_Optimization = np.zeros(Number_Of_Samples)\n",
    "    S = 0\n",
    "    \n",
    "       \n",
    "    Number_Of_Hyperparameter_Iterations = np.floor(Total_Number_Of_Iterations/Number_Of_Iterations_With_These_Hyperparameters)\n",
    "\n",
    "    for i in range(0,int(Number_Of_Hyperparameter_Iterations)):\n",
    "        \n",
    "\n",
    "        ## Generate suggested hyperparameters    \n",
    "        Suggested_Hyper_Parameter_Samples_From_Two_Mode, Random_Search_exponential_Growth_Factor, Targeted_Search_exponential_Growth_Factor =     Two_Mode_Optimizer(Best_Individual_Hyperparameters,Second_Best_Individual_Hyperparameters,Number_Of_Samples,Hyper_Parameter_ChangeVector,Hyper_Parameter_Cost_Change, Random_Search_exponential_Growth_Factor,Targeted_Search_exponential_Growth_Factor,Targeted_Search_Decay_Rate,Random_Search_Growth_Rate,Targeted_Search_Growth_Rate,Random_Search_Period,Significant_Change_Value,Maximal_Random_Search,Adaptive_Amplitude,Hyper_Parameter_Directional_Pool_Ratio, Hyper_Parameter_Pool_Size, Hyper_Parameter_Directional_Pool)\n",
    "        ## Force the hyperparameters to be within the allowed range\n",
    "\n",
    "        Hyperparameters_1,Hyperparameters_2,Hyperparameters_3,Hyperparameters_4,Suggested_Hyper_Parameter_Samples_From_Two_Mode = Force_Suggested_Hyperparameters_To_be_Within_The_Allowed_Limits(Suggested_Hyper_Parameter_Samples_From_Two_Mode,Hyperparameter_Allowed_Values)\n",
    " \n",
    "\n",
    "        ## Update Directional Pools with random gradients\n",
    "        Random_Parameter_Gradients = np.random.rand(len(InitialParameters),Total_Resources)\n",
    "        Random_Parameter_Gradients[:,0] = 0\n",
    "        Cost_Change_From_Probe_Samples = -np.ones(Total_Resources)\n",
    "        Re_Initialize_Directional_Pool(Random_Parameter_Gradients,Cost_Change_From_Probe_Samples,Hyperparameters_1,Hyperparameters_2,Hyperparameters_3,Hyperparameters_4,Suggestion_Tools_1,Suggestion_Tools_2,Suggestion_Tools_3,Suggestion_Tools_4)\n",
    "\n",
    "    \n",
    "    \n",
    "        ## Try to optimize with these hyperparameters (Maybe output the suggestion tools as well?)\n",
    "        Best_Individual_Parameters,Second_Best_Individual_Parameters,CurrentCost,PreviousCost,Best_Cost_Change_From_Individuals1,current_iteration =  Optimize_With_TWO_Mode_Section(Best_Individual_Parameters,Second_Best_Individual_Parameters,PreviousCost,fun,current_iteration,Total_Number_Of_Iterations,Number_Of_Iterations_With_These_Hyperparameters,Hyperparameters_1,Hyperparameters_2,Hyperparameters_3,Hyperparameters_4,Suggestion_Tools_1,Suggestion_Tools_2,Suggestion_Tools_3,Suggestion_Tools_4)\n",
    "       ## Calculate the Cost_Function_For_Hyperparameter_Optimization\n",
    "    \n",
    "\n",
    "        Cost_Function_For_Hyperparameter_Optimization =( np.mean(Best_Cost_Change_From_Individuals1,1) + np.min(Best_Cost_Change_From_Individuals1,1))/2\n",
    "        Cost_Function_For_Hyperparameter_Optimization1 = np.array([Cost_Function_For_Hyperparameter_Optimization[0],Cost_Function_For_Hyperparameter_Optimization[1],Cost_Function_For_Hyperparameter_Optimization[2],Cost_Function_For_Hyperparameter_Optimization[3]])\n",
    "\n",
    "        \n",
    "        ## Find the Best set of hyperparameters and the change vectors\n",
    "        index_min = np.argmin(Cost_Function_For_Hyperparameter_Optimization)\n",
    "        Current_Hyperparameter_Cost = Cost_Function_For_Hyperparameter_Optimization[index_min ]\n",
    "        Hyper_Parameter_Cost_Change = Current_Hyperparameter_Cost - Previous_Hyperparameter_Cost\n",
    "        Hyper_Parameter_ChangeVector = Suggested_Hyper_Parameter_Samples_From_Two_Mode[:,index_min] - Best_Individual_Hyperparameters\n",
    "\n",
    "\n",
    "        \n",
    "        ## This is the RMS Amplitude Addaptation - For the hyperparameter search\n",
    "        Cost_Change_From_Probe_Samples  = Cost_Function_For_Hyperparameter_Optimization - Cost_Function_For_Hyperparameter_Optimization[0]\n",
    "\n",
    "        Change_Vectors = Suggested_Hyper_Parameter_Samples_From_Two_Mode - np.array([Best_Individual_Hyperparameters]).T\n",
    "        Gradient =   Cost_Change_From_Probe_Samples/(Change_Vectors + 0.001)\n",
    "\n",
    "        S = beta*S + (1-beta)*(np.mean(np.abs(Gradient),1))**2\n",
    "        S = S*np.sign(np.mean(Gradient,1))\n",
    "        Adaptive_Amplitude1 = alpha/np.sqrt(np.fmax(S + alpha**2,0.1*alpha**2))\n",
    "        Adaptive_Amplitude[:,:] = np.array([Adaptive_Amplitude1]).T ## this makes an parameter wise adaptive amplitude Based on RMS prob, which we can do element wise multiplication with the random searches.\n",
    "        # This should minimize the amplitude of ones that have large variance, and increase those which have high variance.\n",
    "\n",
    "        \n",
    "        ## Save the Best Sets of hyperparameters and Cost Function\n",
    "        Best_Individual_Hyperparameters = Suggested_Hyper_Parameter_Samples_From_Two_Mode[:,index_min]\n",
    "        Cost_Function_For_Hyperparameter_Optimization[index_min ] = (10**10) + Cost_Function_For_Hyperparameter_Optimization[index_min ]\n",
    "        index_min = np.argmin(Cost_Function_For_Hyperparameter_Optimization)\n",
    "        Second_Best_Individual_Hyperparameters = Suggested_Hyper_Parameter_Samples_From_Two_Mode[:,index_min]\n",
    "        \n",
    "\n",
    "        Previous_Hyperparameter_Cost = Current_Hyperparameter_Cost\n",
    "        \n",
    "\n",
    "        \n",
    "        if print_Cost == 1:\n",
    "            print(\"hyperparameter Summary:\")\n",
    "            print(\"---------------------------------------------------------------------\")\n",
    "#             print(\"CurrentCost is:\",CurrentCost,\"After Running Iteration\",current_iteration,\"\\\\\",Total_Number_Of_Iterations)\n",
    "#             print(\"\")\n",
    "            print(\"After Running hyperparameter Iteration\",i,\"\\\\\",int(Number_Of_Hyperparameter_Iterations))\n",
    "            print(\"Cost_Function_For_Hyperparameter_Optimization\",Cost_Function_For_Hyperparameter_Optimization1)\n",
    "           \n",
    "            print(\"\")\n",
    "            print(\"Best_Individual_Hyperparameters\",Best_Individual_Hyperparameters)\n",
    "            print(\"\")\n",
    "            print(\"the Random_Search_exponential_Growth_Factor for the hyperparameter search is:\",Random_Search_exponential_Growth_Factor)\n",
    "            print(\"the Targeted_Search_exponential_Growth_Factor for the hyperparameter search is:\",Targeted_Search_exponential_Growth_Factor)    \n",
    "    \n",
    "            print(\"---------------------------------------------------------------------\")\n",
    "            print(\"\")            \n",
    "            \n",
    "\n",
    "    return Best_Individual_Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Set_Hyperparameter_Ranges(Random_Search_Growth_Rate_Range, Random_Search_Period_Range   ,Maximal_Random_Search_Range,Targeted_Search_Growth_Rate_Range ,Targeted_Search_Decay_Rate_Range ,Significant_Change_Value_Range,alpha_Range ,beta_Range ,Directional_Pool_Ratio_Range,Pool_Size_Range,Pool_beta_Range):\n",
    "    Hyperparameter_Allowed_Values = np.zeros((11,2))\n",
    "\n",
    "    Hyperparameter_Allowed_Values[0,:] = Random_Search_Growth_Rate_Range\n",
    "    Hyperparameter_Allowed_Values[1,:] = Random_Search_Period_Range    \n",
    "    Hyperparameter_Allowed_Values[2,:] = Maximal_Random_Search_Range    \n",
    "    Hyperparameter_Allowed_Values[3,:] = Targeted_Search_Growth_Rate_Range    \n",
    "    Hyperparameter_Allowed_Values[4,:] = Targeted_Search_Decay_Rate_Range   \n",
    "    Hyperparameter_Allowed_Values[5,:] = Significant_Change_Value_Range\n",
    "    Hyperparameter_Allowed_Values[6,:] = alpha_Range\n",
    "    Hyperparameter_Allowed_Values[7,:] = beta_Range\n",
    "    Hyperparameter_Allowed_Values[8,:] = Directional_Pool_Ratio_Range\n",
    "    Hyperparameter_Allowed_Values[9,:] = Pool_Size_Range\n",
    "    Hyperparameter_Allowed_Values[10,:] = Pool_beta_Range\n",
    "\n",
    "    return Hyperparameter_Allowed_Values\n",
    "\n",
    "\n",
    "def Mean_ChangeValue_fit(Maximal_Random_Search,Number_Of_Parameters):\n",
    "    fit_Parameters = [-0.007921662,0.003755446,5.92*(10**(-5)),-2.45*(10**(-7)),0.250014494] # These were found numerically from the file \"Finding_The_Segnificant_Change_Value.ipynb\"\n",
    "    return    fit_Parameters[0]*Maximal_Random_Search + fit_Parameters[1]*Maximal_Random_Search**2 + fit_Parameters[2]*Number_Of_Parameters + fit_Parameters[3]*Number_Of_Parameters**2 + fit_Parameters[4]*Maximal_Random_Search*Number_Of_Parameters\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Orchestra_Optimize_With_Pinball_Section(x0,fun,Number_Of_Iterations,print_Cost):\n",
    "\n",
    "    ## This will be the Two Mode Conductor\n",
    "\n",
    "    Number_Of_Parameters = len(x0)\n",
    "    InitialParameters = x0\n",
    "    Total_Number_Of_Iterations = Number_Of_Iterations\n",
    "    \n",
    "    Number_Of_Iterations_With_Hyperparameters_Set = 5\n",
    "    \n",
    "    \n",
    "\n",
    "    ############################## Setting up The Conductor (Resource allocation) algorithm ###################################\n",
    "    Total_Resources = 60\n",
    "    Number_Of_Algorithms = 4\n",
    "    Mass =10\n",
    "    Self_Spring_Constants = 10\n",
    "    Interaction_Spring_Constants = 40\n",
    "\n",
    "    Hyperparameters_For_Symmetric_Conductor = Set_Hyperparameters_For_Symmetric_Conductor(Total_Resources,Number_Of_Algorithms,Mass,Self_Spring_Constants,Interaction_Spring_Constants)\n",
    "    Initial_Resource_Allocation = Hyperparameters_For_Symmetric_Conductor[:,0]\n",
    "    Current_Resource_Allocation = Hyperparameters_For_Symmetric_Conductor[:,0]\n",
    "\n",
    "    Changevector = np.array([0, 0 ,0 ,0]) ## Negative is bad\n",
    "    Recomended_Resource_Allocation = Symmetric_Section_Conductor(Current_Resource_Allocation,Changevector,Hyperparameters_For_Symmetric_Conductor) \n",
    "    Current_Resource_Allocation = Recomended_Resource_Allocation\n",
    "#     print(\"Current_Resource_Allocation\",Current_Resource_Allocation)\n",
    "    #############################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    ######################### Setting the hyperparameter ranges ######################################################################\n",
    "    Random_Search_Growth_Rate_Range   = np.array([0.01,10])\n",
    "    Random_Search_Period_Range  = np.array([0.1,100])\n",
    "    Maximal_Random_Search_Range  = np.array([0.00001,10])\n",
    "    Targeted_Search_Growth_Rate_Range  = np.array([0.01,5])\n",
    "    Targeted_Search_Decay_Rate_Range  = np.array([0.1,100]) \n",
    "    Significant_Change_Value_Range  = np.array([10**-30,0.0001]) \n",
    "    alpha_Range = np.array([10**-5,10**15]) ## (Here Do 10**random)\n",
    "    beta_Range = np.array([0.1,1]) \n",
    "    Directional_Pool_Ratio_Range = np.array([0.0001,1]) \n",
    "    Pool_Size_Range = np.array([0.1,2.0]) \n",
    "    Pool_beta_Range = np.array([0.1,1]) \n",
    "\n",
    "    Hyperparameter_Allowed_Values = Set_Hyperparameter_Ranges(Random_Search_Growth_Rate_Range, Random_Search_Period_Range   ,Maximal_Random_Search_Range,Targeted_Search_Growth_Rate_Range ,Targeted_Search_Decay_Rate_Range ,Significant_Change_Value_Range,alpha_Range ,beta_Range ,Directional_Pool_Ratio_Range,Pool_Size_Range,Pool_beta_Range)\n",
    "    #############################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ############################### Setting the Hyperparameters_For_Online_Hyperparameter_improvement  Algorithm #########################\n",
    "    Hyperparameters_For_Online_Hyperparameter_Search = np.zeros(11)\n",
    "    Hyperparameters_For_Online_Hyperparameter_Search[0] =5.0   # Random_Search_Growth_Rate \n",
    "    Hyperparameters_For_Online_Hyperparameter_Search[1] =5.0    # Random_Search_Period \n",
    "    Hyperparameters_For_Online_Hyperparameter_Search[2] =20.02   # Maximal_Random_Search \n",
    "    Hyperparameters_For_Online_Hyperparameter_Search[3] =0.5    # Targeted_Search_Growth_Rate\n",
    "    Hyperparameters_For_Online_Hyperparameter_Search[4] =2.0    # Targeted_Search_Decay_Rate \n",
    "    Hyperparameters_For_Online_Hyperparameter_Search[5] =0.00001*Mean_ChangeValue_fit(Hyperparameters_For_Online_Hyperparameter_Search[2] ,8)    # Significant_Change_Value\n",
    "    Hyperparameters_For_Online_Hyperparameter_Search[6] =(10**4)/Hyperparameters_For_Online_Hyperparameter_Search[2]    # alpha \n",
    "    Hyperparameters_For_Online_Hyperparameter_Search[7] =0.6     # beta\n",
    "    Hyperparameters_For_Online_Hyperparameter_Search[8] =0.6     # directional ratio\n",
    "    Hyperparameters_For_Online_Hyperparameter_Search[9] =3     # pool size\n",
    "    Hyperparameters_For_Online_Hyperparameter_Search[10] =0.7     # pool beta\n",
    "    #############################################################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ########################initialize the different Suggestion_Tools For the Two Mode and the hyperparameters #######################\n",
    "    Suggestion_Tools_1 = Create_New_Two_Mode_Suggestion_Tools_Object(Number_Of_Parameters)\n",
    "    Suggestion_Tools_2 = Create_New_Two_Mode_Suggestion_Tools_Object(Number_Of_Parameters)\n",
    "    Suggestion_Tools_3 = Create_New_Two_Mode_Suggestion_Tools_Object(Number_Of_Parameters)\n",
    "    Suggestion_Tools_4 = Create_New_Two_Mode_Suggestion_Tools_Object(Number_Of_Parameters)\n",
    "\n",
    "    Suggestion_Tools_1.Number_Of_Samples =      Current_Resource_Allocation[0]                                                                                                                     \n",
    "    Suggestion_Tools_2.Number_Of_Samples =      Current_Resource_Allocation[1] \n",
    "    Suggestion_Tools_3.Number_Of_Samples =      Current_Resource_Allocation[2]                                                                                                                                  \n",
    "    Suggestion_Tools_4.Number_Of_Samples =      Current_Resource_Allocation[3]  \n",
    "\n",
    "\n",
    "\n",
    "    ## This is to initiailize the Hyperparameters - Note that they should all be positive numbers (Maybe make 6 and 7 be 10**rand(2))\n",
    "    Initial_Hyper_Parameter_Samples = 10*np.random.rand(11,Number_Of_Algorithms)\n",
    "    Hyperparameters_1,Hyperparameters_2,Hyperparameters_3,Hyperparameters_4,Initial_Hyper_Parameter_Samples = Force_Suggested_Hyperparameters_To_be_Within_The_Allowed_Limits(Initial_Hyper_Parameter_Samples,Hyperparameter_Allowed_Values)\n",
    "    #############################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    Best_Parameters = Run_Optimization_With_TWO_Mode_Online_Hyperparameter_Addaptation(InitialParameters,fun,Total_Number_Of_Iterations,Number_Of_Iterations_With_Hyperparameters_Set,Hyperparameters_1,Hyperparameters_2,Hyperparameters_3,Hyperparameters_4,Suggestion_Tools_1,Suggestion_Tools_2,Suggestion_Tools_3,Suggestion_Tools_4,Hyperparameters_For_Online_Hyperparameter_Search,print_Cost,Total_Resources,Hyperparameter_Allowed_Values)\n",
    "\n",
    "    return Best_Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost 4028914.409437531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benjy\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:68: RuntimeWarning: overflow encountered in square\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Cost 2.65380651641211\n",
      "log10 Total_Evaluations /D=  4.079181246047625\n"
     ]
    }
   ],
   "source": [
    "def fun(x): ## Ellispod\n",
    "#     return np.sum((x-2)**2) + np.sum(np.sin(x))\n",
    "\n",
    "    a = 1000000**(np.arange(D)/D)\n",
    "    return np.sum(a*((x-X_Optimal)**2) )\n",
    "\n",
    "Number_Of_Iterations = 2000\n",
    "\n",
    "D = 10\n",
    "x0 = 10*(np.random.rand(D)-0.5)\n",
    "X_Optimal = 10*(np.random.rand(D)-0.5)\n",
    "\n",
    "print_Cost = 0\n",
    "Print_Two_Mode_Analytics = 0\n",
    "\n",
    "print(\"Initial Cost\",fun(x0))\n",
    "Best_Parameters_After_Optimization = Orchestra_Optimize_With_Two_Mode_Section(x0,fun,Number_Of_Iterations,print_Cost)\n",
    "print(\"Final Cost\",fun(Best_Parameters_After_Optimization))\n",
    "print(\"log10 Total_Evaluations /D= \",np.log10(60*Number_Of_Iterations/D))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two_Mode_Section\n",
      "True\n",
      "47.12590898523435\n",
      "2.92033535691153e-12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benjy\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:68: RuntimeWarning: overflow encountered in square\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Defining Cost Function\n",
    "class Two_Mode_Section:\n",
    "    \n",
    "    Name = 'Two_Mode_Section'\n",
    "\n",
    "    def __init__(self, Name):\n",
    "        self.__name__ = 'Two_Mode_Section'\n",
    "\n",
    "        \n",
    "    def Optimize(fun, x0, Number_Of_Iterations ): \n",
    "        print_Cost = 0\n",
    "\n",
    "        if len(x0) >= 5:\n",
    "            print_Cost = 0\n",
    "\n",
    "        Best_Parameters_After_Optimization = Orchestra_Optimize_With_Two_Mode_Section(x0,fun,Number_Of_Iterations,print_Cost)\n",
    "        \n",
    "  \n",
    "        x_min = Best_Parameters_After_Optimization\n",
    "        return x_min\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "SOLVER = Two_Mode_Section\n",
    "\n",
    "print(SOLVER.__name__)\n",
    "print(SOLVER.__name__ == 'Two_Mode_Section')\n",
    "\n",
    "def fun(x):\n",
    "    return np.sum((x-2)**2)\n",
    "\n",
    "\n",
    "x0= np.random.rand(20)\n",
    "remaining_evals = 500\n",
    "print(fun(x0))\n",
    "x_min1  = SOLVER.Optimize(fun, x0,remaining_evals)\n",
    "print(fun(x_min1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking solver '<class '__main__.Two_Mode_Section'>' with budget=1000*dimension on b'bbob' suite, Sat Feb 23 11:12:08 2019\n",
      "Feb23 11h12:08, d=2, running: f001......"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-72146ea0fbd0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    292\u001b[0m         \u001b[0mmessages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'See \"python example_experiment.py -h\" for help.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbudget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_runs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-72146ea0fbd0>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(budget, max_runs, current_batch, number_of_batches)\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[0mt0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m     batch_loop(SOLVER, suite, observer, budget, max_runs,\n\u001b[1;32m--> 257\u001b[1;33m                current_batch, number_of_batches)\n\u001b[0m\u001b[0;32m    258\u001b[0m     print(\", %s (%s total elapsed time).\" %\n\u001b[0;32m    259\u001b[0m             (time.asctime(), ascetime(time.clock() - t0)))\n",
      "\u001b[1;32m<ipython-input-13-72146ea0fbd0>\u001b[0m in \u001b[0;36mbatch_loop\u001b[1;34m(solver, suite, observer, budget, max_runs, current_batch, number_of_batches)\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[0mshort_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproblem\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         runs = coco_optimize(solver, problem, budget * problem.dimension,\n\u001b[1;32m--> 111\u001b[1;33m                              max_runs)\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mprint_flush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"!\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mruns\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\":\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mruns\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\".\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-72146ea0fbd0>\u001b[0m in \u001b[0;36mcoco_optimize\u001b[1;34m(solver, fun, max_evals, max_runs)\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[1;31m#     CALL MY SOLVER, interfaces vary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m             \u001b[0msolver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mremaining_evals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[1;31m##############################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-c3a6c0c26e04>\u001b[0m in \u001b[0;36mOptimize\u001b[1;34m(fun, x0, Number_Of_Iterations)\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mprint_Cost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mBest_Parameters_After_Optimization\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOrchestra_Optimize_With_Two_Mode_Section\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNumber_Of_Iterations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprint_Cost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-ef41e8f6d142>\u001b[0m in \u001b[0;36mOrchestra_Optimize_With_Two_Mode_Section\u001b[1;34m(x0, fun, Number_Of_Iterations, print_Cost)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m     \u001b[0mBest_Parameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRun_Optimization_With_TWO_Mode_Online_Hyperparameter_Addaptation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mInitialParameters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTotal_Number_Of_Iterations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNumber_Of_Iterations_With_Hyperparameters_Set\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mHyperparameters_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mHyperparameters_2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mHyperparameters_3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mHyperparameters_4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mSuggestion_Tools_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mSuggestion_Tools_2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mSuggestion_Tools_3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mSuggestion_Tools_4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mHyperparameters_For_Online_Hyperparameter_Search\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprint_Cost\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTotal_Resources\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mHyperparameter_Allowed_Values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mBest_Parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-00c218c8faa5>\u001b[0m in \u001b[0;36mRun_Optimization_With_TWO_Mode_Online_Hyperparameter_Addaptation\u001b[1;34m(InitialParameters, fun, Total_Number_Of_Iterations, Number_Of_Iterations_With_Hyperparameters_Set, Hyperparameters_1, Hyperparameters_2, Hyperparameters_3, Hyperparameters_4, Suggestion_Tools_1, Suggestion_Tools_2, Suggestion_Tools_3, Suggestion_Tools_4, Hyperparameters_For_Online_Hyperparameter_Search, print_Cost, Total_Resources, Hyperparameter_Allowed_Values)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[1;31m## Try to optimize with these hyperparameters (Maybe output the suggestion tools as well?)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m         \u001b[0mBest_Individual_Parameters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mSecond_Best_Individual_Parameters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mCurrentCost\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mPreviousCost\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mBest_Cost_Change_From_Individuals1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcurrent_iteration\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mOptimize_With_TWO_Mode_Section\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBest_Individual_Parameters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mSecond_Best_Individual_Parameters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mPreviousCost\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcurrent_iteration\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTotal_Number_Of_Iterations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNumber_Of_Iterations_With_These_Hyperparameters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mHyperparameters_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mHyperparameters_2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mHyperparameters_3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mHyperparameters_4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mSuggestion_Tools_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mSuggestion_Tools_2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mSuggestion_Tools_3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mSuggestion_Tools_4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m        \u001b[1;31m## Calculate the Cost_Function_For_Hyperparameter_Optimization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-b43c86b94db7>\u001b[0m in \u001b[0;36mOptimize_With_TWO_Mode_Section\u001b[1;34m(Best_Individual, Second_Best_Individual, PreviousCost, fun, current_iteration, Total_Number_Of_Iterations, Number_Of_Iterations_With_These_Hyperparameters, Hyperparameters_1, Hyperparameters_2, Hyperparameters_3, Hyperparameters_4, Suggestion_Tools_1, Suggestion_Tools_2, Suggestion_Tools_3, Suggestion_Tools_4)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m       \u001b[1;31m## Update Resource Allocation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m      \u001b[0mBest_Cost_Change_From_Individuals1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUpdate_Resource_Allocation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCost_Change_From_Probe_Samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mIndexes_Of_The_Suggestions_From_The_Different_Algorithms1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mSuggestion_Tools_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mSuggestion_Tools_2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mSuggestion_Tools_3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mSuggestion_Tools_4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mHyperparameters_For_Symmetric_Conductor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-b43c86b94db7>\u001b[0m in \u001b[0;36mUpdate_Resource_Allocation\u001b[1;34m(Cost_Change_From_Probe_Samples, Indexes_Of_The_Suggestions_From_The_Different_Algorithms1, Suggestion_Tools_1, Suggestion_Tools_2, Suggestion_Tools_3, Suggestion_Tools_4, Hyperparameters_For_Symmetric_Conductor)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m  \u001b[0mBest_Cost_Change_From_Individuals\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mGet_Best_Cost_Change_From_Individuals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCost_Change_From_Probe_Samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mIndexes_Of_The_Suggestions_From_The_Different_Algorithms1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m  \u001b[0mCurrent_Resource_Allocation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mSuggestion_Tools_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNumber_Of_Samples\u001b[0m                                          \u001b[1;33m,\u001b[0m\u001b[0mSuggestion_Tools_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNumber_Of_Samples\u001b[0m                                           \u001b[1;33m,\u001b[0m\u001b[0mSuggestion_Tools_3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNumber_Of_Samples\u001b[0m                                           \u001b[1;33m,\u001b[0m\u001b[0mSuggestion_Tools_4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNumber_Of_Samples\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-b43c86b94db7>\u001b[0m in \u001b[0;36mGet_Best_Cost_Change_From_Individuals\u001b[1;34m(Cost_Change_From_Probe_Samples, Indexes_Of_The_Suggestions_From_The_Different_Algorithms1)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m  \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m      \u001b[0mindex_min\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCost_Change_From_Probe_Samples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mIndexes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mIndexes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;31m#         print(\"index_min For the individuals\",index_min)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m      \u001b[0mBest_Cost_Change_From_Individuals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCost_Change_From_Probe_Samples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mIndexes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mIndexes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex_min\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36margmin\u001b[1;34m(a, axis, out)\u001b[0m\n\u001b[0;32m   1099\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m     \"\"\"\n\u001b[1;32m-> 1101\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'argmin'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;31m# An AttributeError occurs if the object does not have\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"Python script for the COCO experimentation module `cocoex`.\n",
    "\n",
    "Usage from a system shell::\n",
    "\n",
    "    python example_experiment.py bbob\n",
    "\n",
    "runs a full but short experiment on the bbob suite. The optimization\n",
    "algorithm used is determined by the `SOLVER` attribute in this file::\n",
    "\n",
    "    python example_experiment.py bbob 20\n",
    "\n",
    "runs the same experiment but with a budget of 20 * dimension\n",
    "f-evaluations::\n",
    "\n",
    "    python example_experiment.py bbob-biobj 1e3 1 20\n",
    "\n",
    "runs the first of 20 batches with maximal budget of\n",
    "1000 * dimension f-evaluations on the bbob-biobj suite.\n",
    "All batches must be run to generate a complete data set.\n",
    "\n",
    "Usage from a python shell:\n",
    "\n",
    ">>> import example_experiment as ee\n",
    ">>> ee.suite_name = \"bbob-biobj\"\n",
    ">>> ee.SOLVER = ee.random_search  # which is default anyway\n",
    ">>> ee.observer_options['algorithm_info'] = '\"default of example_experiment.py\"'\n",
    ">>> ee.main(5, 1+9, 2, 300)  # doctest: +ELLIPSIS\n",
    "Benchmarking solver...\n",
    "\n",
    "runs the 2nd of 300 batches with budget 5 * dimension and at most 9 restarts.\n",
    "\n",
    "Calling `example_experiment` without parameters prints this\n",
    "help and the available suite names.\n",
    "\"\"\"\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "try: range = xrange\n",
    "except NameError: pass\n",
    "import os, sys\n",
    "import time\n",
    "import numpy as np  # \"pip install numpy\" installs numpy\n",
    "import cocoex\n",
    "from scipy import optimize # for tests with fmin_cobyla\n",
    "from cocoex import Suite, Observer, log_level\n",
    "del absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "verbose = 1\n",
    "\n",
    "try: import cma  # cma.fmin is a solver option, \"pip install cma\" installs cma\n",
    "except: pass\n",
    "try: from scipy.optimize import fmin_slsqp  # \"pip install scipy\" installs scipy\n",
    "except: pass\n",
    "try: range = xrange  # let range always be an iterator\n",
    "except NameError: pass\n",
    "\n",
    "from cocoex import default_observers  # see cocoex.__init__.py\n",
    "from cocoex.utilities import ObserverOptions, ShortInfo, ascetime, print_flush\n",
    "from cocoex.solvers import random_search\n",
    "\n",
    "def default_observer_options(budget_=None, suite_name_=None, current_batch_=None):\n",
    "    \"\"\"return defaults computed from input parameters or current global vars\n",
    "    \"\"\"\n",
    "    global budget, suite_name, number_of_batches, current_batch\n",
    "    if budget_ is None:\n",
    "        budget_ = budget\n",
    "    if suite_name_ is None:\n",
    "        suite_name_ = suite_name\n",
    "    if current_batch_ is None and number_of_batches > 1:\n",
    "        current_batch_ = current_batch\n",
    "    opts = {}\n",
    "    try:\n",
    "        opts.update({'result_folder': '\"%s_on_%s%s_budget%04dxD\"'\n",
    "                    % (SOLVER.__name__,\n",
    "                       suite_name_,\n",
    "                       \"\" if current_batch_ is None\n",
    "                          else \"_batch%03dof%d\" % (current_batch_, number_of_batches),\n",
    "                       budget_)})\n",
    "    except: pass\n",
    "    try:\n",
    "        solver_module = '(%s)' % SOLVER.__module__\n",
    "    except:\n",
    "        solver_module = ''\n",
    "    try:\n",
    "        opts.update({'algorithm_name': SOLVER.__name__ + solver_module})\n",
    "    except: pass\n",
    "    return opts\n",
    "\n",
    "# ===============================================\n",
    "# loops over a benchmark problem suite\n",
    "# ===============================================\n",
    "def batch_loop(solver, suite, observer, budget,\n",
    "               max_runs, current_batch, number_of_batches):\n",
    "    \"\"\"loop over all problems in `suite` calling\n",
    "    `coco_optimize(solver, problem, budget * problem.dimension, max_runs)`\n",
    "    for each eligible problem.\n",
    "\n",
    "    A problem is eligible if ``problem_index + current_batch - 1``\n",
    "    modulo ``number_of_batches`` equals ``0``.\n",
    "\n",
    "    This distribution into batches is likely to lead to similar\n",
    "    runtimes for the batches, which is usually desirable.\n",
    "    \"\"\"\n",
    "    addressed_problems = []\n",
    "    short_info = ShortInfo()\n",
    "    for problem_index, problem in enumerate(suite):\n",
    "        if (problem_index + current_batch - 1) % number_of_batches:\n",
    "            continue\n",
    "        observer.observe(problem)\n",
    "        short_info.print(problem) if verbose else None\n",
    "        runs = coco_optimize(solver, problem, budget * problem.dimension,\n",
    "                             max_runs)\n",
    "        if verbose:\n",
    "            print_flush(\"!\" if runs > 2 else \":\" if runs > 1 else \".\")\n",
    "        short_info.add_evals(problem.evaluations + problem.evaluations_constraints, runs)\n",
    "        problem.free()  # not necessary as `enumerate` tears the problem down\n",
    "        addressed_problems += [problem.id]\n",
    "    print(short_info.function_done() + short_info.dimension_done())\n",
    "    short_info.print_timings()\n",
    "    print(\"  %s done (%d of %d problems benchmarked%s)\" %\n",
    "           (suite_name, len(addressed_problems), len(suite),\n",
    "             ((\" in batch %d of %d\" % (current_batch, number_of_batches))\n",
    "               if number_of_batches > 1 else \"\")), end=\"\")\n",
    "    if number_of_batches > 1:\n",
    "        print(\"\\n    MAKE SURE TO RUN ALL BATCHES\", end=\"\")\n",
    "    return addressed_problems\n",
    "\n",
    "#===============================================\n",
    "# interface: ADD AN OPTIMIZER BELOW\n",
    "#===============================================\n",
    "def coco_optimize(solver, fun, max_evals, max_runs=1e9):\n",
    "    \"\"\"`fun` is a callable, to be optimized by `solver`.\n",
    "\n",
    "    The `solver` is called repeatedly with different initial solutions\n",
    "    until either the `max_evals` are exhausted or `max_run` solver calls\n",
    "    have been made or the `solver` has not called `fun` even once\n",
    "    in the last run.\n",
    "\n",
    "    Return number of (almost) independent runs.\n",
    "    \"\"\"\n",
    "    range_ = fun.upper_bounds - fun.lower_bounds\n",
    "    center = fun.lower_bounds + range_ / 2\n",
    "    if fun.evaluations:\n",
    "        print('WARNING: %d evaluations were done before the first solver call' %\n",
    "              fun.evaluations)\n",
    "\n",
    "    for restarts in range(int(max_runs)):\n",
    "        remaining_evals = max_evals - fun.evaluations - fun.evaluations_constraints\n",
    "        x0 = center + (restarts > 0) * 0.8 * range_ * (\n",
    "                np.random.rand(fun.dimension) - 0.5)\n",
    "        fun(x0)  # can be incommented, if this is done by the solver\n",
    "\n",
    "        if solver.__name__ in (\"random_search\", ):\n",
    "            solver(fun, fun.lower_bounds, fun.upper_bounds,\n",
    "                   remaining_evals)\n",
    "        elif solver.__name__ == 'fmin' and solver.__globals__['__name__'] in ['cma', 'cma.evolution_strategy', 'cma.es']:\n",
    "            if x0[0] == center[0]:\n",
    "                sigma0 = 0.02\n",
    "                restarts_ = 0\n",
    "            else:\n",
    "                x0 = \"%f + %f * np.random.rand(%d)\" % (\n",
    "                        center[0], 0.8 * range_[0], fun.dimension)\n",
    "                sigma0 = 0.2\n",
    "                restarts_ = 6 * (observer_options.as_string.find('IPOP') >= 0)\n",
    "\n",
    "            solver(fun, x0, sigma0 * range_[0], restarts=restarts_,\n",
    "                   options=dict(scaling=range_/range_[0], maxfevals=remaining_evals,\n",
    "                                termination_callback=lambda es: fun.final_target_hit,\n",
    "                                verb_log=0, verb_disp=0, verbose=-9))\n",
    "        elif solver.__name__ == 'fmin_slsqp':\n",
    "            solver(fun, x0, iter=1 + remaining_evals / fun.dimension,\n",
    "                   iprint=-1)\n",
    "        elif solver.__name__ in (\"fmin_cobyla\", ):\n",
    "            x0 = fun.initial_solution\n",
    "            solver(fun, x0, lambda x: -fun.constraint(x), maxfun=remaining_evals,\n",
    "                   disp=0, rhoend=1e-9)\n",
    "############################ ADD HERE ########################################\n",
    "        # ### IMPLEMENT HERE THE CALL TO ANOTHER SOLVER/OPTIMIZER ###\n",
    "        elif solver.__name__ == 'Two_Mode_Section':\n",
    "        #     CALL MY SOLVER, interfaces vary\n",
    "\n",
    "            solver.Optimize(fun, x0,remaining_evals)\n",
    "\n",
    "##############################################################################\n",
    "        else:\n",
    "            solver(fun, x0)\n",
    "\n",
    "        if fun.evaluations + fun.evaluations_constraints >= max_evals or \\\n",
    "           fun.final_target_hit:\n",
    "            break\n",
    "        # quit if fun.evaluations did not increase\n",
    "        still_remaining = max_evals - fun.evaluations - fun.evaluations_constraints\n",
    "        if still_remaining >= remaining_evals:  # break loop if no evaluations were done\n",
    "            if still_remaining > remaining_evals:\n",
    "                raise RuntimeError(\"function evaluations decreased\")\n",
    "            if still_remaining >= fun.dimension + 2:\n",
    "                print(\"WARNING: %d evaluations of budget %d remaining\" %\n",
    "                      (still_remaining, max_evals))\n",
    "            break\n",
    "    return 1 + restarts  # number of (almost) independent launches of `solver`\n",
    "\n",
    "# ===============================================\n",
    "# set up: CHANGE HERE SOLVER AND FURTHER SETTINGS AS DESIRED\n",
    "# ===============================================\n",
    "######################### CHANGE HERE ########################################\n",
    "# CAVEAT: this might be modified from input args\n",
    "suite_name = \"bbob\"  # always overwritten when called from system shell\n",
    "                     # see available choices via cocoex.known_suite_names\n",
    "budget = 100  # maxfevals = budget x dimension ### INCREASE budget WHEN THE DATA CHAIN IS STABLE ###\n",
    "max_runs = 1e9  # number of (almost) independent trials per problem instance\n",
    "number_of_batches = 1  # allows to run everything in several batches\n",
    "current_batch = 1      # 1..number_of_batches\n",
    "##############################################################################\n",
    "# By default we call SOLVER(fun, x0), but the INTERFACE CAN BE ADAPTED TO EACH SOLVER ABOVE\n",
    "\n",
    "# SOLVER = random_search\n",
    "# SOLVER = Basic_Two_Mode_Object_first_draft\n",
    "\n",
    "# SOLVER = fmin_slsqp\n",
    "SOLVER = Two_Mode_Section\n",
    "\n",
    "# SOLVER = optimize.fmin_cobyla\n",
    "# SOLVER = my_solver # SOLVER = fmin_slsqp # SOLVER = cma.fmin\n",
    "suite_instance = \"\" # \"year:2016\"\n",
    "suite_options = \"\"  # \"dimensions: 2,3,5,10,20 \"  # if 40 is not desired\n",
    "# for more suite options, see http://numbbo.github.io/coco-doc/C/#suite-parameters\n",
    "observer_options = ObserverOptions({  # is (inherited from) a dictionary\n",
    "                    'algorithm_info': '\"A SIMPLE RANDOM SEARCH ALGORITHM\"', # CHANGE/INCOMMENT THIS!\n",
    "                    # 'algorithm_name': '',  # default already provided from SOLVER name\n",
    "                    # 'result_folder': '',  # default already provided from several global vars\n",
    "                   })\n",
    "######################### END CHANGE HERE ####################################\n",
    "\n",
    "# ===============================================\n",
    "# run (main)\n",
    "# ===============================================\n",
    "def main(budget=budget,\n",
    "         max_runs=max_runs,\n",
    "         current_batch=current_batch,\n",
    "         number_of_batches=number_of_batches):\n",
    "    \"\"\"Initialize suite and observer, then benchmark solver by calling\n",
    "    ``batch_loop(SOLVER, suite, observer, budget,...``\n",
    "    \"\"\"\n",
    "    suite = Suite(suite_name, suite_instance, suite_options)\n",
    "\n",
    "    observer_name = default_observers()[suite_name]\n",
    "    # observer_name = another observer if so desired\n",
    "    observer_options.update_gracefully(default_observer_options())\n",
    "    observer = Observer(observer_name, observer_options.as_string)\n",
    "\n",
    "    print(\"Benchmarking solver '%s' with budget=%d*dimension on %s suite, %s\"\n",
    "          % (' '.join(str(SOLVER).split()[:2]), budget,\n",
    "             suite.name, time.asctime()))\n",
    "    if number_of_batches > 1:\n",
    "        print('Batch usecase, make sure you run *all* %d batches.\\n' %\n",
    "              number_of_batches)\n",
    "    t0 = time.clock()\n",
    "    batch_loop(SOLVER, suite, observer, budget, max_runs,\n",
    "               current_batch, number_of_batches)\n",
    "    print(\", %s (%s total elapsed time).\" %\n",
    "            (time.asctime(), ascetime(time.clock() - t0)))\n",
    "    print('Data written to folder', observer.result_folder)\n",
    "    print('To post-process the data call \\n'\n",
    "          '    python -m cocopp %s \\n'\n",
    "          'from a system shell or \\n'\n",
    "          '    cocopp.main(\"%s\") \\n'\n",
    "          'from a python shell' % (2 * (observer.result_folder,)))\n",
    "\n",
    "# ===============================================\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"read input parameters and call `main()`\"\"\"\n",
    "#     print(sys.argv)\n",
    "    sys.argv[1] = 'bbob'\n",
    "    sys.argv = sys.argv[0:2]\n",
    "#     print(sys.argv)\n",
    "    if len(sys.argv) < 2 or sys.argv[1] in [\"--help\", \"-h\"]:\n",
    "            print(__doc__)\n",
    "            print(\"Recognized suite names: \" + str(cocoex.known_suite_names))\n",
    "            sys.exit(0)\n",
    "    suite_name = sys.argv[1]\n",
    "    suite_name = 'bbob'\n",
    "    if suite_name not in cocoex.known_suite_names:\n",
    "        print('WARNING: \"%s\" not in known names %s' %\n",
    "                (suite_name, str(cocoex.known_suite_names)))\n",
    "    if len(sys.argv) > 2:\n",
    "        budget = float(sys.argv[2])\n",
    "    if len(sys.argv) > 3:\n",
    "        current_batch = int(sys.argv[3])\n",
    "    if len(sys.argv) > 4:\n",
    "        number_of_batches = int(sys.argv[4])\n",
    "    if len(sys.argv) > 5:\n",
    "        messages = ['Argument \"%s\" disregarded (only 4 arguments are recognized).' % sys.argv[i]\n",
    "            for i in range(5, len(sys.argv))]\n",
    "        messages.append('See \"python example_experiment.py -h\" for help.')\n",
    "        raise ValueError('\\n'.join(messages))\n",
    "    main(budget, max_runs, current_batch, number_of_batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# a = float('NaN')\n",
    "# b = 3\n",
    "# if np.isnan(b):\n",
    "#     print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## With the two mode section I ran a 60 eval*D test - which ran into occasional errors in the f19, f6 - \n",
    "## All in all this test took 1:09 hours and 9 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
