{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "## Two Mode Section\n",
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "try: range = xrange\n",
    "except NameError: pass\n",
    "import os, sys\n",
    "import time\n",
    "import numpy as np  # \"pip install numpy\" installs numpy\n",
    "import cocoex\n",
    "from scipy import optimize # for tests with fmin_cobyla\n",
    "from cocoex import Suite, Observer, log_level\n",
    "# del absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "verbose = 1\n",
    "\n",
    "try: import cma  # cma.fmin is a solver option, \"pip install cma\" installs cma\n",
    "except: pass\n",
    "try: from scipy.optimize import fmin_slsqp  # \"pip install scipy\" installs scipy\n",
    "except: pass\n",
    "try: range = xrange  # let range always be an iterator\n",
    "except NameError: pass\n",
    "\n",
    "from cocoex import default_observers  # see cocoex.__init__.py\n",
    "from cocoex.utilities import ObserverOptions, ShortInfo, ascetime, print_flush\n",
    "from cocoex.solvers import random_search\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.seterr(invalid='ignore')\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "\n",
    "def Two_Mode_Optimizer(Seed_Parameters,Second_Best_Seed_Parameters,NumberOfSamples,Parameter_ChangeVector,Cost_Change,Random_Search_exponential_Growth_Factor,Targeted_Search_exponential_Growth_Factor,Targeted_Search_Decay_Rate,Random_Search_Growth_Rate,Targeted_Search_Growth_Rate,Random_Search_Period,Significant_Change_Value,Maximal_Random_Search,Adaptive_Amplitude):\n",
    "\n",
    "    Targeted_Search_Mechanism = 2\n",
    "\n",
    "    NumberOfSamples = np.maximum(NumberOfSamples,1)## This is to avoid allocation errors\n",
    "    Number_Of_Parameters_11 = len(Seed_Parameters)\n",
    "    TargetedSearch = np.zeros((Number_Of_Parameters_11,NumberOfSamples) )\n",
    "    Random_Search_exponential_Growth_Factor1 = Random_Search_exponential_Growth_Factor\n",
    "    \n",
    "\n",
    "    ################################# Testing if the improvement since last iteration was big enough #############\n",
    "    if np.sum(np.abs(Parameter_ChangeVector)) > Significant_Change_Value:\n",
    "        Random_Search_exponential_Growth_Factor = 0\n",
    "        Targeted_Search_exponential_Growth_Factor = Targeted_Search_exponential_Growth_Factor + 1\n",
    "\n",
    "    if np.sum(np.abs(Parameter_ChangeVector)) < Significant_Change_Value:\n",
    "        Targeted_Search_exponential_Growth_Factor = 0\n",
    "        \n",
    "        \n",
    "    if Print_Two_Mode_Analytics == 1:\n",
    "        print(\"Two_Mode_Analytics:\")\n",
    "        print(\"---------------------------------------------------------------------\")\n",
    "        print(\"The np.sum(np.abs(Parameter_ChangeVector)) is \",np.sum(np.abs(Parameter_ChangeVector)), )\n",
    "        print(\"As Compared To the Significant_Change_Value of \", Significant_Change_Value)\n",
    "    ###############################################################################################################        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    ################################# This the Random Search Section #############################################\n",
    "    ## Applying a Genetic Algorithm\n",
    "    # This is to broadcast them to a len(InitialParameters),NumberOfSamples size matrix, for element wise multiplication\n",
    "    Seed_Parameters1 = np.zeros((Number_Of_Parameters_11,NumberOfSamples))\n",
    "    Seed_Parameters1[:,:] = np.array([Seed_Parameters]).T\n",
    "    Second_Best_Seed_Parameters1 = np.zeros((Number_Of_Parameters_11,NumberOfSamples))    \n",
    "    Second_Best_Seed_Parameters1[:,:] = np.array([Second_Best_Seed_Parameters]).T    \n",
    "    \n",
    "    \n",
    "    Random_Search_Scalar_Amplitude = Maximal_Random_Search*np.exp(Random_Search_Growth_Rate*(np.sin(np.mod(Random_Search_exponential_Growth_Factor*np.pi/(2*Random_Search_Period),np.pi/2) + 0.01)-1))\n",
    "    Random_Search_Scalar_Amplitude = np.minimum(Random_Search_Scalar_Amplitude,10)\n",
    "    Random_Samples = 2*(np.random.rand(Number_Of_Parameters_11,NumberOfSamples)-0.5)\n",
    "    \n",
    "    ## Take half of the parameter each of the best two individuals from last iteration + add a smart \n",
    "    ## RMS addaptive amplitude search around the children of the next generation (Mutation) + at the end \n",
    "    ## subtract the initial seed parameters so that we are only left with change vectors\n",
    "    \n",
    "    Genes = np.random.rand(Number_Of_Parameters_11,NumberOfSamples)\n",
    "\n",
    "\n",
    "    RandomSearch = (Genes > 0.5)*Seed_Parameters1 +     (Genes <= 0.5)*Second_Best_Seed_Parameters1 +     Random_Samples*Random_Search_Scalar_Amplitude*Adaptive_Amplitude - Seed_Parameters1\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    if Print_Two_Mode_Analytics == 1:\n",
    "        print(\"\")\n",
    "        print(\"Random_Search_Scalar_Amplitude = \",Random_Search_Scalar_Amplitude)\n",
    "    ###############################################################################################################\n",
    "\n",
    "        \n",
    "    ################################# This the Targeted Search Section #############################################\n",
    "    if np.max(Parameter_ChangeVector) > 0:\n",
    "        RandomNumbers = np.random.rand(np.maximum(NumberOfSamples,1),1) ## To avoid allocation errors\n",
    "\n",
    "        Random_Search_Scalar_Amplitude_1 = Maximal_Random_Search*np.exp(Random_Search_Growth_Rate*(np.sin(np.mod(Random_Search_exponential_Growth_Factor1*np.pi/(2*Random_Search_Period),np.pi/2) + 0.01)-1))\n",
    "        TargetedSearch_Amplitude = 0\n",
    "\n",
    "\n",
    "        if Targeted_Search_exponential_Growth_Factor*Targeted_Search_Growth_Rate -Random_Search_exponential_Growth_Factor*Targeted_Search_Decay_Rate < 100:\n",
    "            TargetedSearch_Amplitude = Random_Search_Scalar_Amplitude_1*RandomNumbers*            np.exp(Targeted_Search_exponential_Growth_Factor*Targeted_Search_Growth_Rate                   -Random_Search_exponential_Growth_Factor*Targeted_Search_Decay_Rate)\n",
    "        else: TargetedSearch_Amplitude = 100*RandomNumbers\n",
    "\n",
    "        if Print_Two_Mode_Analytics == 1:\n",
    "            print(\"Mean Targeted_Search_Scalar_Amplitude =  = \",np.mean(TargetedSearch_Amplitude))\n",
    "\n",
    "\n",
    "\n",
    "        if np.max(np.abs(TargetedSearch_Amplitude)) > 1000: ## This is if the change vector is too big\n",
    "                TargetedSearch_Amplitude = 100*RandomNumbers\n",
    "\n",
    "        if np.isnan(np.max(TargetedSearch_Amplitude)) == 1: ## This is if the change vector is too big\n",
    "                TargetedSearch_Amplitude = 100*RandomNumbers\n",
    "\n",
    "\n",
    "\n",
    "        if Print_Two_Mode_Analytics == 1:\n",
    "                print(\"np.mean(TargetedSearch_Amplitude) after suppression is = \",np.mean(TargetedSearch_Amplitude))\n",
    "\n",
    "\n",
    "        for sample in range (1,NumberOfSamples):\n",
    "\n",
    "            TargetedSearch[:,sample] = TargetedSearch_Amplitude[sample]*Parameter_ChangeVector\n",
    "\n",
    "    ###############################################################################################################\n",
    "    \n",
    "    \n",
    "    ################################ This is the Final Readout and generating Suggestions ########################\n",
    "    if Print_Two_Mode_Analytics == 1:   \n",
    "        print(\"\")\n",
    "        print(\" The Adaptive_Amplitude Vector is :\",Adaptive_Amplitude[:,0])\n",
    "        print(\"---------------------------------------------------------------------\")\n",
    "        print(\"\")\n",
    "    \n",
    "\n",
    "    Suggested_Parameter_Samples_From_Two_Mode = np.array([Seed_Parameters]).T + TargetedSearch + RandomSearch\n",
    "    Suggested_Parameter_Samples_From_Two_Mode[:,0] = Seed_Parameters\n",
    "    \n",
    "    Random_Search_exponential_Growth_Factor = Random_Search_exponential_Growth_Factor +1\n",
    "    ##############################################################################################################\n",
    "\n",
    "    return Suggested_Parameter_Samples_From_Two_Mode, Random_Search_exponential_Growth_Factor, Targeted_Search_exponential_Growth_Factor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Two_Mode_Optimizer2(Seed_Parameters,Best_Quarter_Of_Individuals,NumberOfSamples,Parameter_ChangeVector,Cost_Change,Random_Search_exponential_Growth_Factor,Targeted_Search_exponential_Growth_Factor,Targeted_Search_Decay_Rate,Random_Search_Growth_Rate,Targeted_Search_Growth_Rate,Random_Search_Period,Significant_Change_Value,Maximal_Random_Search,Adaptive_Amplitude):\n",
    "\n",
    "\n",
    "    Targeted_Search_Mechanism = 2\n",
    "\n",
    "    NumberOfSamples = np.maximum(NumberOfSamples,1)## This is to avoid allocation errors\n",
    "    Number_Of_Parameters_11 = len(Seed_Parameters)\n",
    "    TargetedSearch = np.zeros((Number_Of_Parameters_11,NumberOfSamples))\n",
    "    Random_Search_exponential_Growth_Factor1 = Random_Search_exponential_Growth_Factor\n",
    "    \n",
    "\n",
    "    ################################# Testing if the improvement since last iteration was big enough #############\n",
    "    if np.sum(np.abs(Parameter_ChangeVector)) > Significant_Change_Value:\n",
    "        Random_Search_exponential_Growth_Factor = 0\n",
    "        Targeted_Search_exponential_Growth_Factor = Targeted_Search_exponential_Growth_Factor + 1\n",
    "\n",
    "    if np.sum(np.abs(Parameter_ChangeVector)) < Significant_Change_Value:\n",
    "        Targeted_Search_exponential_Growth_Factor = 0\n",
    "        \n",
    "        \n",
    "    if Print_Two_Mode_Analytics == 1:\n",
    "        print(\"Two_Mode_Analytics:\")\n",
    "        print(\"---------------------------------------------------------------------\")\n",
    "        print(\"The np.sum(np.abs(Parameter_ChangeVector)) is \",np.sum(np.abs(Parameter_ChangeVector)), )\n",
    "        print(\"As Compared To the Significant_Change_Value of \", Significant_Change_Value)\n",
    "    ###############################################################################################################        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    ################################# This the Random Search Section #############################################\n",
    "    ## Applying a Genetic Algorithm\n",
    "\n",
    "    ## Crossover\n",
    "    RandomSearch = np.zeros((Number_Of_Parameters_11,NumberOfSamples))\n",
    "    if Best_Quarter_Of_Individuals.shape[1] > 0:\n",
    "        Seed_Parameters1 = np.zeros((Number_Of_Parameters_11,NumberOfSamples))\n",
    "        Seed_Parameters1[:,:] = np.array([Seed_Parameters]).T\n",
    "        Crossover_Indexes = np.random.randint(0,Best_Quarter_Of_Individuals.shape[1],(Number_Of_Parameters_11,NumberOfSamples))\n",
    "        for i in range (0,Number_Of_Parameters_11):\n",
    "            RandomSearch[i,:] = Best_Quarter_Of_Individuals[i,Crossover_Indexes[i,:]]\n",
    "\n",
    "        ## Mutation\n",
    "        Random_Search_Scalar_Amplitude = Maximal_Random_Search*np.exp(Random_Search_Growth_Rate*(np.sin(np.mod(Random_Search_exponential_Growth_Factor*np.pi/(2*Random_Search_Period),np.pi/2) + 0.01)-1))    \n",
    "        Random_Search_Scalar_Amplitude = np.minimum(Random_Search_Scalar_Amplitude,10)\n",
    "        RandomSearch += 2*(np.random.rand(Number_Of_Parameters_11,NumberOfSamples)-0.5)*Random_Search_Scalar_Amplitude*Adaptive_Amplitude - Seed_Parameters1\n",
    "\n",
    "    \n",
    "    \n",
    "    if Print_Two_Mode_Analytics == 1:\n",
    "        print(\"\")\n",
    "        print(\"Random_Search_Scalar_Amplitude = \",Random_Search_Scalar_Amplitude)\n",
    "    ###############################################################################################################\n",
    "\n",
    "        \n",
    "    ################################# This the Targeted Search Section #############################################\n",
    "    if np.max(Parameter_ChangeVector) > 0:\n",
    "        RandomNumbers = np.random.rand(np.maximum(NumberOfSamples,1),1) ## To avoid allocation errors\n",
    "\n",
    "        Random_Search_Scalar_Amplitude_1 = Maximal_Random_Search*np.exp(Random_Search_Growth_Rate*(np.sin(np.mod(Random_Search_exponential_Growth_Factor1*np.pi/(2*Random_Search_Period),np.pi/2) + 0.01)-1))\n",
    "        TargetedSearch_Amplitude = 0\n",
    "\n",
    "\n",
    "        if Targeted_Search_exponential_Growth_Factor*Targeted_Search_Growth_Rate -Random_Search_exponential_Growth_Factor*Targeted_Search_Decay_Rate < 100:\n",
    "            TargetedSearch_Amplitude = Random_Search_Scalar_Amplitude_1*RandomNumbers*            np.exp(Targeted_Search_exponential_Growth_Factor*Targeted_Search_Growth_Rate                   -Random_Search_exponential_Growth_Factor*Targeted_Search_Decay_Rate)\n",
    "        else: TargetedSearch_Amplitude = 100*RandomNumbers\n",
    "\n",
    "\n",
    "        if Print_Two_Mode_Analytics == 1:\n",
    "            print(\"Mean Targeted_Search_Scalar_Amplitude =  = \",np.mean(TargetedSearch_Amplitude))\n",
    "\n",
    "\n",
    "\n",
    "        if np.max(np.abs(TargetedSearch_Amplitude)) > 1000: ## This is if the change vector is too big\n",
    "                TargetedSearch_Amplitude = 100*RandomNumbers\n",
    "\n",
    "        if np.isnan(np.max(TargetedSearch_Amplitude)) == 1: ## This is if the change vector is too big\n",
    "                TargetedSearch_Amplitude = 100*RandomNumbers\n",
    "\n",
    "\n",
    "\n",
    "        if Print_Two_Mode_Analytics == 1:\n",
    "                print(\"np.mean(TargetedSearch_Amplitude) after suppression is = \",np.mean(TargetedSearch_Amplitude))\n",
    "\n",
    "\n",
    "        for sample in range (1,NumberOfSamples):\n",
    "\n",
    "            TargetedSearch[:,sample] = TargetedSearch_Amplitude[sample]*Parameter_ChangeVector\n",
    "\n",
    "    ###############################################################################################################\n",
    "    \n",
    "    \n",
    "    ################################ This is the Final Readout and generating Suggestions ########################\n",
    "    if Print_Two_Mode_Analytics == 1:   \n",
    "        print(\"\")\n",
    "        print(\" The Adaptive_Amplitude Vector is :\",Adaptive_Amplitude[:,0])\n",
    "        print(\"---------------------------------------------------------------------\")\n",
    "        print(\"\")\n",
    "    \n",
    "\n",
    "    Suggested_Parameter_Samples_From_Two_Mode = np.array([Seed_Parameters]).T + TargetedSearch + RandomSearch\n",
    "    Suggested_Parameter_Samples_From_Two_Mode[:,0] = Seed_Parameters\n",
    "    \n",
    "    Random_Search_exponential_Growth_Factor = Random_Search_exponential_Growth_Factor +1\n",
    "    ##############################################################################################################\n",
    "\n",
    "    return Suggested_Parameter_Samples_From_Two_Mode, Random_Search_exponential_Growth_Factor, Targeted_Search_exponential_Growth_Factor\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "## Conductor Algorithm\n",
    "\n",
    "def Symmetric_Section_Conductor(Current_Resource_Allocation,Changevector,Hyper_Parameters):\n",
    "    ## The difference is that this function can allow for different masses and interactions between the algorithms\n",
    "    \n",
    "    \n",
    "    Initial_Resource_Allocation = Hyper_Parameters[:,0]\n",
    "    MassVector = Hyper_Parameters[:,1]\n",
    "    Self_Interaction_Spring_Constants = Hyper_Parameters[:,2]\n",
    "    InteractionMatrix = Hyper_Parameters[:,3:3+Number_Of_Algorithms]\n",
    "\n",
    "    \n",
    "    \n",
    "    Norm = np.sqrt(np.matmul(Changevector,Changevector))\n",
    "    if Norm != 0:\n",
    "        Changevector = Changevector/Norm\n",
    "    \n",
    "    R0 = Initial_Resource_Allocation\n",
    "    R = Current_Resource_Allocation\n",
    "    K0 = Self_Interaction_Spring_Constants\n",
    "\n",
    "    ## This updates the allocated resources\n",
    "    H_Self = -(K0/MassVector)*(R-R0) ## This slowly restores the Resouce allocation to the original values\n",
    "    H_Interaction = np.matmul(InteractionMatrix,(Changevector/MassVector).T) ## this is a resouce conserving interaction\n",
    "    R = R + H_Self + H_Interaction.T\n",
    "        \n",
    "\n",
    "    RecomendedResourceAllocation1 = np.round(R)\n",
    "\n",
    "    ## This makes sure that if the total resources aren't conserved then add or subtract from the best Member.\n",
    "    I = np.argmax(RecomendedResourceAllocation1)\n",
    "    RecomendedResourceAllocation1[I] = RecomendedResourceAllocation1[I] + (np.sum(Initial_Resource_Allocation) - np.sum(RecomendedResourceAllocation1))\n",
    "\n",
    "    ## This convertes the array to an int array\n",
    "    RecomendedResourceAllocation = np.arange(len(RecomendedResourceAllocation1))\n",
    "    for i in range(len(RecomendedResourceAllocation1)):\n",
    "        if np.isnan(RecomendedResourceAllocation1[i]):\n",
    "            Changevector = np.zeros(4)\n",
    "            RecomendedResourceAllocation1 = 16*np.ones(4)\n",
    "        RecomendedResourceAllocation[i] = int(RecomendedResourceAllocation1[i])\n",
    "    \n",
    "    return RecomendedResourceAllocation\n",
    "\n",
    "\n",
    "\n",
    "def Set_Hyperparameters_For_Symmetric_Conductor(Total_Resources,Number_Of_Algorithms,Mass,Self_Spring_Constants,Interaction_Spring_Constants):\n",
    "    \n",
    "\n",
    "    Initial_Resource_Allocation =int(Total_Resources/Number_Of_Algorithms)*np.ones(Number_Of_Algorithms)\n",
    "\n",
    "    ## This fixes Rounding issues\n",
    "    Initial_Resource_Allocation[0] = Initial_Resource_Allocation[0] + (Total_Resources-np.sum(Initial_Resource_Allocation))\n",
    "    MassVector = Mass*np.ones(Number_Of_Algorithms)\n",
    "    Self_Interaction_Spring_Constants = Self_Spring_Constants*np.ones(Number_Of_Algorithms)\n",
    "    InteractionMatrix = Interaction_Spring_Constants*(-np.ones((Number_Of_Algorithms,Number_Of_Algorithms)) + Number_Of_Algorithms*np.diag(np.ones(Number_Of_Algorithms)))\n",
    "\n",
    "    \n",
    "    \n",
    "    Hyperparameters_Array = np.zeros((Number_Of_Algorithms,3+Number_Of_Algorithms))\n",
    "    Hyperparameters_Array[:,0] = Initial_Resource_Allocation\n",
    "    Hyperparameters_Array[:,1] = MassVector\n",
    "    Hyperparameters_Array[:,2] = Self_Interaction_Spring_Constants\n",
    "    Hyperparameters_Array[:,3:3+Number_Of_Algorithms] = InteractionMatrix\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    return Hyperparameters_Array\n",
    "\n",
    "\n",
    "\n",
    "### Test of the conductor algorithm\n",
    "\n",
    "Total_Resources = 60\n",
    "Number_Of_Algorithms = 4\n",
    "Mass =10\n",
    "Self_Spring_Constants = 10\n",
    "Interaction_Spring_Constants = 40\n",
    "    \n",
    "    \n",
    "Hyperparameters_For_Symmetric_Conductor = Set_Hyperparameters_For_Symmetric_Conductor(Total_Resources,Number_Of_Algorithms,Mass,Self_Spring_Constants,Interaction_Spring_Constants)\n",
    "Initial_Resource_Allocation = Hyperparameters_For_Symmetric_Conductor[:,0]\n",
    "Current_Resource_Allocation = Hyperparameters_For_Symmetric_Conductor[:,0]\n",
    "\n",
    "test_resource_allocation = 0\n",
    "if test_resource_allocation == 1:\n",
    "    print(\"Initial Total_Resources\",np.sum(Initial_Resource_Allocation))\n",
    "    for i in range (1,50):\n",
    "        Changevector = np.array([-1, 0 ,0,0]) ## Negative is bad\n",
    "        Recomended_Resource_Allocation = Symmetric_Section_Conductor(Current_Resource_Allocation,Changevector,Hyperparameters_For_Symmetric_Conductor) \n",
    "        Current_Resource_Allocation = Recomended_Resource_Allocation\n",
    "\n",
    "\n",
    "    Minimal_Allocation = np.min(Current_Resource_Allocation)\n",
    "    print(\"\")\n",
    "    print(\"Minimum Resouces test\")\n",
    "    print(\"------------------------------------------------------------------\")\n",
    "    print(\"Minimal_Allocation is:\",Minimal_Allocation)\n",
    "    print(\"(Maximal_Allocation is:\",np.max(Current_Resource_Allocation),\")\")\n",
    "    print(\"Total_Resources After minumum resource allocation Test\",np.sum(Current_Resource_Allocation))\n",
    "\n",
    "    for i in range (1,50):\n",
    "        Changevector = np.array([1,0 ,0,0]) ## Positive is good\n",
    "        Recomended_Resource_Allocation = Symmetric_Section_Conductor(Current_Resource_Allocation,Changevector,Hyperparameters_For_Symmetric_Conductor)\n",
    "        Current_Resource_Allocation = Recomended_Resource_Allocation\n",
    "\n",
    "    Maximal_Allocation = np.max(Current_Resource_Allocation)\n",
    "    print(\"\")\n",
    "    print(\"Maximum Resouces test\")\n",
    "    print(\"------------------------------------------------------------------\")\n",
    "    print(\"Maximal_Allocation is:\",Maximal_Allocation)\n",
    "    print(\"(Minimal_Allocation is:\",np.min(Current_Resource_Allocation),\")\")\n",
    "    print(\"Total_Resources After maximum resource allocation Test\",np.sum(Current_Resource_Allocation))\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "class Two_Mode_Suggestion_Tools(object):\n",
    "\n",
    "    # The class \"constructor\" - It's actually an initializer \n",
    "    def __init__(self, Best_Individual, Second_Best_Individual, Parameter_ChangeVector, Number_Of_Samples,Cost_Change,Random_Search_exponential_Growth_Factor,Targeted_Search_exponential_Growth_Factor,Adaptive_Amplitude_Vector,Squared_Gradient):\n",
    "        self.Best_Individual = Best_Individual\n",
    "        self.Second_Best_Individual = Second_Best_Individual\n",
    "        self.Parameter_ChangeVector = Parameter_ChangeVector\n",
    "        self.Number_Of_Samples = Number_Of_Samples\n",
    "        self.Cost_Change = Cost_Change\n",
    "        self.Random_Search_exponential_Growth_Factor = Random_Search_exponential_Growth_Factor\n",
    "        self.Targeted_Search_exponential_Growth_Factor = Targeted_Search_exponential_Growth_Factor\n",
    "        self.Adaptive_Amplitude_Vector = Adaptive_Amplitude_Vector\n",
    "        self.Squared_Gradient = Squared_Gradient\n",
    "\n",
    "        \n",
    "def Create_New_Two_Mode_Suggestion_Tools_Object(Number_Of_Parameters): ## this is where the directional pool is initialized\n",
    "    N = Number_Of_Parameters\n",
    "    Best_Individual = np.random.rand(N) - 0.5\n",
    "    Second_Best_Individual = np.random.rand(N) - 0.5\n",
    "    Parameter_ChangeVector = np.random.rand(N) - 0.5\n",
    "    Number_Of_Samples = 10\n",
    "    Cost_Change = -1\n",
    "    Random_Search_exponential_Growth_Factor = 1.0\n",
    "    Targeted_Search_exponential_Growth_Factor = 1.0\n",
    "    Adaptive_Amplitude_Vector = np.random.rand(N) - 0.5\n",
    "    Squared_Gradient = np.random.rand(N)\n",
    "    ## Generating and instance For The object the Two mode Suggestions tools object\n",
    "    return  Two_Mode_Suggestion_Tools(Best_Individual, Second_Best_Individual, Parameter_ChangeVector, Number_Of_Samples,Cost_Change,Random_Search_exponential_Growth_Factor,Targeted_Search_exponential_Growth_Factor,Adaptive_Amplitude_Vector,Squared_Gradient)\n",
    "\n",
    "\n",
    "Test_Object_Functionality = 0\n",
    "if Test_Object_Functionality == 1:\n",
    "    ## Setting the inputs for the Two mode Suggestions tools object\n",
    "    N = 30\n",
    "    Best_Individual = np.random.rand(N) - 0.5\n",
    "    Second_Best_Individual = np.random.rand(N) - 0.5\n",
    "    Parameter_ChangeVector = np.random.rand(N) - 0.5\n",
    "    Number_Of_Samples = 10\n",
    "    Cost_Change = -1\n",
    "    Random_Search_exponential_Growth_Factor = 2.0\n",
    "    Targeted_Search_exponential_Growth_Factor = 8.0\n",
    "    Adaptive_Amplitude_Vector = np.random.rand(N) - 0.5\n",
    "    Squared_Gradient = np.zeros(N)\n",
    "\n",
    "    ## Generating and instance For The object the Two mode Suggestions tools object\n",
    "    Suggestion_Tools =  Two_Mode_Suggestion_Tools(Best_Individual, Second_Best_Individual, Parameter_ChangeVector, Number_Of_Samples,Cost_Change,Random_Search_exponential_Growth_Factor,Targeted_Search_exponential_Growth_Factor,Adaptive_Amplitude_Vector,Squared_Gradient)\n",
    "\n",
    "    Best_Individual2 = Suggestion_Tools.Best_Individual\n",
    "    Second_Best_Individual2 = Suggestion_Tools.Second_Best_Individual\n",
    "    Parameter_ChangeVector2 = Suggestion_Tools.Parameter_ChangeVector\n",
    "    Number_Of_Samples2 = Suggestion_Tools.Number_Of_Samples\n",
    "    Cost_Change2 = Suggestion_Tools.Cost_Change\n",
    "    Random_Search_exponential_Growth_Factor2 = Suggestion_Tools.Random_Search_exponential_Growth_Factor\n",
    "    Targeted_Search_exponential_Growth_Factor2 = Suggestion_Tools.Targeted_Search_exponential_Growth_Factor\n",
    "    Adaptive_Amplitude_Vector2 = Suggestion_Tools.Adaptive_Amplitude_Vector\n",
    "    Adaptive_Amplitude2 = np.ones((len(Best_Individual),Number_Of_Samples))\n",
    "    Adaptive_Amplitude2[:,:] = np.array([Adaptive_Amplitude_Vector2]).T\n",
    "    Squared_Gradient = np.zeros(len(Best_Individual))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Test If the Items were loaded Properly into the object:\")\n",
    "    print(\"------------------------------------------------------------\")\n",
    "    print(\"\")\n",
    "    print(Best_Individual == Best_Individual2)\n",
    "    print(\"\")\n",
    "    print(Second_Best_Individual == Second_Best_Individual2)\n",
    "    print(\"\")\n",
    "    print(Parameter_ChangeVector == Parameter_ChangeVector2)\n",
    "    print(\"\")\n",
    "    print(Number_Of_Samples == Number_Of_Samples2)\n",
    "    print(\"\")\n",
    "    print(Cost_Change == Cost_Change2)\n",
    "    print(\"\")\n",
    "    print(Random_Search_exponential_Growth_Factor == Random_Search_exponential_Growth_Factor2)\n",
    "    print(\"\")\n",
    "    print(Targeted_Search_exponential_Growth_Factor == Targeted_Search_exponential_Growth_Factor2)\n",
    "    print(\"\")\n",
    "    print(Adaptive_Amplitude_Vector == Adaptive_Amplitude_Vector2)\n",
    "    print(\"------------------------------------------------------------\")\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(\"Test if we can update the values in the object propely\")\n",
    "    print(\"------------------------------------------------------------\")\n",
    "    print(\"Suggestion_Tools.Number_Of_Samples = \",Suggestion_Tools.Number_Of_Samples)\n",
    "    print(\"Suggestion_Tools.Number_Of_Samples = \",2)\n",
    "    Suggestion_Tools.Number_Of_Samples = 2\n",
    "    print(\"Suggestion_Tools.Number_Of_Samples = \",Suggestion_Tools.Number_Of_Samples)\n",
    "\n",
    "\n",
    "    print(\"Suggestion_Tools.Best_Individual = \",Suggestion_Tools.Best_Individual)\n",
    "    print(\"Suggestion_Tools.Best_Individual = 10* Suggestion_Tools.Best_Individual\")\n",
    "    Suggestion_Tools.Best_Individual = 10*Suggestion_Tools.Best_Individual\n",
    "    print(\"Suggestion_Tools.Best_Individual = \",Suggestion_Tools.Best_Individual)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "\n",
    "def Get_TWO_Mode_Suggestions(fun,Hyperparameters,Suggestion_Tools,Print_Two_Mode_Analytics):\n",
    "   \n",
    "\n",
    "   \n",
    "\n",
    "   Random_Search_Growth_Rate  = Hyperparameters[0]\n",
    "   Random_Search_Period = Hyperparameters[1]\n",
    "   Maximal_Random_Search = Hyperparameters[2]\n",
    "   Targeted_Search_Growth_Rate = Hyperparameters[3]\n",
    "   Targeted_Search_Decay_Rate = Hyperparameters[4]\n",
    "   Significant_Change_Value = Hyperparameters[5]\n",
    "   alpha = Hyperparameters[6]\n",
    "   beta = Hyperparameters[7]\n",
    "\n",
    "   \n",
    "   ######################################## There can also be speed up here by just inputing it directly into the function\n",
    "   Best_Individual = Suggestion_Tools.Best_Individual\n",
    "   Second_Best_Individual = Suggestion_Tools.Second_Best_Individual\n",
    "   Parameter_ChangeVector = Suggestion_Tools.Parameter_ChangeVector\n",
    "   Number_Of_Samples = Suggestion_Tools.Number_Of_Samples\n",
    "   Cost_Change = Suggestion_Tools.Cost_Change\n",
    "   Random_Search_exponential_Growth_Factor = Suggestion_Tools.Random_Search_exponential_Growth_Factor\n",
    "   Targeted_Search_exponential_Growth_Factor = Suggestion_Tools.Targeted_Search_exponential_Growth_Factor\n",
    "   Adaptive_Amplitude1 = Suggestion_Tools.Adaptive_Amplitude_Vector\n",
    "   Adaptive_Amplitude = np.ones((len(Best_Individual),Number_Of_Samples))\n",
    "   Adaptive_Amplitude[:,:] = np.array([Adaptive_Amplitude1]).T\n",
    "\n",
    "   \n",
    "   Suggested_Parameter_Samples_From_Two_Mode, Random_Search_exponential_Growth_Factor, Targeted_Search_exponential_Growth_Factor =     Two_Mode_Optimizer(Best_Individual,Second_Best_Individual,Number_Of_Samples,Parameter_ChangeVector,Cost_Change, Random_Search_exponential_Growth_Factor,Targeted_Search_exponential_Growth_Factor,Targeted_Search_Decay_Rate,Random_Search_Growth_Rate,Targeted_Search_Growth_Rate,Random_Search_Period,Significant_Change_Value,Maximal_Random_Search,Adaptive_Amplitude)\n",
    "\n",
    "   Suggestion_Tools.Random_Search_exponential_Growth_Factor = Random_Search_exponential_Growth_Factor\n",
    "   Suggestion_Tools.Targeted_Search_exponential_Growth_Factor = Targeted_Search_exponential_Growth_Factor\n",
    "   \n",
    "   return Suggested_Parameter_Samples_From_Two_Mode,Suggestion_Tools\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Get_TWO_Mode_Suggestions2(fun,Hyperparameters,Suggestion_Tools,Print_Two_Mode_Analytics,Best_Quarter_Of_Individuals):\n",
    "   \n",
    "\n",
    "   \n",
    "\n",
    "   Random_Search_Growth_Rate  = Hyperparameters[0]\n",
    "   Random_Search_Period = Hyperparameters[1]\n",
    "   Maximal_Random_Search = Hyperparameters[2]\n",
    "   Targeted_Search_Growth_Rate = Hyperparameters[3]\n",
    "   Targeted_Search_Decay_Rate = Hyperparameters[4]\n",
    "   Significant_Change_Value = Hyperparameters[5]\n",
    "   alpha = Hyperparameters[6]\n",
    "   beta = Hyperparameters[7]\n",
    "\n",
    "   \n",
    "   ######################################## There can also be speed up here by just inputing it directly into the function\n",
    "   Best_Individual = Suggestion_Tools.Best_Individual\n",
    "   Second_Best_Individual = Suggestion_Tools.Second_Best_Individual\n",
    "   Parameter_ChangeVector = Suggestion_Tools.Parameter_ChangeVector\n",
    "   Number_Of_Samples = Suggestion_Tools.Number_Of_Samples\n",
    "   Cost_Change = Suggestion_Tools.Cost_Change\n",
    "   Random_Search_exponential_Growth_Factor = Suggestion_Tools.Random_Search_exponential_Growth_Factor\n",
    "   Targeted_Search_exponential_Growth_Factor = Suggestion_Tools.Targeted_Search_exponential_Growth_Factor\n",
    "   Adaptive_Amplitude1 = Suggestion_Tools.Adaptive_Amplitude_Vector\n",
    "   Adaptive_Amplitude = np.ones((len(Best_Individual),Number_Of_Samples))\n",
    "   Adaptive_Amplitude[:,:] = np.array([Adaptive_Amplitude1]).T\n",
    "\n",
    "   \n",
    "   Suggested_Parameter_Samples_From_Two_Mode, Random_Search_exponential_Growth_Factor, Targeted_Search_exponential_Growth_Factor =     Two_Mode_Optimizer2(Best_Individual,Best_Quarter_Of_Individuals,Number_Of_Samples,Parameter_ChangeVector,Cost_Change, Random_Search_exponential_Growth_Factor,Targeted_Search_exponential_Growth_Factor,Targeted_Search_Decay_Rate,Random_Search_Growth_Rate,Targeted_Search_Growth_Rate,Random_Search_Period,Significant_Change_Value,Maximal_Random_Search,Adaptive_Amplitude)\n",
    "\n",
    "   Suggestion_Tools.Random_Search_exponential_Growth_Factor = Random_Search_exponential_Growth_Factor\n",
    "   Suggestion_Tools.Targeted_Search_exponential_Growth_Factor = Targeted_Search_exponential_Growth_Factor\n",
    "   \n",
    "   return Suggested_Parameter_Samples_From_Two_Mode,Suggestion_Tools\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "\n",
    "def fun(x):\n",
    "    return np.sum((x-2)**2)\n",
    "\n",
    "\n",
    "def Get_Suggestions_From_Two_Mode_Section(fun,Hyperparameters_1,Hyperparameters_2,Hyperparameters_3,Hyperparameters_4,Suggestion_Tools_1,Suggestion_Tools_2,Suggestion_Tools_3,Suggestion_Tools_4,Hyperparameters_For_Symmetric_Conductor):\n",
    "\n",
    "    ## This gets The Suggestions from The Different Sets Of hyperparameters Set 1\n",
    "\n",
    "    Suggested_Parameter_Samples_From_Two_Mode_1,Suggestion_Tools_1  = Get_TWO_Mode_Suggestions(fun,Hyperparameters_1,Suggestion_Tools_1 ,Print_Two_Mode_Analytics)   \n",
    "    Suggested_Parameter_Samples_From_Two_Mode_2,Suggestion_Tools_2  = Get_TWO_Mode_Suggestions(fun,Hyperparameters_2,Suggestion_Tools_2 ,Print_Two_Mode_Analytics)   \n",
    "    Suggested_Parameter_Samples_From_Two_Mode_3,Suggestion_Tools_3  = Get_TWO_Mode_Suggestions(fun,Hyperparameters_3,Suggestion_Tools_3 ,Print_Two_Mode_Analytics)   \n",
    "    Suggested_Parameter_Samples_From_Two_Mode_4,Suggestion_Tools_4  = Get_TWO_Mode_Suggestions(fun,Hyperparameters_4,Suggestion_Tools_4 ,Print_Two_Mode_Analytics) \n",
    "\n",
    "\n",
    "    ## This is to orginize the different indexes of the suggestions for later processing\n",
    "    N1 = len(Suggested_Parameter_Samples_From_Two_Mode_1[0,:])\n",
    "    N2 = len(Suggested_Parameter_Samples_From_Two_Mode_2[0,:])\n",
    "    N3 = len(Suggested_Parameter_Samples_From_Two_Mode_3[0,:])\n",
    "    N4 = len(Suggested_Parameter_Samples_From_Two_Mode_4[0,:])\n",
    "\n",
    "    ## Save the indexes in the Suggestions array to know which samples came from which indvidual\n",
    "    Indexes_Of_The_Suggestions_From_The_Different_Algorithms = np.array([0,N1,N1+N2,N1+N2+N3,N1+N2+N3+N4])\n",
    "    ## Combine all the suggested samples into one array\n",
    "    All_Suggestions_From_Two_Mode_Section = np.concatenate((Suggested_Parameter_Samples_From_Two_Mode_1,Suggested_Parameter_Samples_From_Two_Mode_2,Suggested_Parameter_Samples_From_Two_Mode_3,Suggested_Parameter_Samples_From_Two_Mode_4),1)\n",
    "\n",
    "\n",
    "    return All_Suggestions_From_Two_Mode_Section,Indexes_Of_The_Suggestions_From_The_Different_Algorithms\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Get_Suggestions_From_Two_Mode_Section2(fun,Hyperparameters_1,Hyperparameters_2,Hyperparameters_3,Hyperparameters_4,Suggestion_Tools_1,Suggestion_Tools_2,Suggestion_Tools_3,Suggestion_Tools_4,Hyperparameters_For_Symmetric_Conductor,Best_Quarter_Of_Individuals):\n",
    "\n",
    "    ## This gets The Suggestions from The Different Sets Of hyperparameters Set 1\n",
    "\n",
    "    Suggested_Parameter_Samples_From_Two_Mode_1,Suggestion_Tools_1  = Get_TWO_Mode_Suggestions2(fun,Hyperparameters_1,Suggestion_Tools_1 ,Print_Two_Mode_Analytics,Best_Quarter_Of_Individuals)   \n",
    "    Suggested_Parameter_Samples_From_Two_Mode_2,Suggestion_Tools_2  = Get_TWO_Mode_Suggestions2(fun,Hyperparameters_2,Suggestion_Tools_2 ,Print_Two_Mode_Analytics,Best_Quarter_Of_Individuals)   \n",
    "    Suggested_Parameter_Samples_From_Two_Mode_3,Suggestion_Tools_3  = Get_TWO_Mode_Suggestions2(fun,Hyperparameters_3,Suggestion_Tools_3 ,Print_Two_Mode_Analytics,Best_Quarter_Of_Individuals)   \n",
    "    Suggested_Parameter_Samples_From_Two_Mode_4,Suggestion_Tools_4  = Get_TWO_Mode_Suggestions2(fun,Hyperparameters_4,Suggestion_Tools_4 ,Print_Two_Mode_Analytics,Best_Quarter_Of_Individuals) \n",
    "\n",
    "\n",
    "    ## This is to orginize the different indexes of the suggestions for later processing\n",
    "    N1 = len(Suggested_Parameter_Samples_From_Two_Mode_1[0,:])\n",
    "    N2 = len(Suggested_Parameter_Samples_From_Two_Mode_2[0,:])\n",
    "    N3 = len(Suggested_Parameter_Samples_From_Two_Mode_3[0,:])\n",
    "    N4 = len(Suggested_Parameter_Samples_From_Two_Mode_4[0,:])\n",
    "\n",
    "    ## Save the indexes in the Suggestions array to know which samples came from which indvidual\n",
    "    Indexes_Of_The_Suggestions_From_The_Different_Algorithms = np.array([0,N1,N1+N2,N1+N2+N3,N1+N2+N3+N4])\n",
    "    ## Combine all the suggested samples into one array\n",
    "    All_Suggestions_From_Two_Mode_Section = np.concatenate((Suggested_Parameter_Samples_From_Two_Mode_1,Suggested_Parameter_Samples_From_Two_Mode_2,Suggested_Parameter_Samples_From_Two_Mode_3,Suggested_Parameter_Samples_From_Two_Mode_4),1)\n",
    "\n",
    "\n",
    "    return All_Suggestions_From_Two_Mode_Section,Indexes_Of_The_Suggestions_From_The_Different_Algorithms\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "\n",
    "     \n",
    "def Update_Addaptive_Amplitudes(Gradients,Hyperparameters_1,Hyperparameters_2,Hyperparameters_3,Hyperparameters_4,Suggestion_Tools_1,Suggestion_Tools_2,Suggestion_Tools_3,Suggestion_Tools_4):\n",
    "         \n",
    "\n",
    "## This is to do these two lines for all individuals\n",
    "#   alpha = Hyperparameters_1[6]\n",
    "#   beta = Hyperparameters_1[7]\n",
    "#   S = beta*S + (1-beta)*(np.mean(np.abs(Gradients),1))**2\n",
    "#   Adaptive_Amplitude1 = alpha/np.sqrt(S + (alpha**2))\n",
    "#   a = Hyperparameters_1[7]*Suggestion_Tools_1.Squared_Gradient + (1-Hyperparameters_1[7])*(np.mean(np.abs(Gradients,1))**2)\n",
    " if np.max(np.abs(Gradients)) < 10**20:\n",
    "     RMS = np.mean(Gradients - (np.array([np.mean(Gradients ,1)]).T)**2,1) ## actually its just the square mean\n",
    " else: RMS = np.zeros(Gradients.shape[0])\n",
    " ## Update The First Individaul\n",
    " Suggestion_Tools_1.Squared_Gradient = Hyperparameters_1[7]*Suggestion_Tools_1.Squared_Gradient + (1-Hyperparameters_1[7])*RMS\n",
    " Suggestion_Tools_1.Squared_Gradient = Suggestion_Tools_1.Squared_Gradient*np.sign(np.mean(Gradients,1))\n",
    " Suggestion_Tools_1.Adaptive_Amplitude = Hyperparameters_1[6]/np.sqrt(np.maximum(Suggestion_Tools_1.Squared_Gradient + (Hyperparameters_1[6]**2),0.1*(Hyperparameters_1[6]**2)))\n",
    "\n",
    "\n",
    " ## Update The Second Individaul\n",
    " Suggestion_Tools_2.Squared_Gradient = Hyperparameters_2[7]*Suggestion_Tools_2.Squared_Gradient + (1-Hyperparameters_2[7])*RMS\n",
    " Suggestion_Tools_2.Squared_Gradient = Suggestion_Tools_2.Squared_Gradient*np.sign(np.mean(Gradients,1))\n",
    " Suggestion_Tools_2.Adaptive_Amplitude = Hyperparameters_2[6]/np.sqrt(np.maximum(Suggestion_Tools_2.Squared_Gradient + (Hyperparameters_2[6]**2),0.1*(Hyperparameters_2[6]**2)))\n",
    "\n",
    " ## Update The Third  Individaul\n",
    " Suggestion_Tools_3.Squared_Gradient = Hyperparameters_3[7]*Suggestion_Tools_3.Squared_Gradient + (1-Hyperparameters_3[7])*RMS\n",
    " Suggestion_Tools_3.Squared_Gradient = Suggestion_Tools_3.Squared_Gradient*np.sign(np.mean(Gradients,1))\n",
    " Suggestion_Tools_3.Adaptive_Amplitude = Hyperparameters_3[6]/np.sqrt(np.maximum(Suggestion_Tools_3.Squared_Gradient + (Hyperparameters_3[6]**2),0.1*(Hyperparameters_3[6]**2)))\n",
    "\n",
    " ## Update The Third  Individaul\n",
    " Suggestion_Tools_4.Squared_Gradient = Hyperparameters_4[7]*Suggestion_Tools_4.Squared_Gradient + (1-Hyperparameters_4[7])*RMS\n",
    " Suggestion_Tools_4.Squared_Gradient = Suggestion_Tools_4.Squared_Gradient*np.sign(np.mean(Gradients,1))\n",
    " Suggestion_Tools_4.Adaptive_Amplitude = Hyperparameters_4[6]/np.sqrt(np.maximum(Suggestion_Tools_4.Squared_Gradient + (Hyperparameters_4[6]**2),0.1*(Hyperparameters_4[6]**2)))\n",
    "\n",
    "\n",
    "                                                                                                                                \n",
    "                                                                                                                                \n",
    "\n",
    "def Get_Best_Cost_Change_From_Individuals(Cost_Change_From_Probe_Samples,Indexes_Of_The_Suggestions_From_The_Different_Algorithms1):\n",
    "\n",
    " Best_Cost_Change_From_Individuals  = np.zeros(4)\n",
    " Indexes = Indexes_Of_The_Suggestions_From_The_Different_Algorithms1\n",
    "\n",
    " \n",
    " ### There is potential for speed up by writing this out explicitely\n",
    " for i in range (0,4):                                                                                                                       \n",
    "     index_min = np.argmin(Cost_Change_From_Probe_Samples[Indexes[i]:Indexes[i+1]])\n",
    "     Best_Cost_Change_From_Individuals[i] = Cost_Change_From_Probe_Samples[Indexes[i]:Indexes[i+1]][index_min]\n",
    "\n",
    " return  Best_Cost_Change_From_Individuals                                                                                \n",
    "\n",
    "                                                                                                                                \n",
    "                                                                                                                                \n",
    "                                                                                                                     \n",
    "      ## Update the Resouce Allocation\n",
    "def  Update_Resource_Allocation(Cost_Change_From_Probe_Samples,Indexes_Of_The_Suggestions_From_The_Different_Algorithms1,Suggestion_Tools_1,Suggestion_Tools_2,Suggestion_Tools_3,Suggestion_Tools_4,Hyperparameters_For_Symmetric_Conductor):\n",
    "                               \n",
    " # Normalize Cost\n",
    " if np.sum(np.abs(Cost_Change_From_Probe_Samples)) != 0:\n",
    "     Cost_Change_From_Probe_Samples = Cost_Change_From_Probe_Samples/(np.sqrt(np.matmul(Cost_Change_From_Probe_Samples,Cost_Change_From_Probe_Samples)))\n",
    "                                                                                                                           \n",
    " Best_Cost_Change_From_Individuals  = Get_Best_Cost_Change_From_Individuals(Cost_Change_From_Probe_Samples,Indexes_Of_The_Suggestions_From_The_Different_Algorithms1)                                                                                                                     \n",
    " Current_Resource_Allocation = np.array([Suggestion_Tools_1.Number_Of_Samples                                          ,Suggestion_Tools_2.Number_Of_Samples                                           ,Suggestion_Tools_3.Number_Of_Samples                                           ,Suggestion_Tools_4.Number_Of_Samples ])\n",
    "\n",
    " if print_Cost == 1:\n",
    "     print(\"Current_Resource_Allocation\",Current_Resource_Allocation)\n",
    "\n",
    " Recomended_Resource_Allocation = Symmetric_Section_Conductor(Current_Resource_Allocation,-Best_Cost_Change_From_Individuals,Hyperparameters_For_Symmetric_Conductor) \n",
    " Current_Resource_Allocation = Recomended_Resource_Allocation\n",
    "\n",
    " Suggestion_Tools_1.Number_Of_Samples =      Current_Resource_Allocation[0]                                                                                                                     \n",
    " Suggestion_Tools_2.Number_Of_Samples =      Current_Resource_Allocation[1] \n",
    " Suggestion_Tools_3.Number_Of_Samples =      Current_Resource_Allocation[2]                                                                                                                                  \n",
    " Suggestion_Tools_4.Number_Of_Samples =      Current_Resource_Allocation[3]    \n",
    "\n",
    " return Best_Cost_Change_From_Individuals\n",
    "\n",
    "\n",
    "\n",
    "def Update_Best_Individuals_And_Costs_From_This_Round(Best_Individual,Second_Best_Individual,Parameter_ChangeVector,Cost_Change,Suggestion_Tools_1,Suggestion_Tools_2,Suggestion_Tools_3,Suggestion_Tools_4):                                                                                                                           \n",
    "\n",
    "\n",
    " Suggestion_Tools_1.Best_Individual = Best_Individual                                                                                                                    \n",
    " Suggestion_Tools_1.Second_Best_Individual = Second_Best_Individual\n",
    " Suggestion_Tools_1.Parameter_ChangeVector = Parameter_ChangeVector[:,0]                                                                                                                    \n",
    " Suggestion_Tools_1.Cost_Change = Cost_Change                                                                                                                     \n",
    "\n",
    " Suggestion_Tools_2.Best_Individual = Best_Individual                                                                                                                    \n",
    " Suggestion_Tools_2.Second_Best_Individual = Second_Best_Individual\n",
    " Suggestion_Tools_2.Parameter_ChangeVector = Parameter_ChangeVector [:,0]                                                                                                                   \n",
    " Suggestion_Tools_2.Cost_Change = Cost_Change                                                                                                                                        \n",
    "\n",
    " Suggestion_Tools_3.Best_Individual = Best_Individual                                                                                                                    \n",
    " Suggestion_Tools_3.Second_Best_Individual = Second_Best_Individual\n",
    " Suggestion_Tools_3.Parameter_ChangeVector = Parameter_ChangeVector[:,0]                                                                                                                    \n",
    " Suggestion_Tools_3.Cost_Change = Cost_Change                                                                                                                                        \n",
    "\n",
    " Suggestion_Tools_4.Best_Individual = Best_Individual                                                                                                                    \n",
    " Suggestion_Tools_4.Second_Best_Individual = Second_Best_Individual\n",
    " Suggestion_Tools_4.Parameter_ChangeVector = Parameter_ChangeVector[:,0]                                                                                                                    \n",
    " Suggestion_Tools_4.Cost_Change = Cost_Change  \n",
    "\n",
    "\n",
    " \n",
    "def Optimize_With_TWO_Mode_Section(Best_Individual,Second_Best_Individual,PreviousCost,fun,current_iteration,Total_Number_Of_Iterations,Number_Of_Iterations_With_These_Hyperparameters,Hyperparameters_1,Hyperparameters_2,Hyperparameters_3,Hyperparameters_4,Suggestion_Tools_1,Suggestion_Tools_2,Suggestion_Tools_3,Suggestion_Tools_4,Number_Of_Evaluations,Allowed_Number_Of_Function_Evaluations,ForceStop,Cost_Tracker ,Best_Quarter_Of_Individuals):\n",
    "\n",
    " \n",
    " Restart = 0\n",
    " Best_Cost_Change_From_Individuals1 = np.zeros((Number_Of_Algorithms,Number_Of_Iterations_With_These_Hyperparameters))\n",
    "      \n",
    " if current_iteration == 0:\n",
    "     Parameter_ChangeVector = np.zeros((len(Best_Individual),1))\n",
    "     Cost_Change = 0\n",
    "     Update_Best_Individuals_And_Costs_From_This_Round(Best_Individual,Second_Best_Individual,Parameter_ChangeVector,Cost_Change,Suggestion_Tools_1,Suggestion_Tools_2,Suggestion_Tools_3,Suggestion_Tools_4)                                                                                                                           \n",
    "\n",
    "                                                                                                                       \n",
    " for i in range(0,Number_Of_Iterations_With_These_Hyperparameters):\n",
    "\n",
    "         \n",
    "         \n",
    "     current_iteration= current_iteration + 1\n",
    "     ## Readout ony if wanted                                                                                                                           \n",
    "     if print_Cost == 1:\n",
    "         print(\"____________________________________________________\")\n",
    "         print(\"Iteration\",current_iteration,\"\\\\\",Total_Number_Of_Iterations)\n",
    "\n",
    "     \n",
    "                                                                                                                             \n",
    "     ##  Get Sample Suggestions from the Section    \n",
    "     if len(Cost_Tracker) <=1:\n",
    "         All_Suggestions_From_Two_Mode_Section1,Indexes_Of_The_Suggestions_From_The_Different_Algorithms1 =  Get_Suggestions_From_Two_Mode_Section(fun,Hyperparameters_1,Hyperparameters_2,Hyperparameters_3,Hyperparameters_4,Suggestion_Tools_1,Suggestion_Tools_2,Suggestion_Tools_3,Suggestion_Tools_4,Hyperparameters_For_Symmetric_Conductor)\n",
    "     \n",
    "     \n",
    "     if len(Cost_Tracker) > 1:\n",
    "         All_Suggestions_From_Two_Mode_Section1,Indexes_Of_The_Suggestions_From_The_Different_Algorithms1 =  Get_Suggestions_From_Two_Mode_Section2(fun,Hyperparameters_1,Hyperparameters_2,Hyperparameters_3,Hyperparameters_4,Suggestion_Tools_1,Suggestion_Tools_2,Suggestion_Tools_3,Suggestion_Tools_4,Hyperparameters_For_Symmetric_Conductor,Best_Quarter_Of_Individuals)\n",
    "\n",
    "     Cost = np.zeros(len(All_Suggestions_From_Two_Mode_Section1[1,:]))                                                                                                                         \n",
    "     ## Sample the Function at the suggested sample points\n",
    "     for j in range(0,len(All_Suggestions_From_Two_Mode_Section1[1,:])):\n",
    "         \n",
    "         if Number_Of_Evaluations > Allowed_Number_Of_Function_Evaluations:\n",
    "\n",
    "             CurrentCost = PreviousCost\n",
    "             ForceStop = 1\n",
    "             break\n",
    "         Cost[j] =fun(All_Suggestions_From_Two_Mode_Section1[:,j])\n",
    "         Number_Of_Evaluations +=1\n",
    "     if ForceStop == 1:\n",
    "         break\n",
    "\n",
    "\n",
    "     ## Find the Optimzal Sample and calculate the change vectors                                                                                                                          \n",
    "     index_min = np.argmin(Cost)\n",
    "     CurrentCost = Cost[index_min ]\n",
    "     Cost_Change = CurrentCost - PreviousCost\n",
    "     Parameter_ChangeVector = np.array([All_Suggestions_From_Two_Mode_Section1[:,index_min] - Best_Individual]).T\n",
    "\n",
    "\n",
    "     # Update the Addaptive Amplitudes for the next search\n",
    "     # This should minimize the amplitude of ones that have large variance, and increase those which have high variance.\n",
    "     Cost_Change_From_Probe_Samples  = Cost - PreviousCost\n",
    "\n",
    "     Change_Vectors = All_Suggestions_From_Two_Mode_Section1 - np.array([Best_Individual]).T\n",
    "\n",
    "     Gradients =   Cost_Change_From_Probe_Samples/((np.maximum(Change_Vectors,-0.0001) + np.minimum(Change_Vectors,-0.0001)) + 0.001)\n",
    "\n",
    "     Update_Addaptive_Amplitudes(Gradients,Hyperparameters_1,Hyperparameters_2,Hyperparameters_3,Hyperparameters_4,Suggestion_Tools_1,Suggestion_Tools_2,Suggestion_Tools_3,Suggestion_Tools_4)\n",
    " \n",
    "\n",
    "\n",
    "      ## Update Resource Allocation          \n",
    "     Best_Cost_Change_From_Individuals1[:,i] = Update_Resource_Allocation(Cost_Change_From_Probe_Samples,Indexes_Of_The_Suggestions_From_The_Different_Algorithms1,Suggestion_Tools_1,Suggestion_Tools_2,Suggestion_Tools_3,Suggestion_Tools_4,Hyperparameters_For_Symmetric_Conductor)\n",
    "                                                                                                                   \n",
    "                                                                                                                                                                                                                                                                                                                                                     \n",
    "     ## Update Best_Individuals_And_Costs_From_This_Round                                                                                                                                       \n",
    "     Best_Individual = All_Suggestions_From_Two_Mode_Section1[:,index_min]\n",
    "\n",
    "     ## Get the Best Quarter of the population\n",
    "     # This is 1/8th actually\n",
    "     Best_Quarter_Of_Individuals =  All_Suggestions_From_Two_Mode_Section1[:,np.array(np.nonzero(Cost <= np.median(Cost[np.nonzero(Cost <= np.median(Cost[np.nonzero(Cost <= np.median(Cost))]))])))[0,:]]\n",
    "\n",
    "     \n",
    "     Cost[index_min ] = 10000*Cost[index_min ]\n",
    "     index_min = np.argmin(Cost)\n",
    "     Second_Best_Individual = All_Suggestions_From_Two_Mode_Section1[:,index_min]\n",
    "     PreviousCost = CurrentCost                                                                                                                \n",
    "     Update_Best_Individuals_And_Costs_From_This_Round(Best_Individual,Second_Best_Individual,Parameter_ChangeVector,Cost_Change,Suggestion_Tools_1,Suggestion_Tools_2,Suggestion_Tools_3,Suggestion_Tools_4)                                                                                                                           \n",
    "\n",
    "     \n",
    "     Cost_Tracker.append(CurrentCost) \n",
    "\n",
    "\n",
    "     ############### Restart Conditions ##########################\n",
    "     if np.min(np.array([Suggestion_Tools_1.Random_Search_exponential_Growth_Factor,               Suggestion_Tools_2.Random_Search_exponential_Growth_Factor,Suggestion_Tools_3.Random_Search_exponential_Growth_Factor,               Suggestion_Tools_4.Random_Search_exponential_Growth_Factor])) > 50:\n",
    "         Restart = 1\n",
    "#             print(\"Restart Triggered by Random Search Growth Factor\")\n",
    "         break\n",
    "     \n",
    "     \n",
    "     if len(Cost_Tracker) > 100: ## Not Made Much Improvement lately - on absolute scale\n",
    "         if Cost_Tracker[len(Cost_Tracker)-50] - Cost_Tracker[len(Cost_Tracker)-1] < 0.5*10**-13:\n",
    "             Restart = 1\n",
    "#                 print(\"Restart Triggered by Not Made Much Improvement lately\")\n",
    "             break\n",
    "\n",
    "\n",
    "     if len(Cost_Tracker) > 100: ## Not Made Much Improvement lately - on relative scale\n",
    "         if np.abs((Cost_Tracker[len(Cost_Tracker)-50] - Cost_Tracker[len(Cost_Tracker)-1])/Cost_Tracker[0]) < 0.5*10**-13:\n",
    "             Restart = 1\n",
    "#                 print(\"Restart Triggered by Not Made Much Improvement lately - relative scale\")\n",
    "             break\n",
    "             \n",
    "     if len(Cost_Tracker) > 100: ## Stagnation\n",
    "         if Cost_Tracker[len(Cost_Tracker)-50] == Cost_Tracker[len(Cost_Tracker)-1]:\n",
    "             Restart = 1\n",
    "#                 print(\"Restart Triggered by Stagnation\")\n",
    "             break        \n",
    "     \n",
    " \n",
    "     \n",
    "     if print_Cost == 1:\n",
    "         print(\"Run Summary:\")\n",
    "         print(\"---------------------------------------------------------------------\")\n",
    "         print(\"CurrentCost is:\",CurrentCost,\"After Running Iteration\",current_iteration,\"\\\\\",Total_Number_Of_Iterations)\n",
    "         print(\"the Random_Search_exponential_Growth_Factors are :\",Suggestion_Tools_1.Random_Search_exponential_Growth_Factor,               Suggestion_Tools_2.Random_Search_exponential_Growth_Factor,Suggestion_Tools_3.Random_Search_exponential_Growth_Factor,               Suggestion_Tools_4.Random_Search_exponential_Growth_Factor)\n",
    "         print(\"the Targeted_Search_exponential_Growth_Factor is:\",Suggestion_Tools_1.Targeted_Search_exponential_Growth_Factor   ,              Suggestion_Tools_2.Targeted_Search_exponential_Growth_Factor   ,Suggestion_Tools_3.Targeted_Search_exponential_Growth_Factor   ,              Suggestion_Tools_4.Targeted_Search_exponential_Growth_Factor   )    \n",
    "         print(\"---------------------------------------------------------------------\")\n",
    "         print(\"\")            \n",
    "         \n",
    "       \n",
    " return Best_Individual,Second_Best_Individual,CurrentCost,PreviousCost,Best_Cost_Change_From_Individuals1,current_iteration,Number_Of_Evaluations ,ForceStop,Restart,Cost_Tracker,Best_Quarter_Of_Individuals\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "\n",
    "def Force_Suggested_Hyperparameters_To_be_Within_The_Allowed_Limits(Suggested_Hyper_Parameter_Samples_From_Two_Mode,Hyperparameter_Allowed_Values):\n",
    "\n",
    "## The 0 second index is the lower bound of the allowed values, and the 1 second index is the upper bound\n",
    "\n",
    "    Suggested_Hyper_Parameter_Samples_From_Two_Mode[0,:] = np.minimum(np.maximum(Suggested_Hyper_Parameter_Samples_From_Two_Mode[0,:],Hyperparameter_Allowed_Values[0,0]),Hyperparameter_Allowed_Values[0,1])\n",
    "    Suggested_Hyper_Parameter_Samples_From_Two_Mode[1,:] = np.minimum(np.maximum(Suggested_Hyper_Parameter_Samples_From_Two_Mode[1,:],Hyperparameter_Allowed_Values[1,0]),Hyperparameter_Allowed_Values[1,1])\n",
    "    Suggested_Hyper_Parameter_Samples_From_Two_Mode[2,:] = np.minimum(np.maximum(Suggested_Hyper_Parameter_Samples_From_Two_Mode[2,:],Hyperparameter_Allowed_Values[2,0]),Hyperparameter_Allowed_Values[2,1])\n",
    "    Suggested_Hyper_Parameter_Samples_From_Two_Mode[3,:] = np.minimum(np.maximum(Suggested_Hyper_Parameter_Samples_From_Two_Mode[3,:],Hyperparameter_Allowed_Values[3,0]),Hyperparameter_Allowed_Values[3,1])\n",
    "    Suggested_Hyper_Parameter_Samples_From_Two_Mode[4,:] = np.minimum(np.maximum(Suggested_Hyper_Parameter_Samples_From_Two_Mode[4,:],Hyperparameter_Allowed_Values[4,0]),Hyperparameter_Allowed_Values[4,1])\n",
    "    Suggested_Hyper_Parameter_Samples_From_Two_Mode[5,:] = np.minimum(np.maximum(Suggested_Hyper_Parameter_Samples_From_Two_Mode[5,:],Hyperparameter_Allowed_Values[5,0]),Hyperparameter_Allowed_Values[5,1])\n",
    "    Suggested_Hyper_Parameter_Samples_From_Two_Mode[6,:] = np.minimum(np.maximum(Suggested_Hyper_Parameter_Samples_From_Two_Mode[6,:],Hyperparameter_Allowed_Values[6,0]),Hyperparameter_Allowed_Values[6,1])\n",
    "    Suggested_Hyper_Parameter_Samples_From_Two_Mode[7,:] = np.minimum(np.maximum(Suggested_Hyper_Parameter_Samples_From_Two_Mode[7,:],Hyperparameter_Allowed_Values[7,0]),Hyperparameter_Allowed_Values[7,1])\n",
    "\n",
    "    \n",
    "    Hyperparameters_1 = Suggested_Hyper_Parameter_Samples_From_Two_Mode[:,0]\n",
    "    Hyperparameters_2 = Suggested_Hyper_Parameter_Samples_From_Two_Mode[:,1]\n",
    "    Hyperparameters_3 = Suggested_Hyper_Parameter_Samples_From_Two_Mode[:,2]\n",
    "    Hyperparameters_4 = Suggested_Hyper_Parameter_Samples_From_Two_Mode[:,3]\n",
    "\n",
    "    return Hyperparameters_1,Hyperparameters_2,Hyperparameters_3,Hyperparameters_4,Suggested_Hyper_Parameter_Samples_From_Two_Mode\n",
    "\n",
    "\n",
    "\n",
    "def Run_Optimization_With_TWO_Mode_Online_Hyperparameter_Addaptation(InitialParameters,fun,Total_Number_Of_Iterations,Number_Of_Iterations_With_Hyperparameters_Set,Hyperparameters_1,Hyperparameters_2,Hyperparameters_3,Hyperparameters_4,Suggestion_Tools_1,Suggestion_Tools_2,Suggestion_Tools_3,Suggestion_Tools_4,Hyperparameters_For_Online_Hyperparameter_Search,print_Cost,Total_Resources,Hyperparameter_Allowed_Values,Number_Of_Evaluations,Allowed_Number_Of_Function_Evaluations,ForceStop,Restart  ):\n",
    "\n",
    "    Cost_Tracker = []\n",
    "    Number_Of_Iterations_With_These_Hyperparameters = Number_Of_Iterations_With_Hyperparameters_Set\n",
    "\n",
    "    \n",
    "    Random_Search_Growth_Rate  = Hyperparameters_For_Online_Hyperparameter_Search[0]\n",
    "    Random_Search_Period = Hyperparameters_For_Online_Hyperparameter_Search[1]\n",
    "    Maximal_Random_Search =Hyperparameters_For_Online_Hyperparameter_Search[2]\n",
    "    Targeted_Search_Growth_Rate = Hyperparameters_For_Online_Hyperparameter_Search[3]\n",
    "    Targeted_Search_Decay_Rate = Hyperparameters_For_Online_Hyperparameter_Search[4]\n",
    "    Significant_Change_Value = Hyperparameters_For_Online_Hyperparameter_Search[5]\n",
    "    alpha = Hyperparameters_For_Online_Hyperparameter_Search[6]\n",
    "    beta = Hyperparameters_For_Online_Hyperparameter_Search[7]\n",
    "\n",
    "    ## Initialize the suggestion tools for the hyperparameter optimization\n",
    "    Random_Search_exponential_Growth_Factor = 0\n",
    "    Targeted_Search_exponential_Growth_Factor = 0\n",
    "    Hyper_Parameter_ChangeVector = np.zeros(len(Hyperparameters_1))\n",
    "    Hyper_Parameter_Cost_Change = 0\n",
    "    current_iteration = 0\n",
    "    \n",
    "    \n",
    "    ## This is to set the hyperparameters for optimization\n",
    "    Number_Of_Samples = Number_Of_Algorithms\n",
    "    Best_Individual_Hyperparameters = Hyperparameters_1\n",
    "    Second_Best_Individual_Hyperparameters = Hyperparameters_1\n",
    "    \n",
    "    ## This is to set the function parameters for optimization    \n",
    "    Best_Individual_Parameters =     InitialParameters\n",
    "    Second_Best_Individual_Parameters =     InitialParameters\n",
    "    PreviousCost = fun(InitialParameters)\n",
    "    Best_Quarter_Of_Individuals = np.zeros((len(InitialParameters),int(Total_Resources/4)))\n",
    "\n",
    "    ## Update Directional Pools with random gradients\n",
    "    Random_Parameter_Gradients = np.random.rand(len(InitialParameters),Total_Resources)\n",
    "    Random_Parameter_Gradients[:,0] = 0\n",
    "    Cost_Change_From_Probe_Samples = -np.ones(Total_Resources)\n",
    "\n",
    "    ## This is find the Cost of the first set of hyperparameters\n",
    "    Best_Individual_Parameters,Second_Best_Individual_Parameters,CurrentCost,PreviousCost,Best_Cost_Change_From_Individuals1,current_iteration,Number_Of_Evaluations,ForceStop,Restart,Cost_Tracker,Best_Quarter_Of_Individuals  =  Optimize_With_TWO_Mode_Section(Best_Individual_Parameters,Second_Best_Individual_Parameters,PreviousCost,fun,current_iteration,Total_Number_Of_Iterations,Number_Of_Iterations_With_These_Hyperparameters,Hyperparameters_1,Hyperparameters_2,Hyperparameters_3,Hyperparameters_4,Suggestion_Tools_1,Suggestion_Tools_2,Suggestion_Tools_3,Suggestion_Tools_4,Number_Of_Evaluations,Allowed_Number_Of_Function_Evaluations,ForceStop ,Cost_Tracker,Best_Quarter_Of_Individuals)\n",
    "\n",
    "    Cost_Function_For_Hyperparameter_Optimization =( np.mean(Best_Cost_Change_From_Individuals1,1) + np.max(Best_Cost_Change_From_Individuals1,1))/2\n",
    "    index_min = np.argmin(Cost_Function_For_Hyperparameter_Optimization)\n",
    "    Current_Hyperparameter_Cost = Cost_Function_For_Hyperparameter_Optimization[index_min ]\n",
    "    Previous_Hyperparameter_Cost = Cost_Function_For_Hyperparameter_Optimization[index_min]\n",
    "\n",
    "    Adaptive_Amplitude = np.ones((len(Hyperparameters_1),Number_Of_Samples))\n",
    "    Cost_Function_For_Hyperparameter_Optimization = np.zeros(Number_Of_Samples)\n",
    "    S = 0\n",
    "    \n",
    "       \n",
    "    Number_Of_Hyperparameter_Iterations = np.ceil(Total_Number_Of_Iterations/Number_Of_Iterations_With_These_Hyperparameters)\n",
    "\n",
    "    for i in range(0,int(Number_Of_Hyperparameter_Iterations)):\n",
    "        \n",
    "        if Restart  ==1:\n",
    "            break\n",
    "\n",
    "        ## Generate suggested hyperparameters    \n",
    "        Suggested_Hyper_Parameter_Samples_From_Two_Mode, Random_Search_exponential_Growth_Factor, Targeted_Search_exponential_Growth_Factor =     Two_Mode_Optimizer(Best_Individual_Hyperparameters,Second_Best_Individual_Hyperparameters,Number_Of_Samples,Hyper_Parameter_ChangeVector,Hyper_Parameter_Cost_Change, Random_Search_exponential_Growth_Factor,Targeted_Search_exponential_Growth_Factor,Targeted_Search_Decay_Rate,Random_Search_Growth_Rate,Targeted_Search_Growth_Rate,Random_Search_Period,Significant_Change_Value,Maximal_Random_Search,Adaptive_Amplitude)\n",
    "        ## Force the hyperparameters to be within the allowed range\n",
    "        Hyperparameters_1,Hyperparameters_2,Hyperparameters_3,Hyperparameters_4,Suggested_Hyper_Parameter_Samples_From_Two_Mode = Force_Suggested_Hyperparameters_To_be_Within_The_Allowed_Limits(Suggested_Hyper_Parameter_Samples_From_Two_Mode,Hyperparameter_Allowed_Values)\n",
    " \n",
    "\n",
    "        ## Update Directional Pools with random gradients\n",
    "        Random_Parameter_Gradients = np.random.rand(len(InitialParameters),Total_Resources)\n",
    "        Random_Parameter_Gradients[:,0] = 0\n",
    "        Cost_Change_From_Probe_Samples = -np.ones(Total_Resources)\n",
    "\n",
    "    \n",
    "        ## Try to optimize with these hyperparameters (Maybe output the suggestion tools as well?)\n",
    "        Best_Individual_Parameters,Second_Best_Individual_Parameters,CurrentCost,PreviousCost,Best_Cost_Change_From_Individuals1,current_iteration,Number_Of_Evaluations,ForceStop,Restart,Cost_Tracker,Best_Quarter_Of_Individuals  =  Optimize_With_TWO_Mode_Section(Best_Individual_Parameters,Second_Best_Individual_Parameters,PreviousCost,fun,current_iteration,Total_Number_Of_Iterations,Number_Of_Iterations_With_These_Hyperparameters,Hyperparameters_1,Hyperparameters_2,Hyperparameters_3,Hyperparameters_4,Suggestion_Tools_1,Suggestion_Tools_2,Suggestion_Tools_3,Suggestion_Tools_4,Number_Of_Evaluations ,Allowed_Number_Of_Function_Evaluations,ForceStop,Cost_Tracker,Best_Quarter_Of_Individuals)\n",
    "       \n",
    "    \n",
    "        ## Calculate the Cost_Function_For_Hyperparameter_Optimization\n",
    "        Cost_Function_For_Hyperparameter_Optimization =( np.mean(Best_Cost_Change_From_Individuals1,1) + np.min(Best_Cost_Change_From_Individuals1,1))/2\n",
    "        Cost_Function_For_Hyperparameter_Optimization1 = np.array([Cost_Function_For_Hyperparameter_Optimization[0],Cost_Function_For_Hyperparameter_Optimization[1],Cost_Function_For_Hyperparameter_Optimization[2],Cost_Function_For_Hyperparameter_Optimization[3]])\n",
    "\n",
    "        \n",
    "        ## Find the Best set of hyperparameters and the change vectors\n",
    "        index_min = np.argmin(Cost_Function_For_Hyperparameter_Optimization)\n",
    "        Current_Hyperparameter_Cost = Cost_Function_For_Hyperparameter_Optimization[index_min ]\n",
    "        Hyper_Parameter_Cost_Change = Current_Hyperparameter_Cost - Previous_Hyperparameter_Cost\n",
    "        Hyper_Parameter_ChangeVector = Suggested_Hyper_Parameter_Samples_From_Two_Mode[:,index_min] - Best_Individual_Hyperparameters\n",
    "\n",
    "\n",
    "        \n",
    "        ## This is the RMS Amplitude Addaptation - For the hyperparameter search\n",
    "        Cost_Change_From_Probe_Samples  = Cost_Function_For_Hyperparameter_Optimization - Cost_Function_For_Hyperparameter_Optimization[0]\n",
    "\n",
    "        Change_Vectors = Suggested_Hyper_Parameter_Samples_From_Two_Mode - np.array([Best_Individual_Hyperparameters]).T\n",
    "\n",
    "        Gradient =   Cost_Change_From_Probe_Samples/((np.maximum(Change_Vectors,-0.0001) + np.minimum(Change_Vectors,-0.0001)) + 0.001)\n",
    "\n",
    "        S = beta*S + (1-beta)*(np.mean(np.abs(Gradient),1))**2\n",
    "        S = S*np.sign(np.mean(Gradient,1))\n",
    "        Adaptive_Amplitude1 = alpha/np.sqrt(np.fmax(S + alpha**2,0.1*alpha**2))\n",
    "        Adaptive_Amplitude[:,:] = np.array([Adaptive_Amplitude1]).T ## this makes an parameter wise adaptive amplitude Based on RMS prob, which we can do element wise multiplication with the random searches.\n",
    "        # This should minimize the amplitude of ones that have large variance, and increase those which have high variance.\n",
    "\n",
    "        \n",
    "        ## Save the Best Sets of hyperparameters and Cost Function\n",
    "        Best_Individual_Hyperparameters = Suggested_Hyper_Parameter_Samples_From_Two_Mode[:,index_min]\n",
    "        Cost_Function_For_Hyperparameter_Optimization[index_min ] = (10**10) + Cost_Function_For_Hyperparameter_Optimization[index_min ]\n",
    "        index_min = np.argmin(Cost_Function_For_Hyperparameter_Optimization)\n",
    "        Second_Best_Individual_Hyperparameters = Suggested_Hyper_Parameter_Samples_From_Two_Mode[:,index_min]\n",
    "        \n",
    "\n",
    "        Previous_Hyperparameter_Cost = Current_Hyperparameter_Cost\n",
    "        \n",
    "\n",
    "        \n",
    "        if print_Cost == 1:\n",
    "            print(\"hyperparameter Summary:\")\n",
    "            print(\"---------------------------------------------------------------------\")\n",
    "            print(\"After Running hyperparameter Iteration\",i,\"\\\\\",int(Number_Of_Hyperparameter_Iterations))\n",
    "            print(\"Cost_Function_For_Hyperparameter_Optimization\",Cost_Function_For_Hyperparameter_Optimization1)          \n",
    "            print(\"\")\n",
    "            print(\"Best_Individual_Hyperparameters\",Best_Individual_Hyperparameters)\n",
    "            print(\"\")\n",
    "            print(\"the Random_Search_exponential_Growth_Factor for the hyperparameter search is:\",Random_Search_exponential_Growth_Factor)\n",
    "            print(\"the Targeted_Search_exponential_Growth_Factor for the hyperparameter search is:\",Targeted_Search_exponential_Growth_Factor)        \n",
    "            print(\"---------------------------------------------------------------------\")\n",
    "            print(\"\")            \n",
    "            \n",
    "\n",
    "    return Best_Individual_Parameters,CurrentCost,Number_Of_Evaluations ,ForceStop,Restart\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "def Set_Hyperparameter_Ranges(Random_Search_Growth_Rate_Range, Random_Search_Period_Range   ,Maximal_Random_Search_Range,Targeted_Search_Growth_Rate_Range ,Targeted_Search_Decay_Rate_Range ,Significant_Change_Value_Range,alpha_Range ,beta_Range):\n",
    "    Hyperparameter_Allowed_Values = np.zeros((11,2))\n",
    "\n",
    "    Hyperparameter_Allowed_Values[0,:] = Random_Search_Growth_Rate_Range\n",
    "    Hyperparameter_Allowed_Values[1,:] = Random_Search_Period_Range    \n",
    "    Hyperparameter_Allowed_Values[2,:] = Maximal_Random_Search_Range    \n",
    "    Hyperparameter_Allowed_Values[3,:] = Targeted_Search_Growth_Rate_Range    \n",
    "    Hyperparameter_Allowed_Values[4,:] = Targeted_Search_Decay_Rate_Range   \n",
    "    Hyperparameter_Allowed_Values[5,:] = Significant_Change_Value_Range\n",
    "    Hyperparameter_Allowed_Values[6,:] = alpha_Range\n",
    "    Hyperparameter_Allowed_Values[7,:] = beta_Range\n",
    "\n",
    "    return Hyperparameter_Allowed_Values\n",
    "\n",
    "\n",
    "def Mean_ChangeValue_fit(Maximal_Random_Search,Number_Of_Parameters):\n",
    "    fit_Parameters = [-0.007921662,0.003755446,5.92*(10**(-5)),-2.45*(10**(-7)),0.250014494] # These were found numerically from the file \"Finding_The_Segnificant_Change_Value.ipynb\"\n",
    "    return    fit_Parameters[0]*Maximal_Random_Search + fit_Parameters[1]*Maximal_Random_Search**2 + fit_Parameters[2]*Number_Of_Parameters + fit_Parameters[3]*Number_Of_Parameters**2 + fit_Parameters[4]*Maximal_Random_Search*Number_Of_Parameters\n",
    "    \n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "def Orchestra_Optimize_With_Two_Mode_Section(x0,fun,Allowed_Number_Of_Function_Evaluations,print_Cost):\n",
    "\n",
    "    Number_Of_Iterations = int(np.maximum(np.ceil(Allowed_Number_Of_Function_Evaluations/60) -2,1))\n",
    "\n",
    "    Number_Of_Parameters = len(x0)\n",
    "    InitialParameters = x0\n",
    "    Total_Number_Of_Iterations = Number_Of_Iterations\n",
    "    \n",
    "    Number_Of_Iterations_With_Hyperparameters_Set = 5\n",
    "    \n",
    "    \n",
    "\n",
    "    ############################## Setting up The Conductor (Resource allocation) algorithm ###################################\n",
    "    Total_Resources = 64\n",
    "    Number_Of_Algorithms = 4\n",
    "    Mass =10\n",
    "    Self_Spring_Constants = 10\n",
    "    Interaction_Spring_Constants = 40\n",
    "\n",
    "    Hyperparameters_For_Symmetric_Conductor = Set_Hyperparameters_For_Symmetric_Conductor(Total_Resources,Number_Of_Algorithms,Mass,Self_Spring_Constants,Interaction_Spring_Constants)\n",
    "    Initial_Resource_Allocation = Hyperparameters_For_Symmetric_Conductor[:,0]\n",
    "    Current_Resource_Allocation = Hyperparameters_For_Symmetric_Conductor[:,0]\n",
    "\n",
    "    Changevector = np.array([0, 0 ,0 ,0]) ## Negative is bad\n",
    "    Recomended_Resource_Allocation = Symmetric_Section_Conductor(Current_Resource_Allocation,Changevector,Hyperparameters_For_Symmetric_Conductor) \n",
    "    Current_Resource_Allocation = Recomended_Resource_Allocation\n",
    "\n",
    "    #############################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    ######################### Setting the hyperparameter ranges ######################################################################\n",
    "    Random_Search_Growth_Rate_Range   = np.array([0.01,10])\n",
    "    Random_Search_Period_Range  = np.array([0.1,10])\n",
    "    Maximal_Random_Search_Range  = np.array([0.0000001,10])\n",
    "    Targeted_Search_Growth_Rate_Range  = np.array([0.01,5])\n",
    "    Targeted_Search_Decay_Rate_Range  = np.array([0.1,100]) \n",
    "    Significant_Change_Value_Range  = np.array([10**-30,0.01]) \n",
    "    alpha_Range = np.array([10**-5,10**15]) ## (Here Do 10**random)\n",
    "    beta_Range = np.array([0.1,1]) \n",
    "    Directional_Pool_Ratio_Range = np.array([0.0001,1]) \n",
    "\n",
    "    Hyperparameter_Allowed_Values = Set_Hyperparameter_Ranges(Random_Search_Growth_Rate_Range, Random_Search_Period_Range   ,Maximal_Random_Search_Range,Targeted_Search_Growth_Rate_Range ,Targeted_Search_Decay_Rate_Range ,Significant_Change_Value_Range,alpha_Range ,beta_Range )\n",
    "    #############################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ############################### Setting the Hyperparameters_For_Online_Hyperparameter_improvement  Algorithm #########################\n",
    "    Hyperparameters_For_Online_Hyperparameter_Search = np.zeros(8)\n",
    "    Hyperparameters_For_Online_Hyperparameter_Search[0] =5.0   # Random_Search_Growth_Rate \n",
    "    Hyperparameters_For_Online_Hyperparameter_Search[1] =5.0    # Random_Search_Period \n",
    "    Hyperparameters_For_Online_Hyperparameter_Search[2] =20.02   # Maximal_Random_Search \n",
    "    Hyperparameters_For_Online_Hyperparameter_Search[3] =0.5    # Targeted_Search_Growth_Rate\n",
    "    Hyperparameters_For_Online_Hyperparameter_Search[4] =2.0    # Targeted_Search_Decay_Rate \n",
    "    Hyperparameters_For_Online_Hyperparameter_Search[5] =0.00001*Mean_ChangeValue_fit(Hyperparameters_For_Online_Hyperparameter_Search[2] ,8)    # Significant_Change_Value\n",
    "    Hyperparameters_For_Online_Hyperparameter_Search[6] =(10**4)/Hyperparameters_For_Online_Hyperparameter_Search[2]    # alpha \n",
    "    Hyperparameters_For_Online_Hyperparameter_Search[7] =0.6     # beta\n",
    "\n",
    "    #############################################################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    Number_Of_Evaluations  = 0\n",
    "    ForceStop = 0\n",
    "    Restart = 0\n",
    "    Best_Cost = 10**100\n",
    "    Number_Of_Restarts = 0\n",
    "    while ForceStop ==0:    \n",
    "        ########################initialize the different Suggestion_Tools For the Two Mode and the hyperparameters #######################\n",
    "        Suggestion_Tools_1 = Create_New_Two_Mode_Suggestion_Tools_Object(Number_Of_Parameters)\n",
    "        Suggestion_Tools_2 = Create_New_Two_Mode_Suggestion_Tools_Object(Number_Of_Parameters)\n",
    "        Suggestion_Tools_3 = Create_New_Two_Mode_Suggestion_Tools_Object(Number_Of_Parameters)\n",
    "        Suggestion_Tools_4 = Create_New_Two_Mode_Suggestion_Tools_Object(Number_Of_Parameters)\n",
    "\n",
    "        Suggestion_Tools_1.Number_Of_Samples =      Current_Resource_Allocation[0]                                                                                                                     \n",
    "        Suggestion_Tools_2.Number_Of_Samples =      Current_Resource_Allocation[1] \n",
    "        Suggestion_Tools_3.Number_Of_Samples =      Current_Resource_Allocation[2]                                                                                                                                  \n",
    "        Suggestion_Tools_4.Number_Of_Samples =      Current_Resource_Allocation[3]  \n",
    "\n",
    "\n",
    "\n",
    "        ## This is to initiailize the Hyperparameters - Note that they should all be positive numbers (Maybe make 6 and 7 be 10**rand(2))\n",
    "        Initial_Hyper_Parameter_Samples = 10*np.random.rand(11,Number_Of_Algorithms)\n",
    "        Hyperparameters_1,Hyperparameters_2,Hyperparameters_3,Hyperparameters_4,Initial_Hyper_Parameter_Samples = Force_Suggested_Hyperparameters_To_be_Within_The_Allowed_Limits(Initial_Hyper_Parameter_Samples,Hyperparameter_Allowed_Values)\n",
    "        #############################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        Best_Parameters1,Best_Cost1,Number_Of_Evaluations,ForceStop,Restart  = Run_Optimization_With_TWO_Mode_Online_Hyperparameter_Addaptation(InitialParameters,fun,Total_Number_Of_Iterations,Number_Of_Iterations_With_Hyperparameters_Set,Hyperparameters_1,Hyperparameters_2,Hyperparameters_3,Hyperparameters_4,Suggestion_Tools_1,Suggestion_Tools_2,Suggestion_Tools_3,Suggestion_Tools_4,Hyperparameters_For_Online_Hyperparameter_Search,print_Cost,Total_Resources,Hyperparameter_Allowed_Values,Number_Of_Evaluations,Allowed_Number_Of_Function_Evaluations,ForceStop,Restart )\n",
    "        \n",
    "\n",
    "        if Restart == 1:\n",
    "            Number_Of_Restarts +=1\n",
    "#             print(Number_Of_Restarts)\n",
    "#             print(Best_Cost1)\n",
    "\n",
    "        #if np.mod(Number_Of_Restarts,2) ==1 :\n",
    "        InitialParameters = Best_Parameters1\n",
    "\n",
    "      #  if np.mod(Number_Of_Restarts,2) ==0 :\n",
    "      #      InitialParameters = 10*(np.random.rand(len(Best_Parameters1)) - 0.5)\n",
    "            \n",
    "        Restart = 0\n",
    "        if Best_Cost1 <= Best_Cost:\n",
    "\n",
    "            Best_Cost = Best_Cost1\n",
    "            Best_Parameters = Best_Parameters1\n",
    "\n",
    "\n",
    "    return Best_Parameters\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "from multiprocessing import Process\n",
    "\n",
    "def fun(x): ## Ellispod\n",
    "#     return np.sum((x-2)**2) + np.sum(np.sin(x))\n",
    "\n",
    "    a = 1**(np.arange(D)/D)\n",
    "    return np.sum(a*((x-X_Optimal)**2) )\n",
    "\n",
    "\n",
    "D = 320\n",
    "Number_Of_Function_Evaluations = 3000*D\n",
    "Number_Of_Tests = 11\n",
    "print_Cost = 0\n",
    "Print_Two_Mode_Analytics = 0\n",
    "test_Optimization = 0\n",
    "if test_Optimization == 1:\n",
    "\n",
    "    for p in range (Number_Of_Tests):\n",
    "        x0 = 10*(np.random.rand(D)-0.5)\n",
    "        X_Optimal = 10*(np.random.rand(D)-0.5)\n",
    "        print(\"Initial Cost\",fun(x0))\n",
    "        Best_Parameters_After_Optimization = Orchestra_Optimize_With_Two_Mode_Section(x0,fun,Number_Of_Function_Evaluations,print_Cost)\n",
    "        print(\"Final Cost\",fun(Best_Parameters_After_Optimization))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Optimization = 0\n",
    "if Test_Optimization == 1:\n",
    "\n",
    "\n",
    "\n",
    "    from multiprocessing import Process\n",
    "\n",
    "    def Run_Sphere_Test(D,Budget,Function_Number,Process_Number):\n",
    "        Number_Of_Function_Evaluations = Budget*D\n",
    "        x0 = 10*(np.random.rand(D)-0.5)\n",
    "        X_Optimal = 10*(np.random.rand(D)-0.5)\n",
    "\n",
    "        if Function_Number == 1:\n",
    "            def fun(x): ## Sphere\n",
    "                return np.sum(((x-X_Optimal)**2) )\n",
    "\n",
    "        if Function_Number == 2:\n",
    "            def fun(x): ## Ellispod\n",
    "                a = 1000000**(np.arange(D)/D)\n",
    "                return np.sum(a*((x-X_Optimal)**2) )\n",
    "\n",
    "        if Function_Number == 3:\n",
    "            def fun(x): ## Sum_Of_Powers\n",
    "                result = 0\n",
    "                for i in range(0,len(x)):\n",
    "                    result +=     (np.abs(x[i]-X_Optimal[i]))**(2+4*(i/len(x)))    \n",
    "                return result\n",
    "\n",
    "\n",
    "        print(\"Initial Cost\",fun(x0),\"   Process Number  \",Process_Number)\n",
    "        Best_Parameters_After_Optimization = Orchestra_Optimize_With_Two_Mode_Section(x0,fun,Number_Of_Function_Evaluations,print_Cost)\n",
    "        print(\"Final Cost\",fun(Best_Parameters_After_Optimization),\"   Process Number  \",Process_Number)\n",
    "        print()\n",
    "\n",
    "\n",
    "    D = 5\n",
    "    Budget = 3000\n",
    "    Function_Number = 1\n",
    "    Number_Of_Cores = 1\n",
    "    Number_Of_Tests = 4\n",
    "\n",
    "\n",
    "    if Number_Of_Cores == 1:\n",
    "        for test in range (Number_Of_Tests):\n",
    "            Run_Sphere_Test(D,Budget,Function_Number,test)\n",
    "\n",
    "    if Number_Of_Cores == 4:\n",
    "        if __name__ == '__main__':\n",
    "            p1 = Process(target = Run_Sphere_Test, args=(D,Budget,Function_Number,1))\n",
    "            p2 = Process(target = Run_Sphere_Test, args=(D,Budget,Function_Number,2))\n",
    "            p3 = Process(target = Run_Sphere_Test, args=(D,Budget,Function_Number,3))\n",
    "            p4 = Process(target = Run_Sphere_Test, args=(D,Budget,Function_Number,4))\n",
    "            p1.start()\n",
    "            p2.start()\n",
    "            p3.start()\n",
    "            p4.start()\n",
    "            p1.join()\n",
    "            p2.join()\n",
    "            p3.join()\n",
    "            p4.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptive_Two_Mode_4_by_16\n",
      "True\n",
      "28.64826355102086\n",
      "3.922680677984437\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "## Defining Cost Function\n",
    "class Adaptive_Two_Mode_4_by_16:\n",
    "    \n",
    "    Name = 'Adaptive_Two_Mode_4_by_16'\n",
    "\n",
    "    def __init__(self, Name):\n",
    "        self.__name__ = 'Adaptive_Two_Mode_4_by_16'\n",
    "\n",
    "        \n",
    "    def Optimize(fun, x0, remaining_evals ): \n",
    "\n",
    "\n",
    "        Best_Parameters_After_Optimization = Orchestra_Optimize_With_Two_Mode_Section(x0,fun,remaining_evals,print_Cost)\n",
    "        \n",
    "  \n",
    "\n",
    "        return Best_Parameters_After_Optimization\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "SOLVER = Adaptive_Two_Mode_4_by_16\n",
    "\n",
    "print(SOLVER.__name__)\n",
    "print(SOLVER.__name__ == 'Adaptive_Two_Mode_4_by_16')\n",
    "\n",
    "def fun(x):\n",
    "    return np.sum((x-2)**2)\n",
    "\n",
    "D  = 10\n",
    "x0= np.random.rand(D)\n",
    "# remaining_evals = 1000*D\n",
    "Number_Of_Function_Evaluations = 60*D\n",
    "\n",
    "print(fun(x0))\n",
    "x_min1  = SOLVER.Optimize(fun, x0,Number_Of_Function_Evaluations)\n",
    "print(fun(x_min1))\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from multiprocessing import Process\n",
    "\n",
    "\n",
    "\n",
    "def default_observer_options(budget_=None, suite_name_=None, current_batch_=None):\n",
    "    \"\"\"return defaults computed from input parameters or current global vars\n",
    "    \"\"\"\n",
    "    global budget, suite_name, number_of_batches, current_batch\n",
    "    if budget_ is None:\n",
    "        budget_ = budget\n",
    "    if suite_name_ is None:\n",
    "        suite_name_ = suite_name\n",
    "    if current_batch_ is None and number_of_batches > 1:\n",
    "        current_batch_ = current_batch\n",
    "    opts = {}\n",
    "    try:\n",
    "        opts.update({'result_folder': '\"%s_on_%s%s_budget%04dxD\"'\n",
    "                    % (SOLVER.__name__,\n",
    "                       suite_name_,\n",
    "                       \"\" if current_batch_ is None\n",
    "                          else \"_batch%03dof%d\" % (current_batch_, number_of_batches),\n",
    "                       budget_)})\n",
    "    except: pass\n",
    "    try:\n",
    "        solver_module = '(%s)' % SOLVER.__module__\n",
    "    except:\n",
    "        solver_module = ''\n",
    "    try:\n",
    "        opts.update({'algorithm_name': SOLVER.__name__ + solver_module})\n",
    "    except: pass\n",
    "    return opts\n",
    "\n",
    "# ===============================================\n",
    "# loops over a benchmark problem suite\n",
    "# ===============================================\n",
    "def batch_loop(solver, suite, observer, budget,\n",
    "               max_runs, current_batch, number_of_batches):\n",
    "    \"\"\"loop over all problems in `suite` calling\n",
    "    `coco_optimize(solver, problem, budget * problem.dimension, max_runs)`\n",
    "    for each eligible problem.\n",
    "\n",
    "    A problem is eligible if ``problem_index + current_batch - 1``\n",
    "    modulo ``number_of_batches`` equals ``0``.\n",
    "\n",
    "    This distribution into batches is likely to lead to similar\n",
    "    runtimes for the batches, which is usually desirable.\n",
    "    \"\"\"\n",
    "    addressed_problems = []\n",
    "    short_info = ShortInfo()\n",
    "    for problem_index, problem in enumerate(suite):\n",
    "        if (problem_index + current_batch - 1) % number_of_batches:\n",
    "            continue\n",
    "        observer.observe(problem)\n",
    "        short_info.print(problem) if verbose else None\n",
    "        runs = coco_optimize(solver, problem, budget * problem.dimension,\n",
    "                             max_runs)\n",
    "        if verbose:\n",
    "            print_flush(\"!\" if runs > 2 else \":\" if runs > 1 else \".\")\n",
    "        short_info.add_evals(problem.evaluations + problem.evaluations_constraints, runs)\n",
    "        problem.free()  # not necessary as `enumerate` tears the problem down\n",
    "        addressed_problems += [problem.id]\n",
    "    print(short_info.function_done() + short_info.dimension_done())\n",
    "    short_info.print_timings()\n",
    "    print(\"  %s done (%d of %d problems benchmarked%s)\" %\n",
    "           (suite_name, len(addressed_problems), len(suite),\n",
    "             ((\" in batch %d of %d\" % (current_batch, number_of_batches))\n",
    "               if number_of_batches > 1 else \"\")), end=\"\")\n",
    "    if number_of_batches > 1:\n",
    "        print(\"\\n    MAKE SURE TO RUN ALL BATCHES\", end=\"\")\n",
    "    return addressed_problems\n",
    "\n",
    "#===============================================\n",
    "# interface: ADD AN OPTIMIZER BELOW\n",
    "#===============================================\n",
    "def coco_optimize(solver, fun, max_evals, max_runs=1e9):\n",
    "    \"\"\"`fun` is a callable, to be optimized by `solver`.\n",
    "\n",
    "    The `solver` is called repeatedly with different initial solutions\n",
    "    until either the `max_evals` are exhausted or `max_run` solver calls\n",
    "    have been made or the `solver` has not called `fun` even once\n",
    "    in the last run.\n",
    "\n",
    "    Return number of (almost) independent runs.\n",
    "    \"\"\"\n",
    "    range_ = fun.upper_bounds - fun.lower_bounds\n",
    "    center = fun.lower_bounds + range_ / 2\n",
    "    if fun.evaluations:\n",
    "        print('WARNING: %d evaluations were done before the first solver call' %\n",
    "              fun.evaluations)\n",
    "\n",
    "    for restarts in range(int(max_runs)):\n",
    "        remaining_evals = max_evals - fun.evaluations - fun.evaluations_constraints\n",
    "        x0 = center + (restarts > 0) * 0.8 * range_ * (\n",
    "                np.random.rand(fun.dimension) - 0.5)\n",
    "        fun(x0)  # can be incommented, if this is done by the solver\n",
    "\n",
    "        if solver.__name__ in (\"random_search\", ):\n",
    "            solver(fun, fun.lower_bounds, fun.upper_bounds,\n",
    "                   remaining_evals)\n",
    "        elif solver.__name__ == 'fmin' and solver.__globals__['__name__'] in ['cma', 'cma.evolution_strategy', 'cma.es']:\n",
    "            if x0[0] == center[0]:\n",
    "                sigma0 = 0.02\n",
    "                restarts_ = 0\n",
    "            else:\n",
    "                x0 = \"%f + %f * np.random.rand(%d)\" % (\n",
    "                        center[0], 0.8 * range_[0], fun.dimension)\n",
    "                sigma0 = 0.2\n",
    "                restarts_ = 6 * (observer_options.as_string.find('IPOP') >= 0)\n",
    "\n",
    "            solver(fun, x0, sigma0 * range_[0], restarts=restarts_,\n",
    "                   options=dict(scaling=range_/range_[0], maxfevals=remaining_evals,\n",
    "                                termination_callback=lambda es: fun.final_target_hit,\n",
    "                                verb_log=0, verb_disp=0, verbose=-9))\n",
    "        elif solver.__name__ == 'fmin_slsqp':\n",
    "            solver(fun, x0, iter=1 + remaining_evals / fun.dimension,\n",
    "                   iprint=-1)\n",
    "        elif solver.__name__ in (\"fmin_cobyla\", ):\n",
    "            x0 = fun.initial_solution\n",
    "            solver(fun, x0, lambda x: -fun.constraint(x), maxfun=remaining_evals,\n",
    "                   disp=0, rhoend=1e-9)\n",
    "############################ ADD HERE ########################################\n",
    "        # ### IMPLEMENT HERE THE CALL TO ANOTHER SOLVER/OPTIMIZER ###\n",
    "        elif solver.__name__ == 'Adaptive_Two_Mode_4_by_16':\n",
    "        #     CALL MY SOLVER, interfaces vary\n",
    "\n",
    "            solver.Optimize(fun, x0,remaining_evals)\n",
    "\n",
    "##############################################################################\n",
    "        else:\n",
    "            solver(fun, x0)\n",
    "\n",
    "        if fun.evaluations + fun.evaluations_constraints >= max_evals or            fun.final_target_hit:\n",
    "            break\n",
    "        # quit if fun.evaluations did not increase\n",
    "        still_remaining = max_evals - fun.evaluations - fun.evaluations_constraints\n",
    "        if still_remaining >= remaining_evals:  # break loop if no evaluations were done\n",
    "            if still_remaining > remaining_evals:\n",
    "                raise RuntimeError(\"function evaluations decreased\")\n",
    "            if still_remaining >= fun.dimension + 2:\n",
    "                print(\"WARNING: %d evaluations of budget %d remaining\" %\n",
    "                      (still_remaining, max_evals))\n",
    "            break\n",
    "    return 1 + restarts  # number of (almost) independent launches of `solver`\n",
    "\n",
    "# ===============================================\n",
    "# set up: CHANGE HERE SOLVER AND FURTHER SETTINGS AS DESIRED\n",
    "# ===============================================\n",
    "######################### CHANGE HERE ########################################\n",
    "# CAVEAT: this might be modified from input args\n",
    "# suite_name = \"bbob-largescale\"  \n",
    "suite_name = \"bbob\"  \n",
    "    \n",
    "    \n",
    "    \n",
    "budget = 1000  # maxfevals = budget x dimension ### INCREASE budget WHEN THE DATA CHAIN IS STABLE ###\n",
    "max_runs = 1e9  # number of (almost) independent trials per problem instance\n",
    "number_of_batches = 4  # allows to run everything in several batches\n",
    "current_batch = np.arange(number_of_batches)     # 1..number_of_batches\n",
    "##############################################################################\n",
    "# By default we call SOLVER(fun, x0), but the INTERFACE CAN BE ADAPTED TO EACH SOLVER ABOVE\n",
    "\n",
    "# SOLVER = random_search\n",
    "\n",
    "\n",
    "# SOLVER = fmin_slsqp\n",
    "SOLVER = Adaptive_Two_Mode_4_by_16\n",
    "\n",
    "# SOLVER = optimize.fmin_cobyla\n",
    "# SOLVER = my_solver # SOLVER = fmin_slsqp # SOLVER = cma.fmin\n",
    "suite_instance = \"year:2016\" # \"year:2016\"\n",
    "suite_options = \"\"  # \"dimensions: 2,3,5,10,20 \"  # if 40 is not desired\n",
    "# for more suite options, see http://numbbo.github.io/coco-doc/C/#suite-parameters\n",
    "observer_options = ObserverOptions({  # is (inherited from) a dictionary\n",
    "                    'algorithm_info': '\"ADAPTIVE TWO MODE\"', # CHANGE/INCOMMENT THIS!\n",
    "                    # 'algorithm_name': '',  # default already provided from SOLVER name\n",
    "                    # 'result_folder': '',  # default already provided from several global vars\n",
    "                   })\n",
    "######################### END CHANGE HERE ####################################\n",
    "\n",
    "# ===============================================\n",
    "# run (main)\n",
    "# ===============================================\n",
    "def main(budget=budget,\n",
    "         max_runs=max_runs,\n",
    "         current_batch=current_batch,\n",
    "         number_of_batches=number_of_batches):\n",
    "    \"\"\"Initialize suite and observer, then benchmark solver by calling\n",
    "    ``batch_loop(SOLVER, suite, observer, budget,...``\n",
    "    \"\"\"\n",
    "    suite = Suite(suite_name, suite_instance, suite_options)\n",
    "\n",
    "    observer_name = default_observers()[suite_name]\n",
    "    # observer_name = another observer if so desired\n",
    "    observer_options.update_gracefully(default_observer_options())\n",
    "    observer = Observer(observer_name, observer_options.as_string)\n",
    "\n",
    "    print(\"Benchmarking solver '%s' with budget=%d*dimension on %s suite, %s\"\n",
    "          % (' '.join(str(SOLVER).split()[:2]), budget,\n",
    "             suite.name, time.asctime()))\n",
    "    if number_of_batches > 1:\n",
    "        print('Batch usecase, make sure you run *all* %d batches.\\n' %\n",
    "              number_of_batches)\n",
    "    t0 = time.clock()\n",
    "    \n",
    "    \n",
    "    batch_loop(SOLVER, suite, observer, budget, max_runs,\n",
    "               current_batch, number_of_batches)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\", %s (%s total elapsed time).\" %\n",
    "            (time.asctime(), ascetime(time.clock() - t0)))\n",
    "    print('Data written to folder', observer.result_folder)\n",
    "    print('To post-process the data call \\n'\n",
    "          '    python -m cocopp %s \\n'\n",
    "          'from a system shell or \\n'\n",
    "          '    cocopp.main(\"%s\") \\n'\n",
    "          'from a python shell' % (2 * (observer.result_folder,)))\n",
    "\n",
    "# ===============================================\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"read input parameters and call `main()`\"\"\"\n",
    "#     print(len(sys.argv))\n",
    "    if len(sys.argv) > 1:\n",
    "#         sys.argv[1] = 'bbob-largescale'\n",
    "        sys.argv[1] = 'bbob'\n",
    "        sys.argv = sys.argv[0:2]\n",
    "\n",
    "        if len(sys.argv) < 2 or sys.argv[1] in [\"--help\", \"-h\"]:\n",
    "                print(__doc__)\n",
    "                print(\"Recognized suite names: \" + str(cocoex.known_suite_names))\n",
    "                sys.exit(0)\n",
    "        suite_name = sys.argv[1]\n",
    "\n",
    "#     suite_name = 'bbob-largescale'\n",
    "    suite_name = 'bbob'\n",
    "    if suite_name not in cocoex.known_suite_names:\n",
    "        print('WARNING: \"%s\" not in known names %s' %\n",
    "                (suite_name, str(cocoex.known_suite_names)))\n",
    "    if len(sys.argv) > 2:\n",
    "        budget = float(sys.argv[2])\n",
    "    if len(sys.argv) > 3:\n",
    "        current_batch = int(sys.argv[3])\n",
    "    if len(sys.argv) > 4:\n",
    "        number_of_batches = int(sys.argv[4])\n",
    "    if len(sys.argv) > 5:\n",
    "        messages = ['Argument \"%s\" disregarded (only 4 arguments are recognized).' % sys.argv[i]\n",
    "            for i in range(5, len(sys.argv))]\n",
    "        messages.append('See \"python example_experiment.py -h\" for help.')\n",
    "        raise ValueError('\\n'.join(messages))\n",
    "    if number_of_batches == 1:\n",
    "        main(budget, max_runs, current_batch, number_of_batches)\n",
    "    \n",
    "    \n",
    "    ### This is the parallel processing\n",
    "    if number_of_batches == 4:\n",
    "        p1 = Process(target=main, args=(budget, max_runs, 1, number_of_batches))\n",
    "        p2 = Process(target=main, args=(budget, max_runs, 2, number_of_batches))\n",
    "        p3 = Process(target=main, args=(budget, max_runs, 3, number_of_batches))\n",
    "        p4 = Process(target=main, args=(budget, max_runs, 4, number_of_batches))\n",
    "        p1.start()\n",
    "        p2.start()\n",
    "        p3.start()\n",
    "        p4.start()\n",
    "        p1.join()\n",
    "        p2.join()\n",
    "        p3.join()\n",
    "        p4.join()\n",
    "        \n",
    "        \n",
    "    if number_of_batches == 8:\n",
    "        p1 = Process(target=main, args=(budget, max_runs, 1, number_of_batches))\n",
    "        p2 = Process(target=main, args=(budget, max_runs, 2, number_of_batches))\n",
    "        p3 = Process(target=main, args=(budget, max_runs, 3, number_of_batches))\n",
    "        p4 = Process(target=main, args=(budget, max_runs, 4, number_of_batches))\n",
    "        p5 = Process(target=main, args=(budget, max_runs, 5, number_of_batches))\n",
    "        p6 = Process(target=main, args=(budget, max_runs, 6, number_of_batches))\n",
    "        p7 = Process(target=main, args=(budget, max_runs, 7, number_of_batches))\n",
    "        p8 = Process(target=main, args=(budget, max_runs, 8, number_of_batches))\n",
    "        p1.start()\n",
    "        p2.start()\n",
    "        p3.start()\n",
    "        p4.start()\n",
    "        p5.start()\n",
    "        p6.start()\n",
    "        p7.start()\n",
    "        p8.start()\n",
    "        p1.join()\n",
    "        p2.join()\n",
    "        p3.join()\n",
    "        p4.join()\n",
    "        p5.join()\n",
    "        p6.join()\n",
    "        p7.join()\n",
    "        p8.join()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
